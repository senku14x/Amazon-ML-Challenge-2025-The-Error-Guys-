# -*- coding: utf-8 -*-
"""psuedolabelling_finalcode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KhQoVpJ34d4U8JskI0xdOUKUu3L6NRY2
"""

# ============================================================
# SMART PRODUCT PRICING â€” FULL 3-FIELD PARSER + PACK CLEANUP
#   Input : train.csv / test.csv  (original dataset)
#   Output: train_final_clean.csv / test_final_clean.csv
#   Columns added:
#       item_name, item_description, item_pack_clean,
#       has_pack, pack_value, pack_unit, pack_count
# ============================================================

import re, math, unicodedata
from pathlib import Path
from typing import Optional, Dict, Tuple
import pandas as pd

# ---------------------------
# 0) Mount Drive if Colab
# ---------------------------
IN_COLAB = False
try:
    import google.colab  # type: ignore
    IN_COLAB = True
except Exception:
    pass

if IN_COLAB:
    from google.colab import drive  # type: ignore
    drive.mount("/content/drive")

# ---------------------------
# 1) Paths
# ---------------------------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
DATA_DIR = BASE_DIR / "dataset"
TRAIN_PATH = DATA_DIR / "train.csv"
TEST_PATH = DATA_DIR / "test.csv"
TRAIN_OUT = DATA_DIR / "train_final_clean.csv"
TEST_OUT = DATA_DIR / "test_final_clean.csv"

# ---------------------------
# 2) Text normalization
# ---------------------------
def norm_text(s: Optional[str]) -> str:
    if s is None or (isinstance(s, float) and math.isnan(s)):
        return ""
    s = str(s)
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n", s)
    return s.strip()

# ---------------------------
# 3) Regex patterns for parsing
# ---------------------------
NAME_LINE = re.compile(
    r"^\s*(?:item\s*name|product\s*name|item\s*title|title|name)\s*[:\-]\s*(.+?)\s*$",
    re.IGNORECASE | re.MULTILINE,
)
DESC_LINE = re.compile(
    r"^\s*(?:item\s*description|description|bullet\s*points?)\s*[:\-]\s*(.+?)\s*$",
    re.IGNORECASE | re.MULTILINE,
)
BULLET_LINE = re.compile(r"^(?:[-â€¢*]|\d+[.)])\s*(.+)", re.IGNORECASE | re.MULTILINE)
ANY_LABEL = re.compile(
    r"(?:^|\n)\s*(?:item\s*name|product\s*name|item\s*title|title|name|"
    r"item\s*description|description|bullet\s*points?|"
    r"item\s*pack\s*quantity|ipq|quantity|qty|value|size|unit|uom)\s*[:\-]",
    re.IGNORECASE | re.MULTILINE,
)
# Pack patterns
COUNT_X_SIZE = re.compile(r"(?P<count>\d{1,3})\s*[xXÃ—*]\s*(?P<size>\d+(?:\.\d+)?)\s*(?P<unit>[A-Za-z\. ]+)")
SIZE_UNIT = re.compile(r"(?P<size>\d+(?:\.\d+)?)\s*(?P<unit>(?:ml|l|g|kg|mg|oz|fl\.?\s*oz|count|ct|pc|pcs))", re.IGNORECASE)
COUNT_ONLY = re.compile(r"(?P<count>\d{1,3})\s*(?:count|ct|pcs?|units?)", re.IGNORECASE)

# ---------------------------
# 4) Unit canonicalization
# ---------------------------
UNIT_ALIASES = {
    "ml": "ml", "milliliter": "ml", "millilitre": "ml", "milliliters": "ml", "millilitres": "ml",
    "l": "l", "liter": "l", "litre": "l", "liters": "l", "litres": "l",
    "cl": "cl", "fl oz": "fl oz", "fluid ounce": "fl oz", "fluid ounces": "fl oz",
    "g": "g", "gram": "g", "grams": "g", "kg": "kg", "kilogram": "kg", "kilograms": "kg",
    "mg": "mg", "oz": "oz", "ounce": "oz", "ounces": "oz",
    "count": "count", "ct": "count", "pc": "count", "pcs": "count", "piece": "count", "pieces": "count",
    "unit": "count", "units": "count",
}
VALID_UNITS = set(UNIT_ALIASES.values())

def norm_unit(u: Optional[str]) -> Optional[str]:
    if not u:
        return None
    u = unicodedata.normalize("NFKC", str(u)).lower().strip().replace(".", " ")
    u = re.sub(r"\s+", " ", u)
    return UNIT_ALIASES.get(u, u)

# ---------------------------
# 5) Parse a single catalog cell
# ---------------------------
def parse_catalog_cell(text: Optional[str]) -> Dict[str, Optional[str]]:
    t = norm_text(text)

    # --- Item name ---
    m_name = NAME_LINE.search(t)
    if m_name:
        item_name = m_name.group(1).strip()
    else:
        item_name = None
        for ln in (ln.strip() for ln in t.split("\n") if ln.strip()):
            if not re.match(
                r"^(?:item\s*description|description|bullet\s*points?|"
                r"item\s*pack\s*quantity|ipq|quantity|qty|value|size|unit|uom)\s*[:\-]",
                ln,
                re.IGNORECASE,
            ):
                item_name = ln
                break

    # --- Item description ---
    m_desc = DESC_LINE.search(t)
    if m_desc:
        item_description = m_desc.group(1).strip()
    else:
        bullets = [b.strip() for b in BULLET_LINE.findall(t)]
        if bullets:
            item_description = " ".join(dict.fromkeys(bullets))
        else:
            item_description = None
            if item_name:
                try:
                    start = t.lower().find(item_name.lower()) + len(item_name)
                    nxt = ANY_LABEL.search(t[start:])
                    mid = t[start:(start + nxt.start() if nxt else len(t))]
                    cand = [s.strip() for s in re.split(r"(?<=[.?!])\s+|\n", mid) if s.strip()]
                    item_description = " ".join(cand[:8]) if cand else None
                except Exception:
                    item_description = None
    if isinstance(item_description, str) and len(item_description) > 1200:
        item_description = item_description[:1200]

    # --- Item pack (raw detection) ---
    item_pack = None
    for pat in [COUNT_X_SIZE, SIZE_UNIT, COUNT_ONLY]:
        m = pat.search(t)
        if m:
            item_pack = m.group(0).strip()
            break

    return {"item_name": item_name, "item_description": item_description, "item_pack": item_pack}

# ---------------------------
# 6) Parse full DataFrame
# ---------------------------
def parse_dataframe(df: pd.DataFrame, catalog_col="catalog_content") -> pd.DataFrame:
    parsed = df[catalog_col].apply(parse_catalog_cell)
    parsed_df = pd.DataFrame(parsed.tolist(), index=df.index)
    return pd.concat([df.reset_index(drop=True), parsed_df.reset_index(drop=True)], axis=1)

# ---------------------------
# 7) Clean item_pack into numeric fields
# ---------------------------
def parse_item_pack(s: Optional[str]) -> Tuple[Optional[float], Optional[str], Optional[int]]:
    if s is None or (isinstance(s, float) and math.isnan(s)):
        return (None, None, None)
    txt = unicodedata.normalize("NFKC", str(s)).strip().lower()
    txt = re.sub(r"[â€“â€”]", "-", txt)
    txt = re.sub(r"\s+", " ", txt)

    # count x size
    m = COUNT_X_SIZE.search(txt)
    if m:
        return float(m.group("size")), norm_unit(m.group("unit")), int(m.group("count"))
    # size + unit
    m = SIZE_UNIT.search(txt)
    if m:
        return float(m.group("size")), norm_unit(m.group("unit")), None
    # count only
    m = COUNT_ONLY.search(txt)
    if m:
        return None, "count", int(m.group("count"))
    return (None, None, None)

def clean_pack_column(df: pd.DataFrame) -> pd.DataFrame:
    def _parse(cell):
        v, u, c = parse_item_pack(cell)
        if v or c:
            clean = f"{c}x{v:g} {u}" if (v and c and u != 'count') else (
                f"{v:g} {u}" if v and u else (f"{c} count" if c else None))
        else:
            clean = None
        return pd.Series(
            {"item_pack_clean": clean, "has_pack": int(clean is not None), "pack_value": v, "pack_unit": u, "pack_count": c}
        )

    parsed = df["item_pack"].apply(_parse)
    return pd.concat([df, parsed], axis=1)

# ---------------------------
# 8) Sanity check
# ---------------------------
def sanity_report(df: pd.DataFrame, title: str):
    n = len(df)
    def pct(x): return f"{100.0 * x / n:.1f}%"
    print(f"\n=== Sanity Report: {title} (n={n}) ===")
    print("item_name blanks:", pct(df["item_name"].isna().sum()))
    print("item_description blanks:", pct(df["item_description"].isna().sum()))
    print("item_pack_clean blanks:", pct(df["item_pack_clean"].isna().sum()))
    print("has_pack == 1:", pct((df["has_pack"] == 1).sum()))
    print("\nTop pack_unit:")
    print(df["pack_unit"].fillna("NA").value_counts().head(10))
    print("\nTop item_pack_clean:")
    print(df["item_pack_clean"].fillna("NA").str.lower().value_counts().head(15))

# ---------------------------
# 9) Run pipeline
# ---------------------------
def robust_read_csv(path: Path, encodings=("utf-8","latin-1","iso-8859-1","cp1252")) -> pd.DataFrame:
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    raise RuntimeError(f"Could not read {path}")

def main():
    print(f"Reading: {TRAIN_PATH.name}")
    tr = robust_read_csv(TRAIN_PATH)
    print(f"Reading: {TEST_PATH.name}")
    te = robust_read_csv(TEST_PATH)

    print("\nParsing text fields...")
    tr_parsed = parse_dataframe(tr, "catalog_content")
    te_parsed = parse_dataframe(te, "catalog_content")

    print("\nCleaning item_pack...")
    tr_clean = clean_pack_column(tr_parsed)
    te_clean = clean_pack_column(te_parsed)

    sanity_report(tr_clean, "train")
    sanity_report(te_clean, "test")

    tr_clean.to_csv(TRAIN_OUT, index=False)
    te_clean.to_csv(TEST_OUT, index=False)
    print(f"\nSaved:\n - {TRAIN_OUT}\n - {TEST_OUT}")

if __name__ == "__main__":
    main()

# ============================================================
# STEP 2: CREATE TRAIN/VAL SPLIT (Stratified by Price)
# ============================================================
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import StratifiedShuffleSplit

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'
SPLIT_DIR.mkdir(exist_ok=True, parents=True)

print("="*60)
print("STEP 2: Creating Train/Val Split")
print("="*60)

# 1) Load cleaned data
train_clean = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
print(f"âœ… Loaded train: {train_clean.shape}")
print(f"Columns: {train_clean.columns.tolist()}")

# 2) Create stratified split by price quantiles
y_price = train_clean['price'].astype(float).values
y_log = np.log1p(y_price)

# Create 10 bins for stratification
bins = pd.qcut(y_log, q=10, labels=False, duplicates='drop')

# Split: 80% train, 20% val
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
tr_idx, val_idx = next(sss.split(train_clean, bins))

# 3) Save indices
np.save(SPLIT_DIR / 'train_idx.npy', tr_idx)
np.save(SPLIT_DIR / 'val_idx.npy', val_idx)

print(f"\nâœ… Split created:")
print(f"   Train: {len(tr_idx)} samples ({len(tr_idx)/len(train_clean)*100:.1f}%)")
print(f"   Val:   {len(val_idx)} samples ({len(val_idx)/len(train_clean)*100:.1f}%)")

# 4) Save targets
y_tr = y_price[tr_idx]
y_val = y_price[val_idx]
y_tr_log = y_log[tr_idx]
y_val_log = y_log[val_idx]

np.save(SPLIT_DIR / 'y_train.npy', y_tr)
np.save(SPLIT_DIR / 'y_val.npy', y_val)
np.save(SPLIT_DIR / 'y_train_log.npy', y_tr_log)
np.save(SPLIT_DIR / 'y_val_log.npy', y_val_log)

print(f"\nâœ… Targets saved")

# 5) Quick sanity check
print("\n--- Price Distribution Check ---")
print(f"Train - Min: ${y_tr.min():.2f}, Median: ${np.median(y_tr):.2f}, Max: ${y_tr.max():.2f}")
print(f"Val   - Min: ${y_val.min():.2f}, Median: ${np.median(y_val):.2f}, Max: ${y_val.max():.2f}")

# 6) Save sample IDs for reference
train_sample_ids = train_clean['sample_id'].values
np.save(SPLIT_DIR / 'train_sample_ids.npy', train_sample_ids[tr_idx])
np.save(SPLIT_DIR / 'val_sample_ids.npy', train_sample_ids[val_idx])

print(f"\nâœ… All split files saved to: {SPLIT_DIR}")
print("Files created:")
print("  - train_idx.npy, val_idx.npy")
print("  - y_train.npy, y_val.npy, y_train_log.npy, y_val_log.npy")
print("  - train_sample_ids.npy, val_sample_ids.npy")

# ============================================================
# STEP 2.5: ANALYZE & HANDLE MISSING DATA
# Keep all columns, handle missing values intelligently
# ============================================================
import numpy as np
import pandas as pd
from pathlib import Path

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'

print("="*60)
print("STEP 2.5: Missing Data Analysis & Handling")
print("="*60)

# ============================================================
# 1) LOAD DATASETS
# ============================================================
print("\nðŸ“‚ Loading datasets...")
train_full = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
test_full = pd.read_csv(DATA_DIR / 'test_final_clean.csv')

# Load split indices
tr_idx = np.load(SPLIT_DIR / 'train_idx.npy')
val_idx = np.load(SPLIT_DIR / 'val_idx.npy')

# Split
df_train = train_full.iloc[tr_idx].reset_index(drop=True).copy()
df_val = train_full.iloc[val_idx].reset_index(drop=True).copy()
df_test = test_full.reset_index(drop=True).copy()

print(f"âœ… Train: {df_train.shape}")
print(f"âœ… Val:   {df_val.shape}")
print(f"âœ… Test:  {df_test.shape}")

# ============================================================
# 2) ANALYZE MISSING DATA
# ============================================================
print("\n" + "="*60)
print("MISSING DATA ANALYSIS")
print("="*60)

def analyze_missing(df, name):
    print(f"\n{name} Dataset:")
    print("-" * 40)

    missing_info = []
    for col in df.columns:
        if col == 'split':
            continue
        n_missing = df[col].isna().sum()
        pct_missing = n_missing / len(df) * 100
        dtype = df[col].dtype

        missing_info.append({
            'column': col,
            'missing_count': n_missing,
            'missing_pct': pct_missing,
            'dtype': dtype
        })

    missing_df = pd.DataFrame(missing_info).sort_values('missing_pct', ascending=False)

    # Show only columns with missing data
    missing_only = missing_df[missing_df['missing_count'] > 0]
    if len(missing_only) > 0:
        print(missing_only.to_string(index=False))
    else:
        print("âœ… No missing values!")

    return missing_df

train_missing = analyze_missing(df_train, "TRAIN")
val_missing = analyze_missing(df_val, "VAL")
test_missing = analyze_missing(df_test, "TEST")

# ============================================================
# 3) MISSING DATA HANDLING STRATEGY
# ============================================================
print("\n" + "="*60)
print("MISSING DATA HANDLING STRATEGY")
print("="*60)

def handle_missing_values(df, is_test=False):
    """
    Handle missing values based on column type
    Returns: processed dataframe
    """
    df = df.copy()

    # TEXT COLUMNS: Fill with empty string + create 'is_missing' flag
    text_columns = ['catalog_content', 'item_name', 'item_description', 'item_pack_clean']

    for col in text_columns:
        if col in df.columns:
            # Create missing flag (valuable feature!)
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with empty string
            df[col] = df[col].fillna('')

            print(f"âœ… {col:25s} â†’ filled with '' + added '{col}_is_missing' flag")

    # NUMERIC COLUMNS: Fill with -1 (or 0 for counts) + create 'is_missing' flag
    numeric_columns = ['pack_value', 'pack_count']

    for col in numeric_columns:
        if col in df.columns:
            # Create missing flag
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with special value
            fill_value = 0 if 'count' in col else -1
            df[col] = df[col].fillna(fill_value)

            print(f"âœ… {col:25s} â†’ filled with {fill_value} + added '{col}_is_missing' flag")

    # CATEGORICAL COLUMNS: Fill with 'unknown' + create 'is_missing' flag
    categorical_columns = ['pack_unit', 'item_pack']

    for col in categorical_columns:
        if col in df.columns:
            # Create missing flag
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with 'unknown'
            df[col] = df[col].fillna('unknown')

            print(f"âœ… {col:25s} â†’ filled with 'unknown' + added '{col}_is_missing' flag")

    # BINARY COLUMNS: Fill with 0
    binary_columns = ['has_pack']

    for col in binary_columns:
        if col in df.columns:
            df[col] = df[col].fillna(0).astype(int)
            print(f"âœ… {col:25s} â†’ filled with 0")

    # URL COLUMNS: Keep as-is (we'll extract features from them)
    url_columns = ['image_link']
    for col in url_columns:
        if col in df.columns:
            df[f'{col}_is_missing'] = df[col].isna().astype(int)
            df[col] = df[col].fillna('')
            print(f"âœ… {col:25s} â†’ kept + added '{col}_is_missing' flag")

    return df

# Apply to all three datasets
print("\nðŸ“ Processing TRAIN:")
df_train_processed = handle_missing_values(df_train, is_test=False)

print("\nðŸ“ Processing VAL:")
df_val_processed = handle_missing_values(df_val, is_test=False)

print("\nðŸ“ Processing TEST:")
df_test_processed = handle_missing_values(df_test, is_test=True)

# ============================================================
# 4) VERIFY NO MISSING VALUES REMAIN
# ============================================================
print("\n" + "="*60)
print("VERIFICATION: Check No Missing Values Remain")
print("="*60)

for name, df in [('Train', df_train_processed), ('Val', df_val_processed), ('Test', df_test_processed)]:
    total_missing = df.isna().sum().sum()
    print(f"{name:5s}: {total_missing} missing values")

    if total_missing > 0:
        print(f"  âš ï¸ Columns still with missing:")
        print(df.isna().sum()[df.isna().sum() > 0])

# ============================================================
# 5) ADD SPLIT IDENTIFIER
# ============================================================
df_train_processed['split'] = 'train'
df_val_processed['split'] = 'val'
df_test_processed['split'] = 'test'

# ============================================================
# 6) SAVE PROCESSED DATAFRAMES
# ============================================================
print("\nðŸ’¾ Saving processed dataframes...")

df_train_processed.to_parquet(SPLIT_DIR / 'df_train_processed.parquet', index=False)
df_val_processed.to_parquet(SPLIT_DIR / 'df_val_processed.parquet', index=False)
df_test_processed.to_parquet(SPLIT_DIR / 'df_test_processed.parquet', index=False)

print(f"âœ… Saved to: {SPLIT_DIR}")
print("  - df_train_processed.parquet")
print("  - df_val_processed.parquet")
print("  - df_test_processed.parquet")

# ============================================================
# 7) SUMMARY
# ============================================================
print("\n" + "="*60)
print("âœ… STEP 2.5 COMPLETE - MISSING DATA HANDLED")
print("="*60)

print("\nðŸ“Š New columns added (missing indicators):")
new_cols = [col for col in df_train_processed.columns if '_is_missing' in col]
for col in new_cols:
    n_missing_train = df_train_processed[col].sum()
    n_missing_val = df_val_processed[col].sum()
    n_missing_test = df_test_processed[col].sum()
    print(f"  âœ… {col:35s} Train: {n_missing_train:5d} | Val: {n_missing_val:5d} | Test: {n_missing_test:5d}")

print("\nðŸ“¦ What we kept:")
print("  âœ… ALL original columns (catalog_content, image_link, etc.)")
print("  âœ… ALL parsed columns (item_name, item_description, pack_*, etc.)")
print("  âœ… Added _is_missing flags (valuable features!)")
print("  âœ… No NaN values remain")

print("\nðŸ’¡ Strategy:")
print("  1. Keep everything - let SHAP decide what's useful")
print("  2. Missing flags capture 'missingness' as signal")
print("  3. Clean fill values ensure no errors downstream")
print("  4. Same processing for train/val/test = consistency")

print(f"\nðŸ“ˆ Final shapes:")
print(f"  Train: {df_train_processed.shape}")
print(f"  Val:   {df_val_processed.shape}")
print(f"  Test:  {df_test_processed.shape}")

print("\nðŸŽ¯ Ready for Step 3: Feature Extraction!")

# ============================================================
# STEP 2.6: ANALYZE DATA TYPES & PREPARE FOR EMBEDDINGS
# ============================================================
import numpy as np
import pandas as pd
from pathlib import Path

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'

print("="*60)
print("STEP 2.6: Data Type Analysis")
print("="*60)

# ============================================================
# 1) LOAD DATASETS
# ============================================================
print("\nðŸ“‚ Loading datasets...")
train_full = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
test_full = pd.read_csv(DATA_DIR / 'test_final_clean.csv')

# Load split indices
tr_idx = np.load(SPLIT_DIR / 'train_idx.npy')
val_idx = np.load(SPLIT_DIR / 'val_idx.npy')

# Split
df_train = train_full.iloc[tr_idx].reset_index(drop=True).copy()
df_val = train_full.iloc[val_idx].reset_index(drop=True).copy()
df_test = test_full.reset_index(drop=True).copy()

print(f"âœ… Train: {df_train.shape}")
print(f"âœ… Val:   {df_val.shape}")
print(f"âœ… Test:  {df_test.shape}")

# ============================================================
# 2) DATA TYPE ANALYSIS
# ============================================================
print("\n" + "="*60)
print("DATA TYPE ANALYSIS - TRAIN DATASET")
print("="*60)

# Get comprehensive info about each column
def analyze_column(df, col_name):
    """Detailed analysis of a single column"""
    col = df[col_name]

    info = {
        'column': col_name,
        'dtype': str(col.dtype),
        'non_null': col.notna().sum(),
        'null': col.isna().sum(),
        'null_pct': f"{col.isna().sum() / len(df) * 100:.1f}%",
        'unique': col.nunique(),
        'memory_mb': col.memory_usage(deep=True) / 1024**2
    }

    # Type-specific analysis
    if col.dtype == 'object':
        # Text columns
        non_null = col.dropna()
        if len(non_null) > 0:
            info['min_len'] = non_null.astype(str).str.len().min()
            info['max_len'] = non_null.astype(str).str.len().max()
            info['avg_len'] = non_null.astype(str).str.len().mean()
            info['sample'] = str(non_null.iloc[0])[:50] + "..." if len(str(non_null.iloc[0])) > 50 else str(non_null.iloc[0])

    elif col.dtype in ['int64', 'float64']:
        # Numeric columns
        non_null = col.dropna()
        if len(non_null) > 0:
            info['min'] = non_null.min()
            info['max'] = non_null.max()
            info['mean'] = non_null.mean()
            info['median'] = non_null.median()

    return info

# Analyze all columns
print("\n" + "-"*100)
print(f"{'Column':<25} {'Type':<10} {'Non-Null':<10} {'Null %':<8} {'Unique':<8} {'Memory(MB)':<12}")
print("-"*100)

column_analysis = []
for col in df_train.columns:
    info = analyze_column(df_train, col)
    column_analysis.append(info)
    print(f"{info['column']:<25} {info['dtype']:<10} {info['non_null']:<10} {info['null_pct']:<8} {info['unique']:<8} {info['memory_mb']:<12.2f}")

# ============================================================
# 3) DETAILED COLUMN INSPECTION
# ============================================================
print("\n" + "="*60)
print("DETAILED COLUMN INSPECTION")
print("="*60)

# Group columns by purpose
TEXT_COLS = ['catalog_content', 'item_name', 'item_description', 'item_pack', 'item_pack_clean']
NUMERIC_COLS = ['price', 'pack_value', 'pack_count', 'has_pack']
CATEGORICAL_COLS = ['pack_unit']
ID_COLS = ['sample_id']
URL_COLS = ['image_link']

print("\nðŸ“ TEXT COLUMNS (for embeddings):")
print("-" * 80)
for col in TEXT_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            lengths = non_null.astype(str).str.len()
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Length: min={lengths.min()}, max={lengths.max()}, avg={lengths.mean():.0f}")
            print(f"  Sample: {str(non_null.iloc[0])[:100]}...")

print("\n\nðŸ”¢ NUMERIC COLUMNS:")
print("-" * 80)
for col in NUMERIC_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Range: {non_null.min():.2f} to {non_null.max():.2f}")
            print(f"  Mean: {non_null.mean():.2f}, Median: {non_null.median():.2f}")
            print(f"  Samples: {non_null.head(5).tolist()}")

print("\n\nðŸ·ï¸  CATEGORICAL COLUMNS:")
print("-" * 80)
for col in CATEGORICAL_COLS:
    if col in df_train.columns:
        print(f"\n{col}:")
        print(f"  Unique values: {df_train[col].nunique()}")
        print(f"  Top 10 values:")
        print(df_train[col].value_counts().head(10))

print("\n\nðŸ”— URL COLUMNS (for image embeddings):")
print("-" * 80)
for col in URL_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Sample URLs:")
            for url in non_null.head(3):
                print(f"    {url}")

# ============================================================
# 4) EMBEDDING STRATEGY SUMMARY
# ============================================================
print("\n" + "="*60)
print("EMBEDDING STRATEGY")
print("="*60)

print("\nðŸ“Š Based on data type analysis:\n")

print("âœ… TEXT EMBEDDINGS (E5-large-v2, 1024 dims):")
print("   Primary: catalog_content (full text)")
print("   OR combine: item_name + item_description")
print("   â†’ Recommendation: Use catalog_content (most complete)")

print("\nâœ… IMAGE EMBEDDINGS (ViT-L/14, 768 dims):")
print("   Source: image_link URLs")
print("   â†’ Download images from URLs â†’ Encode with OpenCLIP")

print("\nðŸ“¦ Strategy Decision:")
print("   Option 1: Use catalog_content (raw) â†’ Best for completeness")
print("   Option 2: Use item_name + ' ' + item_description â†’ Best for parsed data")
print("   Option 3: Use BOTH as separate embeddings â†’ Most features")
print("\n   ðŸ’¡ Recommendation: Option 1 (catalog_content) - simpler & complete")

# ============================================================
# 5) CHECK CONSISTENCY ACROSS SPLITS
# ============================================================
print("\n" + "="*60)
print("CONSISTENCY CHECK: Train vs Val vs Test")
print("="*60)

print(f"\n{'Column':<25} {'Train Coverage':<15} {'Val Coverage':<15} {'Test Coverage':<15}")
print("-" * 70)

for col in df_train.columns:
    if col != 'price':  # price not in test
        train_cov = f"{(~df_train[col].isna()).sum()}/{len(df_train)}"
        val_cov = f"{(~df_val[col].isna()).sum()}/{len(df_val)}"
        test_cov = f"{(~df_test[col].isna()).sum()}/{len(df_test)}" if col in df_test.columns else "N/A"
        print(f"{col:<25} {train_cov:<15} {val_cov:<15} {test_cov:<15}")

# ============================================================
# 6) SUMMARY & NEXT STEPS
# ============================================================
print("\n" + "="*60)
print("âœ… STEP 2.6 COMPLETE")
print("="*60)

print("\nðŸ“‹ Data Types Summary:")
print(f"  â€¢ Text columns: {len(TEXT_COLS)} (for text embeddings)")
print(f"  â€¢ Numeric columns: {len(NUMERIC_COLS)} (direct features)")
print(f"  â€¢ Categorical columns: {len(CATEGORICAL_COLS)} (need encoding)")
print(f"  â€¢ URL columns: {len(URL_COLS)} (for image embeddings)")
print(f"  â€¢ ID columns: {len(ID_COLS)} (tracking only)")

print("\nðŸŽ¯ Ready for Step 3:")
print("  â†’ Generate Text Embeddings (E5-large-v2)")
print("  â†’ Generate Image Embeddings (ViT-L/14)")
print("  â†’ Use your original pipeline with these inputs!")

print(f"\nðŸ’¾ Key text field for embeddings: 'catalog_content'")
print(f"ðŸ’¾ Key image field for embeddings: 'image_link'")

# ============================================================
# STEP 3A (PART 1): TEXT EMBEDDINGS â€” E5-LARGE-V2 (Fixed, Uses item_name + item_description)
# ============================================================
!pip install -q sentence-transformers tqdm torch

import os, gc, numpy as np, pandas as pd, torch
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ---------------------------
# Config
# ---------------------------
BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
EMB_DIR   = BASE_DIR / "embeddings" / "e5_large"
EMB_DIR.mkdir(parents=True, exist_ok=True)

DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_ID = "intfloat/e5-large-v2"
MAX_LEN  = 512
BATCH_SZ = 32

print(f"Using device: {DEVICE}")
print(f"Model: {MODEL_ID}")

# ---------------------------
# Load data & split indices
# ---------------------------
print("\nðŸ“‚ Loading processed parquet data...")
train_df = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
val_df   = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
test_df  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

print(f"âœ… Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

# ---------------------------
# Combine fields for embedding
# ---------------------------
def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

def prep_texts(df):
    texts = ["passage: " + combine_text(n, d)
             for n, d in zip(df["item_name"], df["item_description"])]
    return texts

texts_train = prep_texts(train_df)
texts_val   = prep_texts(val_df)
texts_test  = prep_texts(test_df)

print("Example text:\n", texts_train[0][:250])

# ---------------------------
# Load model
# ---------------------------
model = SentenceTransformer(MODEL_ID, device=DEVICE)
model.max_seq_length = MAX_LEN
model.eval()

# ---------------------------
# Encode helper
# ---------------------------
@torch.no_grad()
def encode_texts(texts, batch_size=BATCH_SZ):
    out = []
    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i+batch_size]
        emb = model.encode(
            batch,
            batch_size=len(batch),
            device=DEVICE,
            convert_to_numpy=True,
            normalize_embeddings=True,   # L2-normalized
            show_progress_bar=False
        )
        out.append(emb)
    return np.vstack(out)

# ---------------------------
# Encode & save
# ---------------------------
print("\nâš™ï¸ Encoding train texts...")
X_text_tr = encode_texts(texts_train)
print("âš™ï¸ Encoding val texts...")
X_text_val = encode_texts(texts_val)
print("âš™ï¸ Encoding test texts...")
X_text_te = encode_texts(texts_test)

print("\nðŸ’¾ Saving embeddings...")
np.save(EMB_DIR / "X_text_tr_e5l2.npy", X_text_tr)
np.save(EMB_DIR / "X_text_val_e5l2.npy", X_text_val)
np.save(EMB_DIR / "X_text_te_e5l2.npy", X_text_te)

meta = {
    "model": MODEL_ID,
    "pooling": "mean",
    "normalization": "L2",
    "max_len": MAX_LEN,
    "prefix": "passage:",
    "input_fields": ["item_name", "item_description"],
    "dims": int(X_text_tr.shape[1]),
}
pd.Series(meta).to_json(EMB_DIR / "meta.json", indent=2)

print(f"\n Done. Shapes:")
print(f"  Train: {X_text_tr.shape}")
print(f"  Val  : {X_text_val.shape}")
print(f"  Test : {X_text_te.shape}")
print(f"Embeddings saved to: {EMB_DIR}")

!pip install -q transformers accelerate datasets evaluate torch tqdm

import os, gc, numpy as np, pandas as pd, torch
from pathlib import Path
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding,
    EarlyStoppingCallback
)
import evaluate
from tqdm import tqdm

# ---------------------------
# Config
# ---------------------------
BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
MODEL_DIR = BASE_DIR / "models" / "deberta_v3_price_reg"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

MODEL_ID  = "microsoft/deberta-v3-base"
DEVICE    = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN   = 256
SEED      = 42

# ---------------------------
# Load data
# ---------------------------
train_df = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
val_df   = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")

# Target transform
train_df["log_price"] = np.log1p(train_df["price"])
val_df["log_price"]   = np.log1p(val_df["price"])

def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

train_df["text"] = [combine_text(n, d) for n, d in zip(train_df["item_name"], train_df["item_description"])]
val_df["text"]   = [combine_text(n, d) for n, d in zip(val_df["item_name"], val_df["item_description"])]

print(f"âœ… Train: {len(train_df)}, Val: {len(val_df)}")

# ---------------------------
# Tokenization
# ---------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
    )

train_ds = Dataset.from_pandas(train_df[["text", "log_price"]])
val_ds   = Dataset.from_pandas(val_df[["text", "log_price"]])
raw_datasets = DatasetDict({"train": train_ds, "validation": val_ds})
tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=["text"])
tokenized_datasets = tokenized_datasets.rename_column("log_price", "labels")
tokenized_datasets.set_format("torch")

# ---------------------------
# Metrics
# ---------------------------
mae_metric = evaluate.load("mae")

def smape(preds, labels):
    return np.mean(
        np.abs(preds - labels) / ((np.abs(preds) + np.abs(labels)) / 2 + 1e-8)
    ) * 100

def compute_metrics(eval_pred):
    preds = eval_pred.predictions.squeeze()
    labels = eval_pred.label_ids.squeeze()
    mae_val = mae_metric.compute(predictions=preds, references=labels)["mae"]
    smape_val = smape(np.expm1(preds), np.expm1(labels))
    return {"mae": mae_val, "smape": smape_val}

# ---------------------------
# Model + Trainer
# ---------------------------
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID, num_labels=1, problem_type="regression"
).to(DEVICE)

args = TrainingArguments(
    output_dir=str(MODEL_DIR),
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=6,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="mae",
    greater_is_better=False,
    fp16=True,
    logging_dir=str(MODEL_DIR / "logs"),
    logging_strategy="steps",
    logging_steps=100,
    seed=SEED,
    warmup_ratio=0.1,              #
    lr_scheduler_type="cosine",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()

trainer.save_model(str(MODEL_DIR))
tokenizer.save_pretrained(str(MODEL_DIR))
print(f"\nâœ… Model and tokenizer saved to: {MODEL_DIR}")

import numpy as np, pandas as pd, torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
MODEL_DIR = BASE_DIR / "models" / "deberta_v3_price_reg"

DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN = 256
BATCH   = 16

# ---------- load best checkpoint ----------
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))
model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_DIR), num_labels=1)
model.to(DEVICE).eval()

def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

def prepare_texts(df):
    return [combine_text(n, d) for n, d in zip(df["item_name"], df["item_description"])]

# ---------- mean-pool helper ----------
def mean_pool(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    return (last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)

@torch.no_grad()
def encode_texts(texts):
    outs = []
    for i in tqdm(range(0, len(texts), BATCH), desc="DeBERTa encode"):
        batch = texts[i:i+BATCH]
        toks = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt").to(DEVICE)
        # Use the backboneâ€™s hidden states (ignore the regression head)
        hidden = model.deberta(**toks)[0]                 # last_hidden_state [B, L, H]
        emb = mean_pool(hidden, toks["attention_mask"])   # [B, H]
        emb = torch.nn.functional.normalize(emb, p=2, dim=1)  # L2 for safety
        outs.append(emb.detach().cpu().numpy())
    return np.vstack(outs)

# ---------- load splits ----------
tr = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
va = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
te = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

txt_tr = prepare_texts(tr)
txt_va = prepare_texts(va)
txt_te = prepare_texts(te)

X_deb_tr  = encode_texts(txt_tr)
X_deb_val = encode_texts(txt_va)
X_deb_te  = encode_texts(txt_te)

np.save(MODEL_DIR / "X_text_train_deberta.npy", X_deb_tr)
np.save(MODEL_DIR / "X_text_val_deberta.npy", X_deb_val)
np.save(MODEL_DIR / "X_text_test_deberta.npy", X_deb_te)

print("Shapes:", X_deb_tr.shape, X_deb_val.shape, X_deb_te.shape)
print("Saved to:", MODEL_DIR)

# ===========================================
# STEP 4 (FAST) â€” IMAGE EMBEDDINGS FROM CACHE (OpenCLIP ViT-L/14)
# ===========================================
!pip -q install open_clip_torch pillow tqdm

import os, gc, json, numpy as np, pandas as pd, torch
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torch.nn.functional as F
import open_clip
from torch.utils.data import Dataset, DataLoader

# ---------------- Config ----------------
SEED       = 42
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"

BASE_DIR   = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR  = BASE_DIR / "splits"
EMB_DIR    = BASE_DIR / "embeddings" / "clip_vitl14_fast"
IMG_DIR    = BASE_DIR / "images_cache"
EMB_DIR.mkdir(parents=True, exist_ok=True)

MODEL_ARCH = "ViT-L-14"
PRETRAINED = "openai"
BATCH_SIZE = 192          # tune up to 224â€“256 if VRAM allows
NUM_WORKERS = 8
torch.manual_seed(SEED)
torch.backends.cudnn.benchmark = True    # allow autotune
torch.backends.cudnn.deterministic = False

# ---------------- Load splits ----------------
tr = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
va = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
te = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

# ---------------- Model ----------------
print("Loading OpenCLIP:", MODEL_ARCH, PRETRAINED)
model, _, preprocess = open_clip.create_model_and_transforms(MODEL_ARCH, pretrained=PRETRAINED)
model = model.to(DEVICE).eval().half()

# ---------------- Dataset ----------------
class ImgDataset(Dataset):
    def __init__(self, df, cache_dir, preprocess):
        self.paths = [(cache_dir / (str(row["image_link"]) \
                        if str(row["image_link"]).endswith(".jpg") else
                        (hashlib.md5(str(row["image_link"]).encode()).hexdigest() + ".jpg")))
                      for _, row in df.iterrows()]
        self.preprocess = preprocess

    def __len__(self): return len(self.paths)

    def __getitem__(self, i):
        p = self.paths[i]
        if not p.exists():
            img = Image.new("RGB", (224,224), (255,255,255))
        else:
            try:
                img = Image.open(p).convert("RGB")
            except Exception:
                img = Image.new("RGB", (224,224), (255,255,255))
        return self.preprocess(img)

def make_loader(df):
    return DataLoader(ImgDataset(df, IMG_DIR, preprocess),
                      batch_size=BATCH_SIZE,
                      num_workers=NUM_WORKERS,
                      pin_memory=True,
                      shuffle=False,
                      prefetch_factor=2)

@torch.no_grad()
def encode_split(df, split_name):
    dl = make_loader(df)
    sample = next(iter(dl)).to(DEVICE, dtype=torch.float16)
    d = model.encode_image(sample).shape[-1]; del sample
    mmap_path = EMB_DIR / f"img_emb_{split_name}_vitl14_fast.mmap"
    emb = np.memmap(mmap_path, dtype="float32", mode="w+", shape=(len(df), d))

    off = 0
    for batch in tqdm(dl, desc=f"Encoding {split_name}", total=len(dl)):
        batch = batch.to(DEVICE, dtype=torch.float16, non_blocking=True)
        z = model.encode_image(batch)
        z = F.normalize(z.float(), dim=-1).cpu().numpy()
        emb[off:off+z.shape[0]] = z
        off += z.shape[0]
    del emb; gc.collect()
    arr = np.memmap(mmap_path, dtype="float32", mode="r", shape=(len(df), d))
    np.save(EMB_DIR / f"X_img_{split_name}_clip.npy", np.asarray(arr, dtype="float32"))
    print(f"âœ… Saved X_img_{split_name}_clip.npy  â†’ {arr.shape}")
    return arr

# ---------------- Encode all splits ----------------
X_img_tr  = encode_split(tr, "tr")
X_img_val = encode_split(va, "val")
X_img_te  = encode_split(te, "te")

# ---------------- Log meta ----------------
log = {
    "arch": MODEL_ARCH,
    "pretrained": PRETRAINED,
    "batch_size": BATCH_SIZE,
    "num_workers": NUM_WORKERS,
    "device": DEVICE,
    "n_train": len(tr),
    "n_val": len(va),
    "n_test": len(te),
}
with open(EMB_DIR / "image_embed_log.json", "w") as f:
    json.dump(log, f, indent=2)
print(" Fast embedding run complete and logged.")

# ===========================================
# COLAB SETUP â€” Mount Google Drive
# ===========================================
from google.colab import drive
drive.mount('/content/drive')

# Quick sanity check: your project root should exist
import os
base_path = "/content/drive/MyDrive/smart_product_pricing_final"
if not os.path.exists(base_path):
    raise FileNotFoundError(f"âŒ Project folder not found at {base_path}. Please check the path.")
else:
    print(f"Drive mounted successfully. Project root located at:\n{base_path}")

# ===========================================
# CELL 1 â€” Setup, Imports, Paths, Config, Utils  (VAE + Pseudo-Label Edition)
# ===========================================
!pip -q install numpy pandas scikit-learn scipy joblib lightgbm xgboost catboost tqdm pyod

import os, sys, json, math, time, gc, random
from pathlib import Path
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
from tqdm import tqdm
from joblib import load, dump

# Optional torch seeding if available
try:
    import torch
    TORCH_AVAILABLE = True
except Exception:
    TORCH_AVAILABLE = False

# ---------- Global Config ----------
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
if TORCH_AVAILABLE:
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)

# ---------- Directories ----------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")

DATA_DIR        = BASE_DIR / "dataset"
SPLIT_DIR       = BASE_DIR / "splits"
EMB_DIR         = BASE_DIR / "embeddings"
E5_DIR          = EMB_DIR   / "e5_large"
CLIP_DIR        = EMB_DIR   / "clip_vitl14_fast"
DINO_DIR        = EMB_DIR   / "dinov2_base"
DEBERTA_DIR     = BASE_DIR  / "models" / "deberta_v3_price_reg"
EXPERIMENTS_DIR = BASE_DIR  / "experiments"
IMAGES_DIR       = BASE_DIR / "images"
IMAGES_CACHE_DIR = BASE_DIR / "images_cache"

# ---------- New experiment tag ----------
EXP_TAG = "exp_20251013_vae_pseudolabel_v1"     # fixed tag for this phase
OUT_DIR = EXPERIMENTS_DIR / EXP_TAG
OUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------- Subfolders for clarity ----------
for sub in ["pseudo", "vae", "models", "meta"]:
    (OUT_DIR / sub).mkdir(exist_ok=True)

# ---------- Target transform ----------
USE_LOG1P = True

# ---------- PCA dims per modality ----------
PCA_DIMS = dict(e5=128, clip=128, deberta=128, dino=128)

# ---------- Feature toggles ----------
USE_E5, USE_DEBERTA, USE_CLIP, USE_DINO = True, True, True, False
USE_CROSSFE, USE_PRIORS, USE_KNN_FEATS, USE_REGIMES = True, False, False, False

# ---------- Pseudo-label constants ----------
PSEUDO_CONF_THRESHOLD = 0.05   # log-space std threshold for high confidence
PSEUDO_WEIGHT = 0.5            # training weight for pseudo-labeled data

# ---------- Utilities ----------
def ensure_exists(path: Path):
    if not path.exists():
        raise FileNotFoundError(f"Missing expected path: {path}")

def smape(y_true, y_pred):
    """Compute SMAPE in price space."""
    y_true, y_pred = np.asarray(y_true, float), np.asarray(y_pred, float)
    denom = (np.abs(y_true) + np.abs(y_pred))
    mask = denom > 0
    out = np.zeros_like(y_true, float)
    out[mask] = 200.0 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]
    return np.mean(out)

def to_log(y):
    return np.log1p(np.clip(y, 0, None)) if USE_LOG1P else np.log(np.clip(y, 1e-12, None))

def from_log(y_log):
    return np.expm1(y_log) if USE_LOG1P else np.exp(y_log)

def save_json(obj, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w") as f:
        json.dump(obj, f, indent=2)

def load_npy(path: Path):
    ensure_exists(path)
    return np.load(path)

def log(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}")

# ---------- Sanity checks ----------
_expected = [
    DATA_DIR / "train.csv",
    DATA_DIR / "test.csv",
    SPLIT_DIR / "df_train_processed.parquet",
    SPLIT_DIR / "df_val_processed.parquet",
    SPLIT_DIR / "y_train.npy",
    SPLIT_DIR / "y_val.npy",
]
for p in _expected:
    ensure_exists(p)

if USE_E5:      ensure_exists(E5_DIR / "pca_e5.joblib")
if USE_CLIP:    ensure_exists(CLIP_DIR / "pca_clip.joblib")
if USE_DEBERTA: ensure_exists(DEBERTA_DIR / "pca_deb.joblib")
if USE_DINO:    ensure_exists(DINO_DIR / "dino_img_pca.joblib")

# ---------- Save config snapshot ----------
run_config = dict(
    seed=SEED,
    exp_tag=EXP_TAG,
    base_dir=str(BASE_DIR),
    pca_dims=PCA_DIMS,
    toggles=dict(
        USE_E5=USE_E5, USE_DEBERTA=USE_DEBERTA, USE_CLIP=USE_CLIP,
        USE_DINO=USE_DINO, USE_CROSSFE=USE_CROSSFE, USE_PRIORS=USE_PRIORS,
        USE_KNN_FEATS=USE_KNN_FEATS, USE_REGIMES=USE_REGIMES,
    ),
    pseudo=dict(threshold=PSEUDO_CONF_THRESHOLD, weight=PSEUDO_WEIGHT),
    target_transform="log1p" if USE_LOG1P else "log",
)
save_json(run_config, OUT_DIR / "config_snapshot.json")

log(f"Environment ready for pseudo-label + VAE phase.\nOUT_DIR = {OUT_DIR}")

import os, json
from pathlib import Path

EXPERIMENTS_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final/experiments")

def list_dir_structure(base_dir, depth=2):
    tree = {}
    for root, dirs, files in os.walk(base_dir):
        level = root.replace(str(base_dir), '').count(os.sep)
        if level < depth:
            subtree = {d: {} for d in dirs}
            for f in files:
                subtree[f] = None
            parent = tree
            path_parts = root.replace(str(base_dir), '').strip(os.sep).split(os.sep)
            for part in path_parts[:-1]:
                parent = parent.get(part, {})
            parent[path_parts[-1] if path_parts else base_dir.name] = subtree
    return tree

structure = list_dir_structure(EXPERIMENTS_DIR, depth=2)
print(json.dumps(structure, indent=2)[:4000])  # truncate output if very long

# ===========================================
# CELL 2 â€” Load splits, targets, PCA embeddings + PCA-latent + Pseudo-Labels
# ===========================================
import numpy as np, pandas as pd, json, gc
from joblib import load
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# ---------- Load preprocessed splits ----------
df_tr  = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
df_val = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
df_te  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

y_tr      = np.load(SPLIT_DIR / "y_train.npy")
y_val     = np.load(SPLIT_DIR / "y_val.npy")
y_tr_log  = np.load(SPLIT_DIR / "y_train_log.npy")
y_val_log = np.load(SPLIT_DIR / "y_val_log.npy")
log(f"Loaded splits | Train: {len(df_tr):,}  Val: {len(df_val):,}  Test: {len(df_te):,}")

# ---------- Helper to load PCA blocks ----------
def load_pca_block(modality: str):
    if modality == "e5" and USE_E5:
        return (
            np.load(E5_DIR / "X_tr_e5_pca128.npy"),
            np.load(E5_DIR / "X_val_e5_pca128.npy"),
            np.load(E5_DIR / "X_te_e5_pca128.npy"),
        )
    if modality == "deberta" and USE_DEBERTA:
        return (
            np.load(DEBERTA_DIR / "X_tr_deb_pca128.npy"),
            np.load(DEBERTA_DIR / "X_val_deb_pca128.npy"),
            np.load(DEBERTA_DIR / "X_te_deb_pca128.npy"),
        )
    if modality == "clip" and USE_CLIP:
        return (
            np.load(CLIP_DIR / "X_tr_clip_pca128.npy"),
            np.load(CLIP_DIR / "X_val_clip_pca128.npy"),
            np.load(CLIP_DIR / "X_te_clip_pca128.npy"),
        )
    if modality == "dino" and USE_DINO:
        return (
            np.load(DINO_DIR / "X_img_tr_dino_pca128.npy"),
            np.load(DINO_DIR / "X_img_val_dino_pca128.npy"),
            np.load(DINO_DIR / "X_img_te_dino_pca128.npy"),
        )
    return None, None, None

# ---------- Fuse embeddings ----------
X_blocks_tr, X_blocks_val, X_blocks_te, block_names = [], [], [], []
if USE_E5:
    X_tr_e5, X_val_e5, X_te_e5 = load_pca_block("e5")
    X_blocks_tr += [X_tr_e5]; X_blocks_val += [X_val_e5]; X_blocks_te += [X_te_e5]
    block_names.append("e5_pca128"); log(f"E5 loaded â†’ {X_tr_e5.shape}")
if USE_DEBERTA:
    X_tr_deb, X_val_deb, X_te_deb = load_pca_block("deberta")
    X_blocks_tr += [X_tr_deb]; X_blocks_val += [X_val_deb]; X_blocks_te += [X_te_deb]
    block_names.append("deb_pca128"); log(f"DeBERTa loaded â†’ {X_tr_deb.shape}")
if USE_CLIP:
    X_tr_clip, X_val_clip, X_te_clip = load_pca_block("clip")
    X_blocks_tr += [X_tr_clip]; X_blocks_val += [X_val_clip]; X_blocks_te += [X_te_clip]
    block_names.append("clip_pca128"); log(f"CLIP loaded â†’ {X_tr_clip.shape}")
if USE_DINO:
    X_tr_dino, X_val_dino, X_te_dino = load_pca_block("dino")
    X_blocks_tr += [X_tr_dino]; X_blocks_val += [X_val_dino]; X_blocks_te += [X_te_dino]
    block_names.append("dino_pca128"); log(f"DINOv2 loaded â†’ {X_tr_dino.shape}")

X_tr_pca  = np.concatenate(X_blocks_tr,  axis=1)
X_val_pca = np.concatenate(X_blocks_val, axis=1)
X_te_pca  = np.concatenate(X_blocks_te,  axis=1)
log(f"Fused PCA shapes â€” Train {X_tr_pca.shape} | Val {X_val_pca.shape} | Test {X_te_pca.shape}")

# ---------- Add scaled pack features ----------
pack_cols = [c for c in ["pack_value", "pack_count"] if c in df_tr.columns]
candidate_scalers = [
    EXPERIMENTS_DIR / "fusion_multi_gbm" / "scaler_pack_num.joblib",
    EXPERIMENTS_DIR / "fusion_lgbm_quicktest" / "scaler_pack_num.joblib",
    EXPERIMENTS_DIR / "fusion_mlp" / "scaler_pack_num.joblib",
]
scaler_path = next((c for c in candidate_scalers if c.exists()), None)
if pack_cols and scaler_path:
    scaler_pack = load(scaler_path)
    X_tr_pack  = scaler_pack.transform(df_tr[pack_cols].fillna(0).values)
    X_val_pack = scaler_pack.transform(df_val[pack_cols].fillna(0).values)
    X_te_pack  = scaler_pack.transform(df_te[pack_cols].fillna(0).values)
    X_tr_fused = np.concatenate([X_tr_pca, X_tr_pack], axis=1)
    X_val_fused= np.concatenate([X_val_pca, X_val_pack], axis=1)
    X_te_fused = np.concatenate([X_te_pca, X_te_pack], axis=1)
    block_names.append("pack_scaled")
    log(f"Pack features added ({pack_cols}) â†’ scaler: {scaler_path.name}")
else:
    X_tr_fused, X_val_fused, X_te_fused = X_tr_pca, X_val_pca, X_te_pca
    log("âš ï¸ No pack scaler found or pack columns missing; proceeding without pack features.")

# =====================================================
#  PSEUDO-LABELING (Latest ensemble: 124613_fe_v2_stack)
# =====================================================
BASE_PREV_EXP = EXPERIMENTS_DIR / "exp_20251013_124613_fe_v2_stack"
pred_path = BASE_PREV_EXP / "pred_test_lgbmeta.npy"

if pred_path.exists():
    preds_mean = np.log1p(np.clip(np.load(pred_path), 0, None))
    preds_std = np.zeros_like(preds_mean)  # single model â†’ zero variance
    log(f"Loaded pseudo-label base predictions from: {pred_path.name}")
else:
    log("âš ï¸ Fallback â†’ using submission_final_stack_lgbmeta_822pm.csv")
    csv_path = BASE_PREV_EXP / "submission_final_stack_lgbmeta_822pm.csv"
    df_csv = pd.read_csv(csv_path)
    preds_mean = np.log1p(df_csv["price"].values)
    preds_std = np.zeros_like(preds_mean)

mask_conf = preds_std < PSEUDO_CONF_THRESHOLD
pseudo_df = pd.DataFrame({
    "sample_id": df_te["sample_id"],
    "pseudo_price": np.expm1(preds_mean),
    "log_pseudo_price": preds_mean,
    "confidence": 1.0
})
pseudo_df = pseudo_df.loc[mask_conf].reset_index(drop=True)
(OUT_DIR / "pseudo").mkdir(exist_ok=True)
pseudo_df.to_csv(OUT_DIR / "pseudo" / "pseudo_test.csv", index=False)
log(f"Pseudo-labeled subset created â†’ {len(pseudo_df):,} samples (from {pred_path.name}).")

# =====================================================
#  PCA-BASED LATENT FEATURES + RECONSTRUCTION ERROR
# =====================================================
log("Using PCA-based latent features as VAE alternative")

scaler = StandardScaler()
X_tr_scaled = scaler.fit_transform(X_tr_fused)
X_val_scaled = scaler.transform(X_val_fused)
X_te_scaled = scaler.transform(X_te_fused)

pca_latent = PCA(n_components=12, random_state=42)
Z_tr = pca_latent.fit_transform(X_tr_scaled)
Z_val = pca_latent.transform(X_val_scaled)
Z_te = pca_latent.transform(X_te_scaled)
log(f"PCA latent features created â†’ {Z_tr.shape[1]} comps, explained variance: {pca_latent.explained_variance_ratio_.sum():.3f}")

X_tr_recon = pca_latent.inverse_transform(Z_tr)
X_val_recon = pca_latent.inverse_transform(Z_val)
X_te_recon = pca_latent.inverse_transform(Z_te)
recon_err_tr = np.sqrt(((X_tr_scaled - X_tr_recon) ** 2).mean(axis=1, keepdims=True))
recon_err_val = np.sqrt(((X_val_scaled - X_val_recon) ** 2).mean(axis=1, keepdims=True))
recon_err_te = np.sqrt(((X_te_scaled - X_te_recon) ** 2).mean(axis=1, keepdims=True))
log(f"Reconstruction error â†’ Train mean {recon_err_tr.mean():.4f}, Val mean {recon_err_val.mean():.4f}")

X_tr_vae  = np.concatenate([X_tr_fused, Z_tr, recon_err_tr], axis=1)
X_val_vae = np.concatenate([X_val_fused, Z_val, recon_err_val], axis=1)
X_te_vae  = np.concatenate([X_te_fused, Z_te, recon_err_te], axis=1)
log(f"Enhanced features â†’ new dims {X_tr_vae.shape[1]}")

(OUT_DIR / "vae").mkdir(exist_ok=True)
np.save(OUT_DIR / "vae" / "X_tr_vae.npy",  X_tr_vae)
np.save(OUT_DIR / "vae" / "X_val_vae.npy", X_val_vae)
np.save(OUT_DIR / "vae" / "X_te_vae.npy",  X_te_vae)
np.save(OUT_DIR / "y_tr_log.npy", y_tr_log)
np.save(OUT_DIR / "y_val_log.npy", y_val_log)

save_json({
    "method": "PCA_based_alternative",
    "blocks": block_names + ["pca_latent12","pca_recon_err"],
    "train_shape": list(X_tr_vae.shape),
    "val_shape": list(X_val_vae.shape),
    "test_shape": list(X_te_vae.shape),
    "latent_explained_variance": float(pca_latent.explained_variance_ratio_.sum()),
    "pseudo_labeled": len(pseudo_df),
    "base_exp": str(BASE_PREV_EXP),
    "note": "Pseudo-labels from pred_test_lgbmeta.npy"
}, OUT_DIR / "vae" / "vae_summary.json")

gc.collect()
log("âœ… Fused + PCA-latent features and pseudo-labels saved (latest stack baseline).")

import os
from pathlib import Path

DINO_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final/embeddings/dinov2_base")
for f in os.listdir(DINO_DIR):
    if "dino" in f.lower():
        print(f)

# ===========================================
# CELL 2C â€” Fix DINO alignment via split indices
# ===========================================
import numpy as np

# Expected sizes
n_tr, n_val, n_te = len(df_tr), len(df_val), len(df_te)
n_labeled_total = n_tr + n_val

# Load split indices
train_idx = np.load(SPLIT_DIR / "train_idx.npy")
val_idx   = np.load(SPLIT_DIR / "val_idx.npy")

log(f"Split indices â†’ train_idx: {train_idx.shape}, val_idx: {val_idx.shape}")

# Helper to report shapes
def _sh(x):
    return None if x is None else tuple(x.shape)

# Try 1: Use a single master DINO array (most likely X_img_tr_dino_pca128.npy)
X_dino_master = None
master_paths = [
    DINO_DIR / "X_img_tr_dino_pca128.npy",   # often contains all labeled rows in original order
    DINO_DIR / "X_img_tr_dino.npy",          # raw (pre-PCA) fallback (not expected here)
]

for mp in master_paths:
    if mp.exists():
        tmp = np.load(mp)
        log(f"Found potential DINO master â†’ {mp.name} with shape {tmp.shape}")
        if tmp.shape[0] >= n_labeled_total:
            X_dino_master = tmp
            break

# If we didn't find a large enough master, try concatenating provided tr/val files
if X_dino_master is None:
    concat_candidates = []
    for p in [DINO_DIR / "X_img_tr_dino_pca128.npy", DINO_DIR / "X_img_val_dino_pca128.npy"]:
        if p.exists():
            concat_candidates.append(np.load(p))
            log(f"Concat candidate found: {p.name} {_sh(concat_candidates[-1])}")
    if concat_candidates:
        X_dino_master = np.concatenate(concat_candidates, axis=0)
        log(f"Built concatenated DINO master with shape {X_dino_master.shape} "
            f"(âš ï¸ assumes concat order matches original labeled order)")

# Safety check
if X_dino_master is None:
    raise FileNotFoundError("Could not locate a suitable DINO master embedding array to realign.")

# If master is larger than labeled total (e.g., includes holdouts), trim by first n_labeled_total
if X_dino_master.shape[0] > n_labeled_total:
    log(f"Trimming DINO master from {X_dino_master.shape[0]} to first {n_labeled_total} rows to match labeled total.")
    X_dino_master = X_dino_master[:n_labeled_total]

assert X_dino_master.shape[0] == n_labeled_total, \
    f"Master DINO rows ({X_dino_master.shape[0]}) != train+val ({n_labeled_total})."

# Re-slice using split indices
X_tr_dino_aligned  = X_dino_master[train_idx]
X_val_dino_aligned = X_dino_master[val_idx]

log(f"DINO aligned shapes â†’ Train {X_tr_dino_aligned.shape}, Val {X_val_dino_aligned.shape} (expected {n_tr}, {n_val})")

# Replace any previously loaded misaligned DINO blocks in-memory
X_tr_dino, X_val_dino = X_tr_dino_aligned, X_val_dino_aligned
# Test block should already be correct:
X_te_dino = np.load(DINO_DIR / "X_img_te_dino_pca128.npy")

# Optional: persist aligned copies for reuse
np.save(OUT_DIR / "X_tr_dino_pca128_aligned.npy",  X_tr_dino)
np.save(OUT_DIR / "X_val_dino_pca128_aligned.npy", X_val_dino)

log("âœ… DINO train/val embeddings realigned. You can now re-run CELL 3 to build cross-modal features safely.")

# ===========================================
# CELL 3 â€” Cross-Modal + Tier-1 Feature Fusion (v3 + DINO Enabled)
# ===========================================
import re
import numpy as np
import pandas as pd
from numpy.linalg import norm

# ===========================================================
# ENABLE DINO IN FUSION
# ===========================================================
USE_DINO = True
log("âœ… DINO feature integration enabled â€” cross-modal interactions will include DINO.")

# ------------------ utilities ------------------
def safe_cosine(a, b):
    denom = (norm(a, axis=1) * norm(b, axis=1)) + 1e-8
    return np.sum(a * b, axis=1) / denom

def make_interactions(A_tr, A_val, A_te, B_tr, B_val, B_te, name):
    """Return [cos, normA, normB, ratio] blocks for each split."""
    cos_tr  = safe_cosine(A_tr,  B_tr)
    cos_val = safe_cosine(A_val, B_val)
    cos_te  = safe_cosine(A_te,  B_te)

    na_tr, nb_tr  = norm(A_tr,  axis=1), norm(B_tr,  axis=1)
    na_val, nb_val= norm(A_val, axis=1), norm(B_val, axis=1)
    na_te, nb_te  = norm(A_te,  axis=1), norm(B_te,  axis=1)

    ratio_tr  = na_tr  / (nb_tr  + 1e-8)
    ratio_val = na_val / (nb_val + 1e-8)
    ratio_te  = na_te  / (nb_te  + 1e-8)

    X_tr  = np.vstack([cos_tr,  na_tr,  nb_tr,  ratio_tr]).T
    X_val = np.vstack([cos_val, na_val, nb_val, ratio_val]).T
    X_te  = np.vstack([cos_te,  na_te,  nb_te,  ratio_te]).T
    names = [f"{name}_cos", f"{name}_normA", f"{name}_normB", f"{name}_ratio"]
    return X_tr, X_val, X_te, names


# ------------------ build aligned modality pairs ------------------
n_tr, n_val, n_te = len(df_tr), len(df_val), len(df_te)
pairs = []

# Standard cross-modal pairs
if USE_E5 and USE_CLIP:
    pairs.append(("e5_clip", X_tr_e5, X_val_e5, X_te_e5, X_tr_clip, X_val_clip, X_te_clip))
if USE_DEBERTA and USE_CLIP:
    pairs.append(("deb_clip", X_tr_deb, X_val_deb, X_te_deb, X_tr_clip, X_val_clip, X_te_clip))

# DINO-enhanced cross-modal pairs
if USE_DINO:
    if USE_E5:
        pairs.append(("dino_e5", X_tr_dino, X_val_dino, X_te_dino, X_tr_e5, X_val_e5, X_te_e5))
    if USE_DEBERTA:
        pairs.append(("dino_deb", X_tr_dino, X_val_dino, X_te_dino, X_tr_deb, X_val_deb, X_te_deb))
    if USE_CLIP:
        pairs.append(("dino_clip", X_tr_dino, X_val_dino, X_te_dino, X_tr_clip, X_val_clip, X_te_clip))

log(f"Building cross-modal features for {len(pairs)} pair(s): {[p[0] for p in pairs]}")

# ------------------ compute all cross-modal features ------------------
X_tr_cross_list, X_val_cross_list, X_te_cross_list, cross_names = [], [], [], []
for name, A_tr, A_val, A_te, B_tr, B_val, B_te in pairs:
    tr_blk, val_blk, te_blk, names = make_interactions(A_tr, A_val, A_te, B_tr, B_val, B_te, name)
    X_tr_cross_list.append(tr_blk)
    X_val_cross_list.append(val_blk)
    X_te_cross_list.append(te_blk)
    cross_names += names

if X_tr_cross_list:
    X_tr_cross  = np.concatenate(X_tr_cross_list,  axis=1)
    X_val_cross = np.concatenate(X_val_cross_list, axis=1)
    X_te_cross  = np.concatenate(X_te_cross_list,  axis=1)
else:
    X_tr_cross  = np.zeros((n_tr,0), dtype=np.float32)
    X_val_cross = np.zeros((n_val,0), dtype=np.float32)
    X_te_cross  = np.zeros((n_te,0), dtype=np.float32)

# ------------------ pack Ã— cosine interaction ------------------
pack_cols = [c for c in ["pack_value","pack_count"] if c in df_tr.columns]
if pack_cols:
    log("Adding pack-interaction featureâ€¦")
    pv_tr  = np.log1p(np.clip(df_tr[pack_cols[0]].values,  0, None))
    pv_val = np.log1p(np.clip(df_val[pack_cols[0]].values, 0, None))
    pv_te  = np.log1p(np.clip(df_te[pack_cols[0]].values,  0, None))

    cos_tr  = X_tr_cross[:,0] if X_tr_cross.shape[1] else np.zeros(n_tr)
    cos_val = X_val_cross[:,0] if X_val_cross.shape[1] else np.zeros(n_val)
    cos_te  = X_te_cross[:,0] if X_te_cross.shape[1] else np.zeros(n_te)

    cross_mul_tr  = (cos_tr  * pv_tr)[:,None]
    cross_mul_val = (cos_val * pv_val)[:,None]
    cross_mul_te  = (cos_te  * pv_te)[:,None]

    X_tr_cross  = np.concatenate([X_tr_cross,  cross_mul_tr],  axis=1)
    X_val_cross = np.concatenate([X_val_cross, cross_mul_val], axis=1)
    X_te_cross  = np.concatenate([X_te_cross,  cross_mul_te],  axis=1)
    cross_names += ["cos_first_x_logpack"]

log(f"Cross-modal shapes â†’ Train {X_tr_cross.shape}, Val {X_val_cross.shape}, Test {X_te_cross.shape}")

# ===========================================================
# TIER-1 STRUCTURED + TEXTUAL FEATURES
# ===========================================================
def extract_pack_features(df):
    out = pd.DataFrame(index=df.index)
    out["pack_count"] = df["pack_count"].clip(0,1000)
    out["pack_value"] = df["pack_value"].clip(0,10000)
    out["is_single_pack"] = (out["pack_count"]==1).astype(int)
    out["pack_count_x_value"] = out["pack_count"]*out["pack_value"]
    out["log_count_div_value"] = np.log1p(out["pack_count"]/(out["pack_value"]+1e-6))
    return out

num_pattern = re.compile(r"(\d+(?:\.\d+)?)\s?(kg|g|l|ml|oz|inch|cm|mm|w|v|mah|gb|tb|l)?", re.I)
def harvest_numbers(text):
    nums=[float(x[0]) for x in num_pattern.findall(str(text))]
    return [len(nums), np.mean(nums) if nums else 0, np.std(nums) if nums else 0, np.max(nums) if nums else 0]

def numeric_features(df):
    feats=np.vstack(df["item_description"].fillna("").apply(harvest_numbers))
    return pd.DataFrame(feats,columns=["n_numbers","num_mean","num_std","num_max"],index=df.index)

def text_complexity(df):
    txt=df["item_description"].fillna("")
    out=pd.DataFrame(index=df.index)
    out["desc_len"]=txt.str.len()
    out["desc_words"]=txt.str.split().apply(len)
    out["uppercase_ratio"]=txt.apply(lambda s: sum(1 for c in s if c.isupper())/(len(s)+1e-6))
    out["punct_density"]=txt.apply(lambda s: sum(c in ".,;:!?" for c in s)/(len(s)+1e-6))
    out["digit_ratio"]=txt.apply(lambda s: sum(c.isdigit() for c in s)/(len(s)+1e-6))
    return out

common_kw=["premium","organic","refill","combo","mini","xl","wireless","stainless","imported","pack of","set of"]
def keyword_flags(text):
    t=str(text).lower();
    return [int(k in t) for k in common_kw]

def brand_keyword_features(df):
    flags=np.vstack(df["item_name"].fillna("").apply(keyword_flags))
    out=pd.DataFrame(flags,columns=[f"kw_{k.replace(' ','_')}" for k in common_kw],index=df.index)
    brands=df["item_name"].str.extract(r"^([A-Z][A-Za-z0-9&\-]+)")
    out["brand_token"]=brands[0].fillna("UNKNOWN")
    vc=out["brand_token"].value_counts()
    out["brand_freq"]=out["brand_token"].map(vc)
    out.drop(columns=["brand_token"],inplace=True)
    return out

def build_tier1(df):
    return pd.concat([extract_pack_features(df),
                      numeric_features(df),
                      text_complexity(df),
                      brand_keyword_features(df)], axis=1)

tier1_tr  = build_tier1(df_tr)
tier1_val = build_tier1(df_val)
tier1_te  = build_tier1(df_te)
log(f"Tier-1 engineered features â†’ Train {tier1_tr.shape}, Val {tier1_val.shape}, Test {tier1_te.shape}")

# ===========================================================
# FINAL FUSION (v3 + DINO): fused PCA + cross-modal + tier-1
# ===========================================================
X_tr_final_v3  = np.concatenate([X_tr_fused, X_tr_cross, tier1_tr.values], axis=1)
X_val_final_v3 = np.concatenate([X_val_fused, X_val_cross, tier1_val.values], axis=1)
X_te_final_v3  = np.concatenate([X_te_fused, X_te_cross, tier1_te.values], axis=1)

np.save(OUT_DIR / "X_tr_final_v3_tier1.npy",  X_tr_final_v3)
np.save(OUT_DIR / "X_val_final_v3_tier1.npy", X_val_final_v3)
np.save(OUT_DIR / "X_te_final_v3_tier1.npy",  X_te_final_v3)

save_json({
    "cross_features": cross_names,
    "tier1_cols": list(tier1_tr.columns),
    "train_shape": X_tr_final_v3.shape,
    "val_shape":   X_val_final_v3.shape,
    "test_shape":  X_te_final_v3.shape,
    "dino_used": USE_DINO
}, OUT_DIR / "cross_tier1_summary_v3.json")

log("âœ… Cross-modal + Tier-1 feature fusion complete (v3 + DINO enabled).")

# ===========================================
# CELL 4 â€” LightGBM (K-Fold CV) on v3_tier1_pseudolabel features + importances
# ===========================================
!pip -q install lightgbm pandas numpy scikit-learn joblib

import json, gc
import numpy as np
import pandas as pd
from pathlib import Path
from joblib import dump
import lightgbm as lgb
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error

# ---------- Load features ----------
X_tr_final  = np.load(OUT_DIR / "X_tr_final_v3_tier1.npy")
X_val_final = np.load(OUT_DIR / "X_val_final_v3_tier1.npy")
X_te_final  = np.load(OUT_DIR / "X_te_final_v3_tier1.npy")
y_tr_log    = np.load(OUT_DIR / "y_tr_log.npy")
y_val_log   = np.load(OUT_DIR / "y_val_log.npy")

# Combine train+val for CV
X_full = np.concatenate([X_tr_final, X_val_final], axis=0)
y_full = np.concatenate([y_tr_log, y_val_log], axis=0)
n_train = len(y_full)

log(f"K-Fold training on combined train+val set â†’ {X_full.shape}, test {X_te_final.shape}")

# ---------- Metrics ----------
def smape(y_true, y_pred):
    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)
    denom = np.abs(y_true) + np.abs(y_pred)
    mask = denom > 0
    out = np.zeros_like(y_true, dtype=np.float64)
    out[mask] = 200.0 * np.abs(y_pred[mask] - y_true[mask]) / denom[mask]
    return float(np.mean(out))

def inv_log(ylog): return np.expm1(ylog)

# ---------- Feature names ----------
feat_names = None
try:
    meta = json.load(open(OUT_DIR / "cross_tier1_summary_v3.json"))
    cross_names = meta.get("cross_features", [])
    tier1_cols  = meta.get("tier1_cols", [])
    n_total = X_full.shape[1]
    n_cross = len(cross_names)
    n_tier1 = len(tier1_cols)
    n_fused = n_total - n_cross - n_tier1
    fused_names = [f"fused_{i}" for i in range(n_fused)]
    feat_names = fused_names + cross_names + tier1_cols
    if len(feat_names) != n_total:
        feat_names = [f"f{i}" for i in range(n_total)]
        log("âš ï¸ Feature count mismatch â€” reverted to generic names.")
except Exception as e:
    log(f"âš ï¸ Could not load feature names ({e}). Using generic IDs.")
    feat_names = [f"f{i}" for i in range(X_full.shape[1])]

# ---------- LightGBM config ----------
SEED = 42
lgb_params = {
    "objective": "regression_l1",
    "metric": "mae",
    "learning_rate": 0.03,
    "num_leaves": 31,
    "min_data_in_leaf": 400,
    "feature_fraction": 0.55,
    "bagging_fraction": 0.7,
    "bagging_freq": 1,
    "lambda_l1": 8.0,
    "lambda_l2": 20.0,
    "max_depth": -1,
    "verbosity": -1,
    "seed": SEED
}

# ---------- Cross-validation ----------
N_FOLDS = 5
kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)

oof_preds = np.zeros(n_train)
test_preds = np.zeros((X_te_final.shape[0], N_FOLDS))
fold_scores = []
importance_dfs = []

log(f"Starting {N_FOLDS}-fold LightGBM training on v3_tier1_pseudolabel featuresâ€¦")

for fold, (tr_idx, val_idx) in enumerate(kf.split(X_full, y_full), 1):
    log(f"\n===== Fold {fold}/{N_FOLDS} =====")
    X_tr, X_val = X_full[tr_idx], X_full[val_idx]
    y_tr, y_val = y_full[tr_idx], y_full[val_idx]

    ds_tr  = lgb.Dataset(X_tr, label=y_tr, feature_name=feat_names, free_raw_data=False)
    ds_val = lgb.Dataset(X_val, label=y_val, reference=ds_tr, feature_name=feat_names, free_raw_data=False)

    booster = lgb.train(
        params=lgb_params,
        train_set=ds_tr,
        valid_sets=[ds_tr, ds_val],
        valid_names=["train","val"],
        num_boost_round=5000,
        callbacks=[lgb.early_stopping(200), lgb.log_evaluation(250)]
    )

    val_pred = booster.predict(X_val, num_iteration=booster.best_iteration)
    oof_preds[val_idx] = val_pred
    test_preds[:, fold-1] = booster.predict(X_te_final, num_iteration=booster.best_iteration)

    y_val_price  = inv_log(y_val)
    pred_val_price = inv_log(val_pred)
    val_smape = smape(y_val_price, pred_val_price)
    val_mae   = mean_absolute_error(y_val_price, pred_val_price)
    fold_scores.append(val_smape)
    log(f"Fold {fold} â†’ SMAPE: {val_smape:.4f} | MAE: {val_mae:.4f}")

    # collect feature importance for this fold
    gain_imp  = booster.feature_importance(importance_type="gain")
    split_imp = booster.feature_importance(importance_type="split")
    fold_imp = pd.DataFrame({
        "feature": feat_names,
        "gain": gain_imp,
        "split": split_imp,
        "fold": fold
    })
    importance_dfs.append(fold_imp)

    dump(booster, OUT_DIR / f"lgbm_fold{fold}_v3_tier1_pseudolabel.joblib")
    del booster, ds_tr, ds_val
    gc.collect()

# ---------- Aggregate results ----------
mean_smape = np.mean(fold_scores)
log(f"\nOOF SMAPE (mean across folds): {mean_smape:.4f}")

# ---------- Predictions ----------
oof_price  = inv_log(oof_preds)
test_price = inv_log(np.mean(test_preds, axis=1))

np.save(OUT_DIR / "oof_pred_lgbm_v3_tier1_pseudolabel.npy",  oof_price)
np.save(OUT_DIR / "pred_test_lgbm_v3_tier1_pseudolabel.npy", test_price)

# ---------- Aggregate feature importances ----------
imp_all = pd.concat(importance_dfs, axis=0)
imp_mean = (
    imp_all.groupby("feature")[["gain", "split"]]
    .mean()
    .sort_values("gain", ascending=False)
    .reset_index()
)
imp_mean.to_csv(OUT_DIR / "feature_importance_cvmean_v3_tier1_pseudolabel.csv", index=False)
log(f"ðŸ“Š Aggregated feature importances saved â†’ {OUT_DIR/'feature_importance_cvmean_v3_tier1_pseudolabel.csv'}")

# ---------- Save summary ----------
summary = {
    "n_folds": N_FOLDS,
    "fold_smape": fold_scores,
    "mean_smape": mean_smape,
    "train_shape": X_full.shape,
    "test_shape": X_te_final.shape
}
json.dump(summary, open(OUT_DIR / "cv_summary_lgbm_v3_tier1_pseudolabel.json", "w"), indent=2)
log("K-Fold CV completed â€” results, predictions, and importances saved.")

# ===========================================
# STEP â€” Feature Pruning via LGBM Importances
# ===========================================
import pandas as pd
import numpy as np

# ---- Load aggregated feature importances from Cell 4 ----
imp_path = OUT_DIR / "feature_importance_cvmean_v3_tier1_pseudolabel.csv"
imp_df = pd.read_csv(imp_path).sort_values("gain", ascending=False).reset_index(drop=True)

# ---- Compute cumulative gain ----
imp_df["cum_gain"] = imp_df["gain"].cumsum() / imp_df["gain"].sum()

# ---- Keep top 80% cumulative gain ----
CUTOFF = 0.80
selected_feats = imp_df.loc[imp_df["cum_gain"] <= CUTOFF, "feature"].tolist()
print(f"âœ… Selected {len(selected_feats)} / {len(imp_df)} features (top {int(CUTOFF*100)}% cumulative gain).")

# ---- Load full feature matrices ----
X_tr_full  = np.load(OUT_DIR / "X_tr_final_v3_tier1.npy")
X_val_full = np.load(OUT_DIR / "X_val_final_v3_tier1.npy")
X_te_full  = np.load(OUT_DIR / "X_te_final_v3_tier1.npy")

# ---- Rebuild feat_names (same order used during LGBM CV) ----
try:
    meta = json.load(open(OUT_DIR / "cross_tier1_summary_v3.json"))
    cross_names = meta.get("cross_features", [])
    tier1_cols  = meta.get("tier1_cols", [])
    n_total = X_tr_full.shape[1]
    n_cross = len(cross_names)
    n_tier1 = len(tier1_cols)
    n_fused = n_total - n_cross - n_tier1
    fused_names = [f"fused_{i}" for i in range(n_fused)]
    feat_names = fused_names + cross_names + tier1_cols
except Exception as e:
    print(f"âš ï¸ Could not reconstruct feat_names ({e}). Using generic indices.")
    feat_names = [f"f{i}" for i in range(X_tr_full.shape[1])]

# ---- Map selected feature indices ----
feat_idx = [i for i, f in enumerate(feat_names) if f in selected_feats]
print(f"Feature indices retained: {len(feat_idx)} of {len(feat_names)}")

# ---- Slice matrices ----
X_tr_pruned = X_tr_full[:, feat_idx]
X_val_pruned = X_val_full[:, feat_idx]
X_te_pruned = X_te_full[:, feat_idx]

# ---- Save pruned versions ----
np.save(OUT_DIR / "X_tr_final_pruned.npy", X_tr_pruned)
np.save(OUT_DIR / "X_val_final_pruned.npy", X_val_pruned)
np.save(OUT_DIR / "X_te_final_pruned.npy", X_te_pruned)

# ---- Record metadata ----
prune_meta = {
    "cutoff_cum_gain": CUTOFF,
    "selected_count": len(selected_feats),
    "total_features": len(imp_df),
    "feature_names": selected_feats,
}
with open(OUT_DIR / "feature_pruning_summary.json", "w") as f:
    json.dump(prune_meta, f, indent=2)

print(f"[{len(selected_feats)} features retained] Pruned feature matrices saved to {OUT_DIR}.")

# ===========================================
# CELL 4B â€” Weighted Retraining (Labeled + Pseudo-Labeled) on PRUNED features
# ===========================================
import json, gc
import numpy as np
import pandas as pd
from pathlib import Path
from joblib import dump
import lightgbm as lgb
from sklearn.metrics import mean_absolute_error

# ---------- Load PRUNED feature matrices ----------
X_tr_final  = np.load(OUT_DIR / "X_tr_final_pruned.npy")
X_val_final = np.load(OUT_DIR / "X_val_final_pruned.npy")
X_te_final  = np.load(OUT_DIR / "X_te_final_pruned.npy")
y_tr_log    = np.load(OUT_DIR / "y_tr_log.npy")
y_val_log   = np.load(OUT_DIR / "y_val_log.npy")

# ---------- Load pseudo-labeled data ----------
pseudo_path = OUT_DIR / "pseudo" / "pseudo_test.csv"
pseudo_df = pd.read_csv(pseudo_path)
y_pseudo_log = pseudo_df["log_pseudo_price"].values

# ensure same order for pseudo embeddings
X_pseudo = X_te_final[: len(pseudo_df)]
n_pseudo = len(pseudo_df)
log(f"Loaded {n_pseudo:,} pseudo-labeled samples for weighted training (PRUNED).")

# ---------- Merge labeled + pseudo ----------
X_labeled = np.concatenate([X_tr_final, X_val_final], axis=0)
y_labeled = np.concatenate([y_tr_log, y_val_log], axis=0)
X_train_full = np.concatenate([X_labeled, X_pseudo], axis=0)
y_train_full = np.concatenate([y_labeled, y_pseudo_log], axis=0)

# weights: 1.0 for labeled, 0.5 for pseudo
weights = np.concatenate([
    np.ones(len(y_labeled)),
    np.full(len(y_pseudo_log), 0.5)
])
log(f"Final weighted dataset â†’ {X_train_full.shape}, weights mean={weights.mean():.3f}")

# ---------- LightGBM params ----------
SEED = 42
lgb_params = {
    "objective": "regression_l1",
    "metric": "mae",
    "learning_rate": 0.03,
    "num_leaves": 31,
    "min_data_in_leaf": 400,
    "feature_fraction": 0.55,
    "bagging_fraction": 0.7,
    "bagging_freq": 1,
    "lambda_l1": 8.0,
    "lambda_l2": 20.0,
    "max_depth": -1,
    "device_type": "gpu",
    "verbosity": -1,
    "seed": SEED
}

# ---------- Train full model with weights ----------
ds_full = lgb.Dataset(X_train_full, label=y_train_full, weight=weights)
booster = lgb.train(
    params=lgb_params,
    train_set=ds_full,
    num_boost_round=6000,
    valid_sets=[ds_full],
    valid_names=["train_full"],
    callbacks=[lgb.log_evaluation(500)]
)

# ---------- Predict on test ----------
pred_test_log = booster.predict(X_te_final, num_iteration=booster.best_iteration)
pred_test = np.expm1(pred_test_log)

# ---------- Save results ----------
np.save(OUT_DIR / "pred_test_lgbm_weighted_pruned.npy", pred_test)
dump(booster, OUT_DIR / "lgbm_full_weighted_pruned.joblib")

out_csv = OUT_DIR / "submission_lgbm_weighted_pruned.csv"
test_df = pd.read_csv(BASE_DIR / "dataset" / "test_final_clean.csv")
submission = pd.DataFrame({
    "sample_id": test_df["sample_id"],
    "price": pred_test
})
submission.to_csv(out_csv, index=False)

log(f"âœ… Weighted retraining (PRUNED) complete â€” submission saved â†’ {out_csv}")
log(f"Mean predicted price: {pred_test.mean():.3f} | Min: {pred_test.min():.3f} | Max: {pred_test.max():.3f}")

# ===========================================
# CELL 5A â€” Weighted Retraining (XGBoost GPU) on PRUNED features
# ===========================================
!pip -q install xgboost joblib

import numpy as np, pandas as pd, xgboost as xgb
from joblib import dump
from pathlib import Path

# ---------- Load PRUNED feature matrices ----------
X_tr_final  = np.load(OUT_DIR / "X_tr_final_pruned.npy")
X_val_final = np.load(OUT_DIR / "X_val_final_pruned.npy")
X_te_final  = np.load(OUT_DIR / "X_te_final_pruned.npy")
y_tr_log    = np.load(OUT_DIR / "y_tr_log.npy")
y_val_log   = np.load(OUT_DIR / "y_val_log.npy")

# ---------- Load pseudo-labeled data ----------
pseudo_df = pd.read_csv(OUT_DIR / "pseudo" / "pseudo_test.csv")
y_pseudo_log = pseudo_df["log_pseudo_price"].values
X_pseudo = X_te_final[: len(pseudo_df)]

# ---------- Merge labeled + pseudo ----------
X_labeled = np.concatenate([X_tr_final, X_val_final], axis=0)
y_labeled = np.concatenate([y_tr_log, y_val_log], axis=0)
X_train_full = np.concatenate([X_labeled, X_pseudo], axis=0)
y_train_full = np.concatenate([y_labeled, y_pseudo_log], axis=0)

weights = np.concatenate([
    np.ones(len(y_labeled)),
    np.full(len(y_pseudo_log), 0.5)
])

print(f"Training XGBoost on {X_train_full.shape} (weights mean={weights.mean():.3f})")

# ---------- GPU parameters ----------
xgb_params = {
    "objective": "reg:squarederror",
    "eval_metric": "mae",
    "tree_method": "gpu_hist",
    "learning_rate": 0.03,
    "max_depth": 7,
    "subsample": 0.7,
    "colsample_bytree": 0.55,
    "lambda": 20.0,
    "alpha": 8.0,
    "random_state": 42
}

dtrain = xgb.DMatrix(X_train_full, label=y_train_full, weight=weights)
dtest  = xgb.DMatrix(X_te_final)

booster = xgb.train(
    params=xgb_params,
    dtrain=dtrain,
    num_boost_round=4000,
    evals=[(dtrain, "train_full")],
    verbose_eval=500
)

# ---------- Predict ----------
pred_test_log = booster.predict(dtest)
pred_test = np.expm1(pred_test_log)

# ---------- Save ----------
np.save(OUT_DIR / "pred_test_xgb_weighted_pruned.npy", pred_test)
booster.save_model(str(OUT_DIR / "xgb_full_weighted_pruned.json"))

out_csv = OUT_DIR / "submission_xgb_weighted_pruned.csv"
test_df = pd.read_csv(BASE_DIR / "dataset" / "test_final_clean.csv")
pd.DataFrame({"sample_id": test_df["sample_id"], "price": pred_test}).to_csv(out_csv, index=False)

print(f"âœ… XGBoost weighted retraining complete â€” saved â†’ {out_csv}")
print(f"Mean predicted price: {pred_test.mean():.3f} | Min: {pred_test.min():.3f} | Max: {pred_test.max():.3f}")

# ===========================================
# CELL 5B â€” Weighted Retraining (CatBoost GPU) on PRUNED features
# ===========================================
!pip -q install catboost joblib

import numpy as np, pandas as pd
from catboost import CatBoostRegressor, Pool
from joblib import dump

# ---------- Load PRUNED feature matrices ----------
X_tr_final  = np.load(OUT_DIR / "X_tr_final_pruned.npy")
X_val_final = np.load(OUT_DIR / "X_val_final_pruned.npy")
X_te_final  = np.load(OUT_DIR / "X_te_final_pruned.npy")
y_tr_log    = np.load(OUT_DIR / "y_tr_log.npy")
y_val_log   = np.load(OUT_DIR / "y_val_log.npy")

# ---------- Load pseudo-labeled data ----------
pseudo_df = pd.read_csv(OUT_DIR / "pseudo" / "pseudo_test.csv")
y_pseudo_log = pseudo_df["log_pseudo_price"].values
X_pseudo = X_te_final[: len(pseudo_df)]

# ---------- Merge labeled + pseudo ----------
X_labeled = np.concatenate([X_tr_final, X_val_final], axis=0)
y_labeled = np.concatenate([y_tr_log, y_val_log], axis=0)
X_train_full = np.concatenate([X_labeled, X_pseudo], axis=0)
y_train_full = np.concatenate([y_labeled, y_pseudo_log], axis=0)

weights = np.concatenate([
    np.ones(len(y_labeled)),
    np.full(len(y_pseudo_log), 0.5)
])

print(f"Training CatBoost on {X_train_full.shape} (weights mean={weights.mean():.3f})")

train_pool = Pool(X_train_full, label=y_train_full, weight=weights)
test_pool  = Pool(X_te_final)

cat_params = {
    "loss_function": "MAE",
    "learning_rate": 0.03,
    "depth": 7,
    "l2_leaf_reg": 30.0,
    "bootstrap_type": "Bernoulli",  # Added: Required for subsample
    "subsample": 0.7,
    #"rsm": 0.55,
    "iterations": 3000,
    "task_type": "GPU",
    "random_seed": 42,
    "verbose": 500,
    "allow_writing_files": False
}

model = CatBoostRegressor(**cat_params)
model.fit(train_pool)

# ---------- Predict ----------
pred_test_log = model.predict(test_pool)
pred_test = np.expm1(pred_test_log)

# ---------- Save ----------
np.save(OUT_DIR / "pred_test_cat_weighted_pruned.npy", pred_test)
model.save_model(str(OUT_DIR / "catboost_full_weighted_pruned.cbm"))

out_csv = OUT_DIR / "submission_cat_weighted_pruned.csv"
test_df = pd.read_csv(BASE_DIR / "dataset" / "test_final_clean.csv")
pd.DataFrame({"sample_id": test_df["sample_id"], "price": pred_test}).to_csv(out_csv, index=False)

print(f"âœ… CatBoost weighted retraining complete â€” saved â†’ {out_csv}")
print(f"Mean predicted price: {pred_test.mean():.3f} | Min: {pred_test.min():.3f} | Max: {pred_test.max():.3f}")

# ===========================================
# FINAL COMPARISON â€” OptBlend (Uploaded) vs Pseudo-Ensemble (Pruned)
# ===========================================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
from google.colab import files

# -------------------------------------------
# Step 1 â€” Upload your OLD OptBlend CSV manually
# -------------------------------------------
print("ðŸ“‚ Please upload your old OptBlend submission CSV (e.g., submission_final_optblend_smooth.csv)")
uploaded = files.upload()

# Get filename dynamically
uploaded_file = list(uploaded.keys())[0]

# Load uploaded CSV
old_sub = pd.read_csv(uploaded_file)
print(f"âœ… Uploaded file loaded: {uploaded_file}")
print(f"Shape: {old_sub.shape}")
print(old_sub.head())

# -------------------------------------------
# Step 2 â€” Load NEW pseudo-labeled predictions
# -------------------------------------------
BASE_NEW = Path("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_vae_pseudolabel_v1")

# New model predictions
pred_lgb = np.load(BASE_NEW / "pred_test_lgbm_weighted_pruned.npy")
pred_xgb = np.load(BASE_NEW / "pred_test_xgb_weighted_pruned.npy")
pred_cat = np.load(BASE_NEW / "pred_test_cat_weighted_pruned.npy")

# Weighted blend of new models
blend_new = 0.4 * pred_lgb + 0.3 * pred_xgb + 0.3 * pred_cat

# -------------------------------------------
# Step 3 â€” Global Statistics Comparison
# -------------------------------------------
print("\n========== Global Statistics Comparison ==========")
print(f"Old mean: {old_sub.price.mean():.3f} | New blend mean: {blend_new.mean():.3f}")
print(f"Î”mean: {(blend_new.mean() - old_sub.price.mean()):+.3f}")
print(f"Old std : {old_sub.price.std():.3f} | New blend std : {blend_new.std():.3f}")
print(f"Î”std : {(blend_new.std() - old_sub.price.std()):+.3f}")
print(f"Old range: {old_sub.price.min():.2f} â€“ {old_sub.price.max():.2f}")
print(f"New range: {blend_new.min():.2f} â€“ {blend_new.max():.2f}")

# Correlation
corr = np.corrcoef(old_sub.price, blend_new)[0, 1]
print(f"Correlation between old and new: {corr:.4f}")

# -------------------------------------------
# Step 4 â€” Save candidate blended submission
# -------------------------------------------
cand_csv = BASE_NEW / "submission_final_candidate_blend.csv"
pd.DataFrame({
    "sample_id": old_sub.iloc[:, 0],  # assumes first column is sample_id
    "price": blend_new
}).to_csv(cand_csv, index=False)

print(f"\nâœ… Candidate blended submission saved â†’ {cand_csv}")

# -------------------------------------------
# Step 5 â€” Visualization
# -------------------------------------------
plt.figure(figsize=(8, 5))
plt.hist(old_sub.price, bins=60, alpha=0.5, label="Old OptBlend", color='skyblue')
plt.hist(blend_new, bins=60, alpha=0.5, label="New Pseudo-Ensemble", color='salmon')
plt.legend()
plt.title("Price Distribution Comparison")
plt.xlabel("Price")
plt.ylabel("Frequency")
plt.show()

print("\nâœ… Validation plot generated â€” visually inspect price distribution stability.")

# ===========================================
# VALIDATION SUITE â€” Weighted Pseudo-Labeled Ensemble (with real OOF)
# ===========================================
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from pathlib import Path
from sklearn.metrics import mean_absolute_error

# --- Paths ---
OUT_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_vae_pseudolabel_v1")
SPLIT_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final/splits")

# --- Load true labeled data ---
y_tr_log  = np.load(SPLIT_DIR / "y_train_log.npy")
y_val_log = np.load(SPLIT_DIR / "y_val_log.npy")
y_true_log = np.concatenate([y_tr_log, y_val_log])
y_true = np.expm1(y_true_log)

# --- Load model predictions (test) ---
pred_lgb = np.load(OUT_DIR / "pred_test_lgbm_weighted_pruned.npy")
pred_xgb = np.load(OUT_DIR / "pred_test_xgb_weighted_pruned.npy")
pred_cat = np.load(OUT_DIR / "pred_test_cat_weighted_pruned.npy")
blend_pred = 0.4*pred_lgb + 0.3*pred_xgb + 0.3*pred_cat

# --- Load pseudo labels ---
pseudo_df = pd.read_csv(OUT_DIR / "pseudo" / "pseudo_test.csv")
y_pseudo_log = pseudo_df["log_pseudo_price"].values
conf_mask = pseudo_df["confidence"] > 0.9
y_pseudo_conf = y_pseudo_log[conf_mask]

# =====================================================
# 1ï¸âƒ£ True-Labeled MAE (using v3_tier1_pruned OOFs)
# =====================================================
# =====================================================
# 1ï¸âƒ£ True-Labeled MAE (fixed for log-space OOFs)
# =====================================================
# =====================================================
# 1ï¸âƒ£ True-Labeled MAE (price-space OOFs)
# =====================================================
try:
    # Already in price scale
    oof_lgb = np.load("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_124613_fe_v2_stack/oof_pred_lgbm_v3_tier1_pruned.npy")
    oof_xgb = np.load("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_124613_fe_v2_stack/oof_pred_xgb_v3_tier1_pruned.npy")
    oof_cat = np.load("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_124613_fe_v2_stack/oof_pred_cat_v3_tier1_pruned.npy")

    # Weighted blend in price space
    oof_blend = 0.4*oof_lgb + 0.3*oof_xgb + 0.3*oof_cat

    # y_true is still in log space â†’ convert once
    y_true_price = np.expm1(y_true_log)

    mae_val = mean_absolute_error(y_true_price, oof_blend)
    print(f"âœ… True-labeled MAE (OOF v3_tier1_pruned blend, price-space): {mae_val:.4f}")

except Exception as e:
    print(f"âš ï¸ Could not compute true-labeled MAE: {e}")

# =====================================================
# 2ï¸âƒ£ Pseudo-Label Consistency (log space)
# =====================================================
pred_test_log = np.log1p(blend_pred.clip(min=1e-6))
diff_pseudo = np.mean(np.abs(pred_test_log[conf_mask] - y_pseudo_conf))
print(f"âœ… Mean abs diff vs high-confidence pseudo labels: {diff_pseudo:.4f} log-points")

# =====================================================
# 3ï¸âƒ£ Distribution Sanity â€” True vs Test
# =====================================================
plt.figure(figsize=(8,5))
plt.hist(y_true, bins=50, alpha=0.5, label="True Labeled Prices", color="skyblue")
plt.hist(blend_pred, bins=50, alpha=0.5, label="Test Predictions (Blend)", color="salmon")
plt.title("Distribution Overlap â€” True vs Test"); plt.legend(); plt.xlabel("Price"); plt.ylabel("Count");
plt.show()

print(f"Labeled mean: {y_true.mean():.3f} | Test mean: {blend_pred.mean():.3f}")
print(f"Labeled range: {y_true.min():.2f} â€“ {y_true.max():.2f}")
print(f"Test range: {blend_pred.min():.2f} â€“ {blend_pred.max():.2f}")

# =====================================================
# 4ï¸âƒ£ Cross-Model Correlation (Stability)
# =====================================================
corr_lgb_xgb = np.corrcoef(pred_lgb, pred_xgb)[0,1]
corr_lgb_cat = np.corrcoef(pred_lgb, pred_cat)[0,1]
corr_xgb_cat = np.corrcoef(pred_xgb, pred_cat)[0,1]

print(f"âœ… Cross-model correlations:")
print(f"   LGBâ€“XGB : {corr_lgb_xgb:.4f}")
print(f"   LGBâ€“CAT : {corr_lgb_cat:.4f}")
print(f"   XGBâ€“CAT : {corr_xgb_cat:.4f}")

plt.figure(figsize=(6,6))
plt.scatter(pred_lgb, pred_xgb, s=2, alpha=0.3)
plt.xlabel("LGB preds"); plt.ylabel("XGB preds"); plt.title("Cross-Model Correlation (LGB vs XGB)");
plt.show()

from google.colab import files
files.download("/content/drive/MyDrive/smart_product_pricing_final/experiments/exp_20251013_vae_pseudolabel_v1/submission_final_pseudo_optblend.csv")
