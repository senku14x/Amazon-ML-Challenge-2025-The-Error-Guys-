# -*- coding: utf-8 -*-
"""amazon_ml_challenge_final_draft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Ob02-QRUU0xq_azRB3V-pR0GkexATRf
"""

# ============================================================
# SMART PRODUCT PRICING — FULL 3-FIELD PARSER + PACK CLEANUP
#   Input : train.csv / test.csv  (original dataset)
#   Output: train_final_clean.csv / test_final_clean.csv
#   Columns added:
#       item_name, item_description, item_pack_clean,
#       has_pack, pack_value, pack_unit, pack_count
# ============================================================

import re, math, unicodedata
from pathlib import Path
from typing import Optional, Dict, Tuple
import pandas as pd

# ---------------------------
# 0) Mount Drive if Colab
# ---------------------------
IN_COLAB = False
try:
    import google.colab  # type: ignore
    IN_COLAB = True
except Exception:
    pass

if IN_COLAB:
    from google.colab import drive  # type: ignore
    drive.mount("/content/drive")

# ---------------------------
# 1) Paths
# ---------------------------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
DATA_DIR = BASE_DIR / "dataset"
TRAIN_PATH = DATA_DIR / "train.csv"
TEST_PATH = DATA_DIR / "test.csv"
TRAIN_OUT = DATA_DIR / "train_final_clean.csv"
TEST_OUT = DATA_DIR / "test_final_clean.csv"

# ---------------------------
# 2) Text normalization
# ---------------------------
def norm_text(s: Optional[str]) -> str:
    if s is None or (isinstance(s, float) and math.isnan(s)):
        return ""
    s = str(s)
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\r", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n", s)
    return s.strip()

# ---------------------------
# 3) Regex patterns for parsing
# ---------------------------
NAME_LINE = re.compile(
    r"^\s*(?:item\s*name|product\s*name|item\s*title|title|name)\s*[:\-]\s*(.+?)\s*$",
    re.IGNORECASE | re.MULTILINE,
)
DESC_LINE = re.compile(
    r"^\s*(?:item\s*description|description|bullet\s*points?)\s*[:\-]\s*(.+?)\s*$",
    re.IGNORECASE | re.MULTILINE,
)
BULLET_LINE = re.compile(r"^(?:[-•*]|\d+[.)])\s*(.+)", re.IGNORECASE | re.MULTILINE)
ANY_LABEL = re.compile(
    r"(?:^|\n)\s*(?:item\s*name|product\s*name|item\s*title|title|name|"
    r"item\s*description|description|bullet\s*points?|"
    r"item\s*pack\s*quantity|ipq|quantity|qty|value|size|unit|uom)\s*[:\-]",
    re.IGNORECASE | re.MULTILINE,
)
# Pack patterns
COUNT_X_SIZE = re.compile(r"(?P<count>\d{1,3})\s*[xX×*]\s*(?P<size>\d+(?:\.\d+)?)\s*(?P<unit>[A-Za-z\. ]+)")
SIZE_UNIT = re.compile(r"(?P<size>\d+(?:\.\d+)?)\s*(?P<unit>(?:ml|l|g|kg|mg|oz|fl\.?\s*oz|count|ct|pc|pcs))", re.IGNORECASE)
COUNT_ONLY = re.compile(r"(?P<count>\d{1,3})\s*(?:count|ct|pcs?|units?)", re.IGNORECASE)

# ---------------------------
# 4) Unit canonicalization
# ---------------------------
UNIT_ALIASES = {
    "ml": "ml", "milliliter": "ml", "millilitre": "ml", "milliliters": "ml", "millilitres": "ml",
    "l": "l", "liter": "l", "litre": "l", "liters": "l", "litres": "l",
    "cl": "cl", "fl oz": "fl oz", "fluid ounce": "fl oz", "fluid ounces": "fl oz",
    "g": "g", "gram": "g", "grams": "g", "kg": "kg", "kilogram": "kg", "kilograms": "kg",
    "mg": "mg", "oz": "oz", "ounce": "oz", "ounces": "oz",
    "count": "count", "ct": "count", "pc": "count", "pcs": "count", "piece": "count", "pieces": "count",
    "unit": "count", "units": "count",
}
VALID_UNITS = set(UNIT_ALIASES.values())

def norm_unit(u: Optional[str]) -> Optional[str]:
    if not u:
        return None
    u = unicodedata.normalize("NFKC", str(u)).lower().strip().replace(".", " ")
    u = re.sub(r"\s+", " ", u)
    return UNIT_ALIASES.get(u, u)

# ---------------------------
# 5) Parse a single catalog cell
# ---------------------------
def parse_catalog_cell(text: Optional[str]) -> Dict[str, Optional[str]]:
    t = norm_text(text)

    # --- Item name ---
    m_name = NAME_LINE.search(t)
    if m_name:
        item_name = m_name.group(1).strip()
    else:
        item_name = None
        for ln in (ln.strip() for ln in t.split("\n") if ln.strip()):
            if not re.match(
                r"^(?:item\s*description|description|bullet\s*points?|"
                r"item\s*pack\s*quantity|ipq|quantity|qty|value|size|unit|uom)\s*[:\-]",
                ln,
                re.IGNORECASE,
            ):
                item_name = ln
                break

    # --- Item description ---
    m_desc = DESC_LINE.search(t)
    if m_desc:
        item_description = m_desc.group(1).strip()
    else:
        bullets = [b.strip() for b in BULLET_LINE.findall(t)]
        if bullets:
            item_description = " ".join(dict.fromkeys(bullets))
        else:
            item_description = None
            if item_name:
                try:
                    start = t.lower().find(item_name.lower()) + len(item_name)
                    nxt = ANY_LABEL.search(t[start:])
                    mid = t[start:(start + nxt.start() if nxt else len(t))]
                    cand = [s.strip() for s in re.split(r"(?<=[.?!])\s+|\n", mid) if s.strip()]
                    item_description = " ".join(cand[:8]) if cand else None
                except Exception:
                    item_description = None
    if isinstance(item_description, str) and len(item_description) > 1200:
        item_description = item_description[:1200]

    # --- Item pack (raw detection) ---
    item_pack = None
    for pat in [COUNT_X_SIZE, SIZE_UNIT, COUNT_ONLY]:
        m = pat.search(t)
        if m:
            item_pack = m.group(0).strip()
            break

    return {"item_name": item_name, "item_description": item_description, "item_pack": item_pack}

# ---------------------------
# 6) Parse full DataFrame
# ---------------------------
def parse_dataframe(df: pd.DataFrame, catalog_col="catalog_content") -> pd.DataFrame:
    parsed = df[catalog_col].apply(parse_catalog_cell)
    parsed_df = pd.DataFrame(parsed.tolist(), index=df.index)
    return pd.concat([df.reset_index(drop=True), parsed_df.reset_index(drop=True)], axis=1)

# ---------------------------
# 7) Clean item_pack into numeric fields
# ---------------------------
def parse_item_pack(s: Optional[str]) -> Tuple[Optional[float], Optional[str], Optional[int]]:
    if s is None or (isinstance(s, float) and math.isnan(s)):
        return (None, None, None)
    txt = unicodedata.normalize("NFKC", str(s)).strip().lower()
    txt = re.sub(r"[–—]", "-", txt)
    txt = re.sub(r"\s+", " ", txt)

    # count x size
    m = COUNT_X_SIZE.search(txt)
    if m:
        return float(m.group("size")), norm_unit(m.group("unit")), int(m.group("count"))
    # size + unit
    m = SIZE_UNIT.search(txt)
    if m:
        return float(m.group("size")), norm_unit(m.group("unit")), None
    # count only
    m = COUNT_ONLY.search(txt)
    if m:
        return None, "count", int(m.group("count"))
    return (None, None, None)

def clean_pack_column(df: pd.DataFrame) -> pd.DataFrame:
    def _parse(cell):
        v, u, c = parse_item_pack(cell)
        if v or c:
            clean = f"{c}x{v:g} {u}" if (v and c and u != 'count') else (
                f"{v:g} {u}" if v and u else (f"{c} count" if c else None))
        else:
            clean = None
        return pd.Series(
            {"item_pack_clean": clean, "has_pack": int(clean is not None), "pack_value": v, "pack_unit": u, "pack_count": c}
        )

    parsed = df["item_pack"].apply(_parse)
    return pd.concat([df, parsed], axis=1)

# ---------------------------
# 8) Sanity check
# ---------------------------
def sanity_report(df: pd.DataFrame, title: str):
    n = len(df)
    def pct(x): return f"{100.0 * x / n:.1f}%"
    print(f"\n=== Sanity Report: {title} (n={n}) ===")
    print("item_name blanks:", pct(df["item_name"].isna().sum()))
    print("item_description blanks:", pct(df["item_description"].isna().sum()))
    print("item_pack_clean blanks:", pct(df["item_pack_clean"].isna().sum()))
    print("has_pack == 1:", pct((df["has_pack"] == 1).sum()))
    print("\nTop pack_unit:")
    print(df["pack_unit"].fillna("NA").value_counts().head(10))
    print("\nTop item_pack_clean:")
    print(df["item_pack_clean"].fillna("NA").str.lower().value_counts().head(15))

# ---------------------------
# 9) Run pipeline
# ---------------------------
def robust_read_csv(path: Path, encodings=("utf-8","latin-1","iso-8859-1","cp1252")) -> pd.DataFrame:
    for enc in encodings:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    raise RuntimeError(f"Could not read {path}")

def main():
    print(f"Reading: {TRAIN_PATH.name}")
    tr = robust_read_csv(TRAIN_PATH)
    print(f"Reading: {TEST_PATH.name}")
    te = robust_read_csv(TEST_PATH)

    print("\nParsing text fields...")
    tr_parsed = parse_dataframe(tr, "catalog_content")
    te_parsed = parse_dataframe(te, "catalog_content")

    print("\nCleaning item_pack...")
    tr_clean = clean_pack_column(tr_parsed)
    te_clean = clean_pack_column(te_parsed)

    sanity_report(tr_clean, "train")
    sanity_report(te_clean, "test")

    tr_clean.to_csv(TRAIN_OUT, index=False)
    te_clean.to_csv(TEST_OUT, index=False)
    print(f"\nSaved:\n - {TRAIN_OUT}\n - {TEST_OUT}")

if __name__ == "__main__":
    main()

# ============================================================
# STEP 2: CREATE TRAIN/VAL SPLIT (Stratified by Price)
# ============================================================
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.model_selection import StratifiedShuffleSplit

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'
SPLIT_DIR.mkdir(exist_ok=True, parents=True)

print("="*60)
print("STEP 2: Creating Train/Val Split")
print("="*60)

# 1) Load cleaned data
train_clean = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
print(f"✅ Loaded train: {train_clean.shape}")
print(f"Columns: {train_clean.columns.tolist()}")

# 2) Create stratified split by price quantiles
y_price = train_clean['price'].astype(float).values
y_log = np.log1p(y_price)

# Create 10 bins for stratification
bins = pd.qcut(y_log, q=10, labels=False, duplicates='drop')

# Split: 80% train, 20% val
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
tr_idx, val_idx = next(sss.split(train_clean, bins))

# 3) Save indices
np.save(SPLIT_DIR / 'train_idx.npy', tr_idx)
np.save(SPLIT_DIR / 'val_idx.npy', val_idx)

print(f"\n✅ Split created:")
print(f"   Train: {len(tr_idx)} samples ({len(tr_idx)/len(train_clean)*100:.1f}%)")
print(f"   Val:   {len(val_idx)} samples ({len(val_idx)/len(train_clean)*100:.1f}%)")

# 4) Save targets
y_tr = y_price[tr_idx]
y_val = y_price[val_idx]
y_tr_log = y_log[tr_idx]
y_val_log = y_log[val_idx]

np.save(SPLIT_DIR / 'y_train.npy', y_tr)
np.save(SPLIT_DIR / 'y_val.npy', y_val)
np.save(SPLIT_DIR / 'y_train_log.npy', y_tr_log)
np.save(SPLIT_DIR / 'y_val_log.npy', y_val_log)

print(f"\n✅ Targets saved")

# 5) Quick sanity check
print("\n--- Price Distribution Check ---")
print(f"Train - Min: ${y_tr.min():.2f}, Median: ${np.median(y_tr):.2f}, Max: ${y_tr.max():.2f}")
print(f"Val   - Min: ${y_val.min():.2f}, Median: ${np.median(y_val):.2f}, Max: ${y_val.max():.2f}")

# 6) Save sample IDs for reference
train_sample_ids = train_clean['sample_id'].values
np.save(SPLIT_DIR / 'train_sample_ids.npy', train_sample_ids[tr_idx])
np.save(SPLIT_DIR / 'val_sample_ids.npy', train_sample_ids[val_idx])

print(f"\n✅ All split files saved to: {SPLIT_DIR}")
print("Files created:")
print("  - train_idx.npy, val_idx.npy")
print("  - y_train.npy, y_val.npy, y_train_log.npy, y_val_log.npy")
print("  - train_sample_ids.npy, val_sample_ids.npy")

# ============================================================
# STEP 2.5: ANALYZE & HANDLE MISSING DATA
# Keep all columns, handle missing values intelligently
# ============================================================
import numpy as np
import pandas as pd
from pathlib import Path

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'

print("="*60)
print("STEP 2.5: Missing Data Analysis & Handling")
print("="*60)

# ============================================================
# 1) LOAD DATASETS
# ============================================================
print("\n📂 Loading datasets...")
train_full = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
test_full = pd.read_csv(DATA_DIR / 'test_final_clean.csv')

# Load split indices
tr_idx = np.load(SPLIT_DIR / 'train_idx.npy')
val_idx = np.load(SPLIT_DIR / 'val_idx.npy')

# Split
df_train = train_full.iloc[tr_idx].reset_index(drop=True).copy()
df_val = train_full.iloc[val_idx].reset_index(drop=True).copy()
df_test = test_full.reset_index(drop=True).copy()

print(f"✅ Train: {df_train.shape}")
print(f"✅ Val:   {df_val.shape}")
print(f"✅ Test:  {df_test.shape}")

# ============================================================
# 2) ANALYZE MISSING DATA
# ============================================================
print("\n" + "="*60)
print("MISSING DATA ANALYSIS")
print("="*60)

def analyze_missing(df, name):
    print(f"\n{name} Dataset:")
    print("-" * 40)

    missing_info = []
    for col in df.columns:
        if col == 'split':
            continue
        n_missing = df[col].isna().sum()
        pct_missing = n_missing / len(df) * 100
        dtype = df[col].dtype

        missing_info.append({
            'column': col,
            'missing_count': n_missing,
            'missing_pct': pct_missing,
            'dtype': dtype
        })

    missing_df = pd.DataFrame(missing_info).sort_values('missing_pct', ascending=False)

    # Show only columns with missing data
    missing_only = missing_df[missing_df['missing_count'] > 0]
    if len(missing_only) > 0:
        print(missing_only.to_string(index=False))
    else:
        print("✅ No missing values!")

    return missing_df

train_missing = analyze_missing(df_train, "TRAIN")
val_missing = analyze_missing(df_val, "VAL")
test_missing = analyze_missing(df_test, "TEST")

# ============================================================
# 3) MISSING DATA HANDLING STRATEGY
# ============================================================
print("\n" + "="*60)
print("MISSING DATA HANDLING STRATEGY")
print("="*60)

def handle_missing_values(df, is_test=False):
    """
    Handle missing values based on column type
    Returns: processed dataframe
    """
    df = df.copy()

    # TEXT COLUMNS: Fill with empty string + create 'is_missing' flag
    text_columns = ['catalog_content', 'item_name', 'item_description', 'item_pack_clean']

    for col in text_columns:
        if col in df.columns:
            # Create missing flag (valuable feature!)
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with empty string
            df[col] = df[col].fillna('')

            print(f"✅ {col:25s} → filled with '' + added '{col}_is_missing' flag")

    # NUMERIC COLUMNS: Fill with -1 (or 0 for counts) + create 'is_missing' flag
    numeric_columns = ['pack_value', 'pack_count']

    for col in numeric_columns:
        if col in df.columns:
            # Create missing flag
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with special value
            fill_value = 0 if 'count' in col else -1
            df[col] = df[col].fillna(fill_value)

            print(f"✅ {col:25s} → filled with {fill_value} + added '{col}_is_missing' flag")

    # CATEGORICAL COLUMNS: Fill with 'unknown' + create 'is_missing' flag
    categorical_columns = ['pack_unit', 'item_pack']

    for col in categorical_columns:
        if col in df.columns:
            # Create missing flag
            df[f'{col}_is_missing'] = df[col].isna().astype(int)

            # Fill with 'unknown'
            df[col] = df[col].fillna('unknown')

            print(f"✅ {col:25s} → filled with 'unknown' + added '{col}_is_missing' flag")

    # BINARY COLUMNS: Fill with 0
    binary_columns = ['has_pack']

    for col in binary_columns:
        if col in df.columns:
            df[col] = df[col].fillna(0).astype(int)
            print(f"✅ {col:25s} → filled with 0")

    # URL COLUMNS: Keep as-is (we'll extract features from them)
    url_columns = ['image_link']
    for col in url_columns:
        if col in df.columns:
            df[f'{col}_is_missing'] = df[col].isna().astype(int)
            df[col] = df[col].fillna('')
            print(f"✅ {col:25s} → kept + added '{col}_is_missing' flag")

    return df

# Apply to all three datasets
print("\n📝 Processing TRAIN:")
df_train_processed = handle_missing_values(df_train, is_test=False)

print("\n📝 Processing VAL:")
df_val_processed = handle_missing_values(df_val, is_test=False)

print("\n📝 Processing TEST:")
df_test_processed = handle_missing_values(df_test, is_test=True)

# ============================================================
# 4) VERIFY NO MISSING VALUES REMAIN
# ============================================================
print("\n" + "="*60)
print("VERIFICATION: Check No Missing Values Remain")
print("="*60)

for name, df in [('Train', df_train_processed), ('Val', df_val_processed), ('Test', df_test_processed)]:
    total_missing = df.isna().sum().sum()
    print(f"{name:5s}: {total_missing} missing values")

    if total_missing > 0:
        print(f"  ⚠️ Columns still with missing:")
        print(df.isna().sum()[df.isna().sum() > 0])

# ============================================================
# 5) ADD SPLIT IDENTIFIER
# ============================================================
df_train_processed['split'] = 'train'
df_val_processed['split'] = 'val'
df_test_processed['split'] = 'test'

# ============================================================
# 6) SAVE PROCESSED DATAFRAMES
# ============================================================
print("\n💾 Saving processed dataframes...")

df_train_processed.to_parquet(SPLIT_DIR / 'df_train_processed.parquet', index=False)
df_val_processed.to_parquet(SPLIT_DIR / 'df_val_processed.parquet', index=False)
df_test_processed.to_parquet(SPLIT_DIR / 'df_test_processed.parquet', index=False)

print(f"✅ Saved to: {SPLIT_DIR}")
print("  - df_train_processed.parquet")
print("  - df_val_processed.parquet")
print("  - df_test_processed.parquet")

# ============================================================
# 7) SUMMARY
# ============================================================
print("\n" + "="*60)
print("✅ STEP 2.5 COMPLETE - MISSING DATA HANDLED")
print("="*60)

print("\n📊 New columns added (missing indicators):")
new_cols = [col for col in df_train_processed.columns if '_is_missing' in col]
for col in new_cols:
    n_missing_train = df_train_processed[col].sum()
    n_missing_val = df_val_processed[col].sum()
    n_missing_test = df_test_processed[col].sum()
    print(f"  ✅ {col:35s} Train: {n_missing_train:5d} | Val: {n_missing_val:5d} | Test: {n_missing_test:5d}")

print("\n📦 What we kept:")
print("  ✅ ALL original columns (catalog_content, image_link, etc.)")
print("  ✅ ALL parsed columns (item_name, item_description, pack_*, etc.)")
print("  ✅ Added _is_missing flags (valuable features!)")
print("  ✅ No NaN values remain")

print("\n💡 Strategy:")
print("  1. Keep everything - let SHAP decide what's useful")
print("  2. Missing flags capture 'missingness' as signal")
print("  3. Clean fill values ensure no errors downstream")
print("  4. Same processing for train/val/test = consistency")

print(f"\n📈 Final shapes:")
print(f"  Train: {df_train_processed.shape}")
print(f"  Val:   {df_val_processed.shape}")
print(f"  Test:  {df_test_processed.shape}")

print("\n🎯 Ready for Step 3: Feature Extraction!")

# ============================================================
# STEP 2.6: ANALYZE DATA TYPES & PREPARE FOR EMBEDDINGS
# ============================================================
import numpy as np
import pandas as pd
from pathlib import Path

# Paths
BASE_DIR = Path('/content/drive/MyDrive/smart_product_pricing_final')
DATA_DIR = BASE_DIR / 'dataset'
SPLIT_DIR = BASE_DIR / 'splits'

print("="*60)
print("STEP 2.6: Data Type Analysis")
print("="*60)

# ============================================================
# 1) LOAD DATASETS
# ============================================================
print("\n📂 Loading datasets...")
train_full = pd.read_csv(DATA_DIR / 'train_final_clean.csv')
test_full = pd.read_csv(DATA_DIR / 'test_final_clean.csv')

# Load split indices
tr_idx = np.load(SPLIT_DIR / 'train_idx.npy')
val_idx = np.load(SPLIT_DIR / 'val_idx.npy')

# Split
df_train = train_full.iloc[tr_idx].reset_index(drop=True).copy()
df_val = train_full.iloc[val_idx].reset_index(drop=True).copy()
df_test = test_full.reset_index(drop=True).copy()

print(f"✅ Train: {df_train.shape}")
print(f"✅ Val:   {df_val.shape}")
print(f"✅ Test:  {df_test.shape}")

# ============================================================
# 2) DATA TYPE ANALYSIS
# ============================================================
print("\n" + "="*60)
print("DATA TYPE ANALYSIS - TRAIN DATASET")
print("="*60)

# Get comprehensive info about each column
def analyze_column(df, col_name):
    """Detailed analysis of a single column"""
    col = df[col_name]

    info = {
        'column': col_name,
        'dtype': str(col.dtype),
        'non_null': col.notna().sum(),
        'null': col.isna().sum(),
        'null_pct': f"{col.isna().sum() / len(df) * 100:.1f}%",
        'unique': col.nunique(),
        'memory_mb': col.memory_usage(deep=True) / 1024**2
    }

    # Type-specific analysis
    if col.dtype == 'object':
        # Text columns
        non_null = col.dropna()
        if len(non_null) > 0:
            info['min_len'] = non_null.astype(str).str.len().min()
            info['max_len'] = non_null.astype(str).str.len().max()
            info['avg_len'] = non_null.astype(str).str.len().mean()
            info['sample'] = str(non_null.iloc[0])[:50] + "..." if len(str(non_null.iloc[0])) > 50 else str(non_null.iloc[0])

    elif col.dtype in ['int64', 'float64']:
        # Numeric columns
        non_null = col.dropna()
        if len(non_null) > 0:
            info['min'] = non_null.min()
            info['max'] = non_null.max()
            info['mean'] = non_null.mean()
            info['median'] = non_null.median()

    return info

# Analyze all columns
print("\n" + "-"*100)
print(f"{'Column':<25} {'Type':<10} {'Non-Null':<10} {'Null %':<8} {'Unique':<8} {'Memory(MB)':<12}")
print("-"*100)

column_analysis = []
for col in df_train.columns:
    info = analyze_column(df_train, col)
    column_analysis.append(info)
    print(f"{info['column']:<25} {info['dtype']:<10} {info['non_null']:<10} {info['null_pct']:<8} {info['unique']:<8} {info['memory_mb']:<12.2f}")

# ============================================================
# 3) DETAILED COLUMN INSPECTION
# ============================================================
print("\n" + "="*60)
print("DETAILED COLUMN INSPECTION")
print("="*60)

# Group columns by purpose
TEXT_COLS = ['catalog_content', 'item_name', 'item_description', 'item_pack', 'item_pack_clean']
NUMERIC_COLS = ['price', 'pack_value', 'pack_count', 'has_pack']
CATEGORICAL_COLS = ['pack_unit']
ID_COLS = ['sample_id']
URL_COLS = ['image_link']

print("\n📝 TEXT COLUMNS (for embeddings):")
print("-" * 80)
for col in TEXT_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            lengths = non_null.astype(str).str.len()
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Length: min={lengths.min()}, max={lengths.max()}, avg={lengths.mean():.0f}")
            print(f"  Sample: {str(non_null.iloc[0])[:100]}...")

print("\n\n🔢 NUMERIC COLUMNS:")
print("-" * 80)
for col in NUMERIC_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Range: {non_null.min():.2f} to {non_null.max():.2f}")
            print(f"  Mean: {non_null.mean():.2f}, Median: {non_null.median():.2f}")
            print(f"  Samples: {non_null.head(5).tolist()}")

print("\n\n🏷️  CATEGORICAL COLUMNS:")
print("-" * 80)
for col in CATEGORICAL_COLS:
    if col in df_train.columns:
        print(f"\n{col}:")
        print(f"  Unique values: {df_train[col].nunique()}")
        print(f"  Top 10 values:")
        print(df_train[col].value_counts().head(10))

print("\n\n🔗 URL COLUMNS (for image embeddings):")
print("-" * 80)
for col in URL_COLS:
    if col in df_train.columns:
        non_null = df_train[col].dropna()
        if len(non_null) > 0:
            print(f"\n{col}:")
            print(f"  Coverage: {len(non_null)}/{len(df_train)} ({len(non_null)/len(df_train)*100:.1f}%)")
            print(f"  Sample URLs:")
            for url in non_null.head(3):
                print(f"    {url}")

# ============================================================
# 4) EMBEDDING STRATEGY SUMMARY
# ============================================================
print("\n" + "="*60)
print("EMBEDDING STRATEGY")
print("="*60)

print("\n📊 Based on data type analysis:\n")

print("✅ TEXT EMBEDDINGS (E5-large-v2, 1024 dims):")
print("   Primary: catalog_content (full text)")
print("   OR combine: item_name + item_description")
print("   → Recommendation: Use catalog_content (most complete)")

print("\n✅ IMAGE EMBEDDINGS (ViT-L/14, 768 dims):")
print("   Source: image_link URLs")
print("   → Download images from URLs → Encode with OpenCLIP")

print("\n📦 Strategy Decision:")
print("   Option 1: Use catalog_content (raw) → Best for completeness")
print("   Option 2: Use item_name + ' ' + item_description → Best for parsed data")
print("   Option 3: Use BOTH as separate embeddings → Most features")
print("\n   💡 Recommendation: Option 1 (catalog_content) - simpler & complete")

# ============================================================
# 5) CHECK CONSISTENCY ACROSS SPLITS
# ============================================================
print("\n" + "="*60)
print("CONSISTENCY CHECK: Train vs Val vs Test")
print("="*60)

print(f"\n{'Column':<25} {'Train Coverage':<15} {'Val Coverage':<15} {'Test Coverage':<15}")
print("-" * 70)

for col in df_train.columns:
    if col != 'price':  # price not in test
        train_cov = f"{(~df_train[col].isna()).sum()}/{len(df_train)}"
        val_cov = f"{(~df_val[col].isna()).sum()}/{len(df_val)}"
        test_cov = f"{(~df_test[col].isna()).sum()}/{len(df_test)}" if col in df_test.columns else "N/A"
        print(f"{col:<25} {train_cov:<15} {val_cov:<15} {test_cov:<15}")

# ============================================================
# 6) SUMMARY & NEXT STEPS
# ============================================================
print("\n" + "="*60)
print("✅ STEP 2.6 COMPLETE")
print("="*60)

print("\n📋 Data Types Summary:")
print(f"  • Text columns: {len(TEXT_COLS)} (for text embeddings)")
print(f"  • Numeric columns: {len(NUMERIC_COLS)} (direct features)")
print(f"  • Categorical columns: {len(CATEGORICAL_COLS)} (need encoding)")
print(f"  • URL columns: {len(URL_COLS)} (for image embeddings)")
print(f"  • ID columns: {len(ID_COLS)} (tracking only)")

print("\n🎯 Ready for Step 3:")
print("  → Generate Text Embeddings (E5-large-v2)")
print("  → Generate Image Embeddings (ViT-L/14)")
print("  → Use your original pipeline with these inputs!")

print(f"\n💾 Key text field for embeddings: 'catalog_content'")
print(f"💾 Key image field for embeddings: 'image_link'")

# ============================================================
# STEP 3A (PART 1): TEXT EMBEDDINGS — E5-LARGE-V2 (Fixed, Uses item_name + item_description)
# ============================================================
!pip install -q sentence-transformers tqdm torch

import os, gc, numpy as np, pandas as pd, torch
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# ---------------------------
# Config
# ---------------------------
BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
EMB_DIR   = BASE_DIR / "embeddings" / "e5_large"
EMB_DIR.mkdir(parents=True, exist_ok=True)

DEVICE   = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_ID = "intfloat/e5-large-v2"
MAX_LEN  = 512
BATCH_SZ = 32

print(f"Using device: {DEVICE}")
print(f"Model: {MODEL_ID}")

# ---------------------------
# Load data & split indices
# ---------------------------
print("\n📂 Loading processed parquet data...")
train_df = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
val_df   = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
test_df  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

print(f"✅ Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

# ---------------------------
# Combine fields for embedding
# ---------------------------
def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

def prep_texts(df):
    texts = ["passage: " + combine_text(n, d)
             for n, d in zip(df["item_name"], df["item_description"])]
    return texts

texts_train = prep_texts(train_df)
texts_val   = prep_texts(val_df)
texts_test  = prep_texts(test_df)

print("Example text:\n", texts_train[0][:250])

# ---------------------------
# Load model
# ---------------------------
model = SentenceTransformer(MODEL_ID, device=DEVICE)
model.max_seq_length = MAX_LEN
model.eval()

# ---------------------------
# Encode helper
# ---------------------------
@torch.no_grad()
def encode_texts(texts, batch_size=BATCH_SZ):
    out = []
    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i+batch_size]
        emb = model.encode(
            batch,
            batch_size=len(batch),
            device=DEVICE,
            convert_to_numpy=True,
            normalize_embeddings=True,   # L2-normalized
            show_progress_bar=False
        )
        out.append(emb)
    return np.vstack(out)

# ---------------------------
# Encode & save
# ---------------------------
print("\n⚙️ Encoding train texts...")
X_text_tr = encode_texts(texts_train)
print("⚙️ Encoding val texts...")
X_text_val = encode_texts(texts_val)
print("⚙️ Encoding test texts...")
X_text_te = encode_texts(texts_test)

print("\n💾 Saving embeddings...")
np.save(EMB_DIR / "X_text_tr_e5l2.npy", X_text_tr)
np.save(EMB_DIR / "X_text_val_e5l2.npy", X_text_val)
np.save(EMB_DIR / "X_text_te_e5l2.npy", X_text_te)

meta = {
    "model": MODEL_ID,
    "pooling": "mean",
    "normalization": "L2",
    "max_len": MAX_LEN,
    "prefix": "passage:",
    "input_fields": ["item_name", "item_description"],
    "dims": int(X_text_tr.shape[1]),
}
pd.Series(meta).to_json(EMB_DIR / "meta.json", indent=2)

print(f"\n Done. Shapes:")
print(f"  Train: {X_text_tr.shape}")
print(f"  Val  : {X_text_val.shape}")
print(f"  Test : {X_text_te.shape}")
print(f"Embeddings saved to: {EMB_DIR}")

!pip install -q transformers accelerate datasets evaluate torch tqdm

import os, gc, numpy as np, pandas as pd, torch
from pathlib import Path
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding,
    EarlyStoppingCallback
)
import evaluate
from tqdm import tqdm

# ---------------------------
# Config
# ---------------------------
BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
MODEL_DIR = BASE_DIR / "models" / "deberta_v3_price_reg"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

MODEL_ID  = "microsoft/deberta-v3-base"
DEVICE    = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN   = 256
SEED      = 42

# ---------------------------
# Load data
# ---------------------------
train_df = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
val_df   = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")

# Target transform
train_df["log_price"] = np.log1p(train_df["price"])
val_df["log_price"]   = np.log1p(val_df["price"])

def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

train_df["text"] = [combine_text(n, d) for n, d in zip(train_df["item_name"], train_df["item_description"])]
val_df["text"]   = [combine_text(n, d) for n, d in zip(val_df["item_name"], val_df["item_description"])]

print(f"✅ Train: {len(train_df)}, Val: {len(val_df)}")

# ---------------------------
# Tokenization
# ---------------------------
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

def tokenize(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LEN,
    )

train_ds = Dataset.from_pandas(train_df[["text", "log_price"]])
val_ds   = Dataset.from_pandas(val_df[["text", "log_price"]])
raw_datasets = DatasetDict({"train": train_ds, "validation": val_ds})
tokenized_datasets = raw_datasets.map(tokenize, batched=True, remove_columns=["text"])
tokenized_datasets = tokenized_datasets.rename_column("log_price", "labels")
tokenized_datasets.set_format("torch")

# ---------------------------
# Metrics
# ---------------------------
mae_metric = evaluate.load("mae")

def smape(preds, labels):
    return np.mean(
        np.abs(preds - labels) / ((np.abs(preds) + np.abs(labels)) / 2 + 1e-8)
    ) * 100

def compute_metrics(eval_pred):
    preds = eval_pred.predictions.squeeze()
    labels = eval_pred.label_ids.squeeze()
    mae_val = mae_metric.compute(predictions=preds, references=labels)["mae"]
    smape_val = smape(np.expm1(preds), np.expm1(labels))
    return {"mae": mae_val, "smape": smape_val}

# ---------------------------
# Model + Trainer
# ---------------------------
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_ID, num_labels=1, problem_type="regression"
).to(DEVICE)

args = TrainingArguments(
    output_dir=str(MODEL_DIR),
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=6,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="mae",
    greater_is_better=False,
    fp16=True,
    logging_dir=str(MODEL_DIR / "logs"),
    logging_strategy="steps",
    logging_steps=100,
    seed=SEED,
    warmup_ratio=0.1,              #
    lr_scheduler_type="cosine",
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()

trainer.save_model(str(MODEL_DIR))
tokenizer.save_pretrained(str(MODEL_DIR))
print(f"\n✅ Model and tokenizer saved to: {MODEL_DIR}")

import numpy as np, pandas as pd, torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm

BASE_DIR  = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
MODEL_DIR = BASE_DIR / "models" / "deberta_v3_price_reg"

DEVICE  = "cuda" if torch.cuda.is_available() else "cpu"
MAX_LEN = 256
BATCH   = 16

# ---------- load best checkpoint ----------
tokenizer = AutoTokenizer.from_pretrained(str(MODEL_DIR))
model = AutoModelForSequenceClassification.from_pretrained(str(MODEL_DIR), num_labels=1)
model.to(DEVICE).eval()

def combine_text(name, desc):
    name = str(name).strip() if pd.notna(name) else ""
    desc = str(desc).strip() if pd.notna(desc) else ""
    return (name + ". " + desc).strip()

def prepare_texts(df):
    return [combine_text(n, d) for n, d in zip(df["item_name"], df["item_description"])]

# ---------- mean-pool helper ----------
def mean_pool(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    return (last_hidden_state * mask).sum(1) / mask.sum(1).clamp(min=1e-9)

@torch.no_grad()
def encode_texts(texts):
    outs = []
    for i in tqdm(range(0, len(texts), BATCH), desc="DeBERTa encode"):
        batch = texts[i:i+BATCH]
        toks = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors="pt").to(DEVICE)
        # Use the backbone’s hidden states (ignore the regression head)
        hidden = model.deberta(**toks)[0]                 # last_hidden_state [B, L, H]
        emb = mean_pool(hidden, toks["attention_mask"])   # [B, H]
        emb = torch.nn.functional.normalize(emb, p=2, dim=1)  # L2 for safety
        outs.append(emb.detach().cpu().numpy())
    return np.vstack(outs)

# ---------- load splits ----------
tr = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
va = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
te = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

txt_tr = prepare_texts(tr)
txt_va = prepare_texts(va)
txt_te = prepare_texts(te)

X_deb_tr  = encode_texts(txt_tr)
X_deb_val = encode_texts(txt_va)
X_deb_te  = encode_texts(txt_te)

np.save(MODEL_DIR / "X_text_train_deberta.npy", X_deb_tr)
np.save(MODEL_DIR / "X_text_val_deberta.npy", X_deb_val)
np.save(MODEL_DIR / "X_text_test_deberta.npy", X_deb_te)

print("Shapes:", X_deb_tr.shape, X_deb_val.shape, X_deb_te.shape)
print("Saved to:", MODEL_DIR)

# ===========================================
# STEP 4 (FAST) — IMAGE EMBEDDINGS FROM CACHE (OpenCLIP ViT-L/14)
# ===========================================
!pip -q install open_clip_torch pillow tqdm

import os, gc, json, numpy as np, pandas as pd, torch
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torch.nn.functional as F
import open_clip
from torch.utils.data import Dataset, DataLoader

# ---------------- Config ----------------
SEED       = 42
DEVICE     = "cuda" if torch.cuda.is_available() else "cpu"

BASE_DIR   = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR  = BASE_DIR / "splits"
EMB_DIR    = BASE_DIR / "embeddings" / "clip_vitl14_fast"
IMG_DIR    = BASE_DIR / "images_cache"
EMB_DIR.mkdir(parents=True, exist_ok=True)

MODEL_ARCH = "ViT-L-14"
PRETRAINED = "openai"
BATCH_SIZE = 192          # tune up to 224–256 if VRAM allows
NUM_WORKERS = 8
torch.manual_seed(SEED)
torch.backends.cudnn.benchmark = True    # allow autotune
torch.backends.cudnn.deterministic = False

# ---------------- Load splits ----------------
tr = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
va = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
te = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

# ---------------- Model ----------------
print("Loading OpenCLIP:", MODEL_ARCH, PRETRAINED)
model, _, preprocess = open_clip.create_model_and_transforms(MODEL_ARCH, pretrained=PRETRAINED)
model = model.to(DEVICE).eval().half()

# ---------------- Dataset ----------------
class ImgDataset(Dataset):
    def __init__(self, df, cache_dir, preprocess):
        self.paths = [(cache_dir / (str(row["image_link"]) \
                        if str(row["image_link"]).endswith(".jpg") else
                        (hashlib.md5(str(row["image_link"]).encode()).hexdigest() + ".jpg")))
                      for _, row in df.iterrows()]
        self.preprocess = preprocess

    def __len__(self): return len(self.paths)

    def __getitem__(self, i):
        p = self.paths[i]
        if not p.exists():
            img = Image.new("RGB", (224,224), (255,255,255))
        else:
            try:
                img = Image.open(p).convert("RGB")
            except Exception:
                img = Image.new("RGB", (224,224), (255,255,255))
        return self.preprocess(img)

def make_loader(df):
    return DataLoader(ImgDataset(df, IMG_DIR, preprocess),
                      batch_size=BATCH_SIZE,
                      num_workers=NUM_WORKERS,
                      pin_memory=True,
                      shuffle=False,
                      prefetch_factor=2)

@torch.no_grad()
def encode_split(df, split_name):
    dl = make_loader(df)
    sample = next(iter(dl)).to(DEVICE, dtype=torch.float16)
    d = model.encode_image(sample).shape[-1]; del sample
    mmap_path = EMB_DIR / f"img_emb_{split_name}_vitl14_fast.mmap"
    emb = np.memmap(mmap_path, dtype="float32", mode="w+", shape=(len(df), d))

    off = 0
    for batch in tqdm(dl, desc=f"Encoding {split_name}", total=len(dl)):
        batch = batch.to(DEVICE, dtype=torch.float16, non_blocking=True)
        z = model.encode_image(batch)
        z = F.normalize(z.float(), dim=-1).cpu().numpy()
        emb[off:off+z.shape[0]] = z
        off += z.shape[0]
    del emb; gc.collect()
    arr = np.memmap(mmap_path, dtype="float32", mode="r", shape=(len(df), d))
    np.save(EMB_DIR / f"X_img_{split_name}_clip.npy", np.asarray(arr, dtype="float32"))
    print(f"✅ Saved X_img_{split_name}_clip.npy  → {arr.shape}")
    return arr

# ---------------- Encode all splits ----------------
X_img_tr  = encode_split(tr, "tr")
X_img_val = encode_split(va, "val")
X_img_te  = encode_split(te, "te")

# ---------------- Log meta ----------------
log = {
    "arch": MODEL_ARCH,
    "pretrained": PRETRAINED,
    "batch_size": BATCH_SIZE,
    "num_workers": NUM_WORKERS,
    "device": DEVICE,
    "n_train": len(tr),
    "n_val": len(va),
    "n_test": len(te),
}
with open(EMB_DIR / "image_embed_log.json", "w") as f:
    json.dump(log, f, indent=2)
print(" Fast embedding run complete and logged.")

# ===========================================
# FUSION LGBM — CLIP ⊕ E5 ⊕ DeBERTa ⊕ PACK  (heavy reg, MAE + SMAPE)
# ===========================================
!pip -q install lightgbm joblib pyarrow

import os, json, gc, numpy as np, pandas as pd, joblib
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb

# -------- Config & paths --------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
EMB_DIRS_CLIP = [BASE_DIR / "embeddings" / "clip_vitl14_fast",
                 BASE_DIR / "embeddings" / "clip_vitl14"]
E5_DIR   = BASE_DIR / "embeddings" / "e5_large"
DEB_DIR  = BASE_DIR / "models" / "deberta_v3_price_reg"

OUT_DIR  = BASE_DIR / "experiments" / "fusion_lgbm_quicktest"
OUT_DIR.mkdir(parents=True, exist_ok=True)

N_PCA_CLIP = 64
N_PCA_E5   = 64
N_PCA_DEB  = 37
SEED = 42

# -------- Helpers --------
def smape(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=np.float64)
    y_pred = np.asarray(y_pred, dtype=np.float64)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    # avoid 0/0
    mask = denom != 0
    out = np.zeros_like(denom)
    out[mask] = np.abs(y_true[mask] - y_pred[mask]) / denom[mask]
    return out.mean() * 100.0

def first_existing(*paths):
    for p in paths:
        if p.exists():
            return p
    raise FileNotFoundError(f"None of the provided paths exist: {paths}")

def load_clip_arrays():
    d = None
    for trydir in EMB_DIRS_CLIP:
        if (trydir / "X_img_tr_clip.npy").exists():
            d = trydir
            break
    if d is None:
        raise FileNotFoundError("Could not find CLIP embeddings in clip_vitl14_fast/ or clip_vitl14/.")
    X_tr = np.load(d / "X_img_tr_clip.npy")
    X_val = np.load(d / "X_img_val_clip.npy")
    X_te = np.load(d / "X_img_te_clip.npy")
    return X_tr, X_val, X_te, d

# -------- Load processed splits & targets --------
df_tr  = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
df_val = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
df_te  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

assert "price" in df_tr.columns, "price missing in df_train_processed.parquet"
y_tr  = df_tr["price"].astype(float).values
y_val = df_val["price"].astype(float).values
y_tr_log  = np.log1p(y_tr).astype("float32")
y_val_log = np.log1p(y_val).astype("float32")

# -------- Load embeddings --------
# CLIP (image)
Ximg_tr, Ximg_val, Ximg_te, CLIP_FOLDER = load_clip_arrays()


X_e5_tr  = np.load(E5_DIR / "X_text_tr_e5l2.npy")
X_e5_val = np.load(E5_DIR / "X_text_val_e5l2.npy")
X_e5_te  = np.load(E5_DIR / "X_text_te_e5l2.npy")

# DeBERTa (text, from fine-tuned model export)
X_deb_tr  = np.load(DEB_DIR / "X_text_train_deberta.npy")
X_deb_val = np.load(DEB_DIR / "X_text_val_deberta.npy")
X_deb_te  = np.load(DEB_DIR / "X_text_test_deberta.npy")

# -------- Pack features (from processed parquets) --------
# Available from your earlier parsing step:
# has_pack, pack_value, pack_unit, pack_count, plus any *_is_missing flags if present
pack_cols_num = [c for c in ["pack_value", "pack_count"] if c in df_tr.columns]
pack_cols_bin = [c for c in ["has_pack"] if c in df_tr.columns]

def extract_pack(df):
    Xn = df[pack_cols_num].astype("float32") if pack_cols_num else pd.DataFrame(index=df.index)
    Xb = df[pack_cols_bin].astype("float32") if pack_cols_bin else pd.DataFrame(index=df.index)
    # Optional one-hot for pack_unit if present and small cardinality
    if "pack_unit" in df.columns:
        vc = df["pack_unit"].fillna("unknown").astype(str)
        if vc.nunique() <= 25:
            oh = pd.get_dummies(vc, prefix="unit", dtype="float32")
        else:
            # fallback: keep as 'unknown' to avoid huge one-hot
            oh = pd.get_dummies(pd.Series(["unknown"]*len(df), index=df.index), prefix="unit", dtype="float32")
    else:
        oh = pd.DataFrame(index=df.index)
    X = pd.concat([Xn, Xb, oh], axis=1)
    return X

Xpack_tr  = extract_pack(df_tr)
Xpack_val = extract_pack(df_val)
Xpack_te  = extract_pack(df_te)

# Standardize numeric pack cols only (leave one-hots/binary as is)
if len(pack_cols_num):
    scaler = StandardScaler()
    Xpack_tr.loc[:, pack_cols_num]  = scaler.fit_transform(Xpack_tr[pack_cols_num])
    Xpack_val.loc[:, pack_cols_num] = scaler.transform(Xpack_val[pack_cols_num])
    Xpack_te.loc[:, pack_cols_num]  = scaler.transform(Xpack_te[pack_cols_num])
    joblib.dump(scaler, OUT_DIR / "scaler_pack_num.joblib")

# Ensure same column order across splits
Xpack_tr = Xpack_tr.reindex(columns=sorted(Xpack_tr.columns)).fillna(0).astype("float32")
Xpack_val = Xpack_val.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")
Xpack_te = Xpack_te.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")

# -------- PCA per modality (fit on train only) --------
def fit_pca(name, Xtrain, Xval, Xtest, n_comp):
    # guard for tiny n_comp > dim
    n_comp_eff = min(n_comp, Xtrain.shape[1])
    pca = PCA(n_components=n_comp_eff, svd_solver="auto", random_state=SEED)
    Ztr  = pca.fit_transform(Xtrain)
    Zval = pca.transform(Xval)
    Zte  = pca.transform(Xtest)
    joblib.dump(pca, OUT_DIR / f"pca_{name}.joblib")
    return Ztr.astype("float32"), Zval.astype("float32"), Zte.astype("float32")

Zimg_tr, Zimg_val, Zimg_te = fit_pca("clip", Ximg_tr, Ximg_val, Ximg_te, N_PCA_CLIP)
Ze5_tr,  Ze5_val,  Ze5_te  = fit_pca("e5",   X_e5_tr,  X_e5_val,  X_e5_te,  N_PCA_E5)
Zdeb_tr, Zdeb_val, Zdeb_te = fit_pca("deb",  X_deb_tr, X_deb_val, X_deb_te, N_PCA_DEB)

# -------- Concatenate fused features --------
def hcat(*Xs): return np.hstack([np.asarray(X, dtype="float32") for X in Xs])

X_tr_fused  = hcat(Zimg_tr, Ze5_tr, Zdeb_tr, Xpack_tr.values)
X_val_fused = hcat(Zimg_val, Ze5_val, Zdeb_val, Xpack_val.values)
X_te_fused  = hcat(Zimg_te, Ze5_te, Zdeb_te, Xpack_te.values)

print("Fused shapes ->",
      "train:", X_tr_fused.shape,
      "val:", X_val_fused.shape,
      "test:", X_te_fused.shape)

# -------- LightGBM (heavy regularization) --------
lgb_params = {
    "objective": "regression_l1",      # MAE on log(price)
    "metric": "mae",
    "learning_rate": 0.03,
    "num_leaves": 31,                  # smaller leaves
    "min_data_in_leaf": 600,           # heavier min data per leaf
    "feature_fraction": 0.45,
    "bagging_fraction": 0.7,
    "bagging_freq": 1,
    "lambda_l1": 4.0,                  # stronger L1
    "lambda_l2": 12.0,                 # stronger L2
    "max_depth": -1,
    "verbosity": -1,
    "seed": SEED
}

ds_tr  = lgb.Dataset(X_tr_fused, label=y_tr_log, free_raw_data=False)
ds_val = lgb.Dataset(X_val_fused, label=y_val_log, reference=ds_tr, free_raw_data=False)

# Define early stopping callback
early_stop_callback = lgb.early_stopping(stopping_rounds=100, verbose=True)
log_callback = lgb.log_evaluation(period=250)

gbm = lgb.train(
    params=lgb_params,
    train_set=ds_tr,
    valid_sets=[ds_tr, ds_val],
    valid_names=["train","val"],
    num_boost_round=10000,
    callbacks=[early_stop_callback, log_callback]
)

# -------- Predictions (log-space and back-transform) --------
pred_val_log = gbm.predict(X_val_fused, num_iteration=gbm.best_iteration)
pred_te_log  = gbm.predict(X_te_fused,  num_iteration=gbm.best_iteration)

pred_val = np.expm1(pred_val_log).clip(0)
pred_te  = np.expm1(pred_te_log).clip(0)

# -------- Metrics --------
from sklearn.metrics import mean_absolute_error
val_mae   = mean_absolute_error(y_val, pred_val)
val_smape = smape(y_val, pred_val)

print(f"\n===== Fusion LGBM (Heavy-Reg) — Validation =====")
print(f"Best iter: {gbm.best_iteration}")
print(f"MAE       : {val_mae:,.4f}")
print(f"SMAPE (%) : {val_smape:,.4f}")

# -------- Save artifacts --------
np.save(OUT_DIR / "pred_val_fusion_lgb.npy", pred_val)
np.save(OUT_DIR / "pred_val_log_fusion_lgb.npy", pred_val_log)
np.save(OUT_DIR / "pred_test_log_fusion_lgb.npy", pred_te_log)
np.save(OUT_DIR / "pred_test_fusion_lgb.npy", pred_te)

gbm.save_model(str(OUT_DIR / "fusion_lgbm.txt"))

meta = {
    "clip_folder": str(CLIP_FOLDER),
    "n_pca": {"clip": N_PCA_CLIP, "e5": N_PCA_E5, "deb": N_PCA_DEB},
    "pack_cols": list(Xpack_tr.columns),
    "best_iteration": int(gbm.best_iteration),
    "val_mae": float(val_mae),
    "val_smape": float(val_smape)
}
with open(OUT_DIR / "run_meta.json", "w") as f:
    json.dump(meta, f, indent=2)

print("\nArtifacts saved to:", OUT_DIR)
gc.collect();

# ===========================================
# FUSION Multi-GBM — CLIP ⊕ E5 ⊕ DeBERTa ⊕ PACK
# Train LightGBM, XGBoost, and CatBoost
# ===========================================
!pip -q install lightgbm xgboost catboost joblib pyarrow

import os, json, gc, numpy as np, pandas as pd, joblib
from pathlib import Path
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, Pool

# -------- Config & paths --------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
EMB_DIRS_CLIP = [BASE_DIR / "embeddings" / "clip_vitl14_fast",
                 BASE_DIR / "embeddings" / "clip_vitl14"]
E5_DIR   = BASE_DIR / "embeddings" / "e5_large"
DEB_DIR  = BASE_DIR / "models" / "deberta_v3_price_reg"

OUT_DIR  = BASE_DIR / "experiments" / "fusion_multi_gbm"
OUT_DIR.mkdir(parents=True, exist_ok=True)

N_PCA_CLIP = 128
N_PCA_E5   = 128
N_PCA_DEB  = 128
SEED = 42

# -------- Helpers --------
def smape(y_true, y_pred):
    y_true = np.asarray(y_true, dtype=np.float64)
    y_pred = np.asarray(y_pred, dtype=np.float64)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    mask = denom != 0
    out = np.zeros_like(denom)
    out[mask] = np.abs(y_true[mask] - y_pred[mask]) / denom[mask]
    return out.mean() * 100.0

def load_clip_arrays():
    d = None
    for trydir in EMB_DIRS_CLIP:
        if (trydir / "X_img_tr_clip.npy").exists():
            d = trydir
            break
    if d is None:
        raise FileNotFoundError("Could not find CLIP embeddings in clip_vitl14_fast/ or clip_vitl14/.")
    X_tr = np.load(d / "X_img_tr_clip.npy")
    X_val = np.load(d / "X_img_val_clip.npy")
    X_te = np.load(d / "X_img_te_clip.npy")
    return X_tr, X_val, X_te, d

# -------- Load processed splits & targets --------
df_tr  = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
df_val = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
df_te  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

assert "price" in df_tr.columns, "price missing in df_train_processed.parquet"
y_tr  = df_tr["price"].astype(float).values
y_val = df_val["price"].astype(float).values
y_tr_log  = np.log1p(y_tr).astype("float32")
y_val_log = np.log1p(y_val).astype("float32")

# -------- Load embeddings --------
Ximg_tr, Ximg_val, Ximg_te, CLIP_FOLDER = load_clip_arrays()

X_e5_tr  = np.load(E5_DIR / "X_text_tr_e5l2.npy")
X_e5_val = np.load(E5_DIR / "X_text_val_e5l2.npy")
X_e5_te  = np.load(E5_DIR / "X_text_te_e5l2.npy")

X_deb_tr  = np.load(DEB_DIR / "X_text_train_deberta.npy")
X_deb_val = np.load(DEB_DIR / "X_text_val_deberta.npy")
X_deb_te  = np.load(DEB_DIR / "X_text_test_deberta.npy")

# -------- Pack features --------
pack_cols_num = [c for c in ["pack_value", "pack_count"] if c in df_tr.columns]
pack_cols_bin = [c for c in ["has_pack"] if c in df_tr.columns]

def extract_pack(df):
    Xn = df[pack_cols_num].astype("float32") if pack_cols_num else pd.DataFrame(index=df.index)
    Xb = df[pack_cols_bin].astype("float32") if pack_cols_bin else pd.DataFrame(index=df.index)
    if "pack_unit" in df.columns:
        vc = df["pack_unit"].fillna("unknown").astype(str)
        if vc.nunique() <= 25:
            oh = pd.get_dummies(vc, prefix="unit", dtype="float32")
        else:
            oh = pd.get_dummies(pd.Series(["unknown"]*len(df), index=df.index), prefix="unit", dtype="float32")
    else:
        oh = pd.DataFrame(index=df.index)
    X = pd.concat([Xn, Xb, oh], axis=1)
    return X

Xpack_tr  = extract_pack(df_tr)
Xpack_val = extract_pack(df_val)
Xpack_te  = extract_pack(df_te)

if len(pack_cols_num):
    scaler = StandardScaler()
    Xpack_tr.loc[:, pack_cols_num]  = scaler.fit_transform(Xpack_tr[pack_cols_num])
    Xpack_val.loc[:, pack_cols_num] = scaler.transform(Xpack_val[pack_cols_num])
    Xpack_te.loc[:, pack_cols_num]  = scaler.transform(Xpack_te[pack_cols_num])
    joblib.dump(scaler, OUT_DIR / "scaler_pack_num.joblib")

Xpack_tr = Xpack_tr.reindex(columns=sorted(Xpack_tr.columns)).fillna(0).astype("float32")
Xpack_val = Xpack_val.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")
Xpack_te = Xpack_te.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")

# -------- PCA per modality --------
def fit_pca(name, Xtrain, Xval, Xtest, n_comp):
    n_comp_eff = min(n_comp, Xtrain.shape[1])
    pca = PCA(n_components=n_comp_eff, svd_solver="auto", random_state=SEED)
    Ztr  = pca.fit_transform(Xtrain)
    Zval = pca.transform(Xval)
    Zte  = pca.transform(Xtest)
    joblib.dump(pca, OUT_DIR / f"pca_{name}.joblib")
    return Ztr.astype("float32"), Zval.astype("float32"), Zte.astype("float32")

Zimg_tr, Zimg_val, Zimg_te = fit_pca("clip", Ximg_tr, Ximg_val, Ximg_te, N_PCA_CLIP)
Ze5_tr,  Ze5_val,  Ze5_te  = fit_pca("e5",   X_e5_tr,  X_e5_val,  X_e5_te,  N_PCA_E5)
Zdeb_tr, Zdeb_val, Zdeb_te = fit_pca("deb",  X_deb_tr, X_deb_val, X_deb_te, N_PCA_DEB)

# -------- Concatenate fused features --------
def hcat(*Xs): return np.hstack([np.asarray(X, dtype="float32") for X in Xs])

X_tr_fused  = hcat(Zimg_tr, Ze5_tr, Zdeb_tr, Xpack_tr.values)
X_val_fused = hcat(Zimg_val, Ze5_val, Zdeb_val, Xpack_val.values)
X_te_fused  = hcat(Zimg_te, Ze5_te, Zdeb_te, Xpack_te.values)

print("Fused shapes ->",
      "train:", X_tr_fused.shape,
      "val:", X_val_fused.shape,
      "test:", X_te_fused.shape)

# ====================================================================
# 1. LIGHTGBM
# ====================================================================
print("\n" + "="*60)
print("TRAINING LIGHTGBM")
print("="*60)

"""lgb_params = {
    "objective": "regression_l1",
    "metric": "mae",
    "learning_rate": 0.03,
    "num_leaves": 28,
    "min_data_in_leaf": 400,
    "feature_fraction": 0.45,
    "bagging_fraction": 0.7,
    "bagging_freq": 1,
    "lambda_l1": 4.0,
    "lambda_l2": 12.0,
    "max_depth": -1,
    "verbosity": -1,
    "seed": SEED
}"""
lgb_params = {
    "objective": "regression_l1",
    "metric": "mae",
    "learning_rate": 0.02,         # <-- SLOWER: Learns more cautiously.
    "num_leaves": 21,              # <-- REDUCED: Forces much simpler trees.
    "min_data_in_leaf": 600,       # <-- INCREASED: Prevents learning from tiny, noisy groups.
    "feature_fraction": 0.4,       # <-- REDUCED: Uses fewer features per tree.
    "bagging_fraction": 0.6,
    "bagging_freq": 1,
    "lambda_l1": 25.0,             # <-- INCREASED: Stronger L1 penalty.
    "lambda_l2": 50.0,             # <-- INCREASED: Much stronger L2 penalty.
    "max_depth": -1,
    "verbosity": -1,
    "seed": SEED
}
ds_tr  = lgb.Dataset(X_tr_fused, label=y_tr_log, free_raw_data=False)
ds_val = lgb.Dataset(X_val_fused, label=y_val_log, reference=ds_tr, free_raw_data=False)

early_stop_lgb = lgb.early_stopping(stopping_rounds=100, verbose=True)
log_lgb = lgb.log_evaluation(period=250)

gbm_lgb = lgb.train(
    params=lgb_params,
    train_set=ds_tr,
    valid_sets=[ds_tr, ds_val],
    valid_names=["train","val"],
    num_boost_round=10000,
    callbacks=[early_stop_lgb, log_lgb]
)

pred_val_lgb = np.expm1(gbm_lgb.predict(X_val_fused, num_iteration=gbm_lgb.best_iteration)).clip(0)
pred_te_lgb  = np.expm1(gbm_lgb.predict(X_te_fused, num_iteration=gbm_lgb.best_iteration)).clip(0)

mae_lgb = mean_absolute_error(y_val, pred_val_lgb)
smape_lgb = smape(y_val, pred_val_lgb)

print(f"\nLightGBM Results:")
print(f"  Best iter: {gbm_lgb.best_iteration}")
print(f"  MAE      : {mae_lgb:,.4f}")
print(f"  SMAPE    : {smape_lgb:.4f}%")

gbm_lgb.save_model(str(OUT_DIR / "lightgbm_model.txt"))
np.save(OUT_DIR / "pred_val_lgb.npy", pred_val_lgb)
np.save(OUT_DIR / "pred_test_lgb.npy", pred_te_lgb)

# ====================================================================
# 2. XGBOOST
# ====================================================================
print("\n" + "="*60)
print("TRAINING XGBOOST")
print("="*60)

"""xgb_params = {
    "objective": "reg:absoluteerror",
    "eval_metric": "mae",
    "learning_rate": 0.03,
    "max_depth": 6,
    "min_child_weight": 600,
    "subsample": 0.7,
    "colsample_bytree": 0.45,
    "reg_alpha": 4.0,
    "reg_lambda": 12.0,
    "tree_method": "hist",
    "seed": SEED,
    "verbosity": 0
}"""

xgb_params = {
    "objective": "reg:absoluteerror",
    "eval_metric": "mae",
    "learning_rate": 0.02,         # <-- SLOWER: Consistent with LGBM.
    "max_depth": 5,                # <-- REDUCED: Simpler trees (instead of 6).
    "min_child_weight": 800,       # <-- INCREASED: More conservative than before.
    "subsample": 0.6,
    "colsample_bytree": 0.4,
    "reg_alpha": 25.0,             # <-- INCREASED: Stronger L1 penalty.
    "reg_lambda": 50.0,            # <-- INCREASED: Much stronger L2 penalty.
    "tree_method": "hist",
    "seed": SEED,
    "verbosity": 0
}

dtrain = xgb.DMatrix(X_tr_fused, label=y_tr_log)
dval = xgb.DMatrix(X_val_fused, label=y_val_log)
dtest = xgb.DMatrix(X_te_fused)

evals = [(dtrain, "train"), (dval, "val")]

gbm_xgb = xgb.train(
    params=xgb_params,
    dtrain=dtrain,
    num_boost_round=10000,
    evals=evals,
    early_stopping_rounds=100,
    verbose_eval=250
)

pred_val_xgb = np.expm1(gbm_xgb.predict(dval, iteration_range=(0, gbm_xgb.best_iteration+1))).clip(0)
pred_te_xgb  = np.expm1(gbm_xgb.predict(dtest, iteration_range=(0, gbm_xgb.best_iteration+1))).clip(0)

mae_xgb = mean_absolute_error(y_val, pred_val_xgb)
smape_xgb = smape(y_val, pred_val_xgb)

print(f"\nXGBoost Results:")
print(f"  Best iter: {gbm_xgb.best_iteration}")
print(f"  MAE      : {mae_xgb:,.4f}")
print(f"  SMAPE    : {smape_xgb:.4f}%")

gbm_xgb.save_model(str(OUT_DIR / "xgboost_model.json"))
np.save(OUT_DIR / "pred_val_xgb.npy", pred_val_xgb)
np.save(OUT_DIR / "pred_test_xgb.npy", pred_te_xgb)

# ====================================================================
# 3. CATBOOST
# ====================================================================
print("\n" + "="*60)
print("TRAINING CATBOOST")
print("="*60)

"""cat_params = {
    "loss_function": "MAE",
    "eval_metric": "MAE",
    "learning_rate": 0.03,
    "depth": 6,
    "min_data_in_leaf": 600,
    "subsample": 0.7,
    "rsm": 0.45,  # colsample equivalent
    "l2_leaf_reg": 12.0,
    "random_seed": SEED,
    "verbose": 250,
    "early_stopping_rounds": 100,
    "iterations": 10000
}"""
cat_params = {
    "loss_function": "MAE",
    "eval_metric": "MAE",
    "learning_rate": 0.02,         # <-- SLOWER: Consistent with others.
    "depth": 5,                    # <-- REDUCED: Simpler trees (instead of 6).
    "min_data_in_leaf": 800,       # <-- INCREASED: More conservative.
    "subsample": 0.6,
    "rsm": 0.4,                    # <-- Colsample equivalent, reduced slightly.
    "l2_leaf_reg": 50.0,           # <-- INCREASED: Much stronger L2 penalty.
    "random_seed": SEED,
    "verbose": 250,
    "early_stopping_rounds": 100,
    "iterations": 10000
}

train_pool = Pool(X_tr_fused, y_tr_log)
val_pool = Pool(X_val_fused, y_val_log)

gbm_cat = CatBoostRegressor(**cat_params)
gbm_cat.fit(
    train_pool,
    eval_set=val_pool,
    use_best_model=True,
    verbose=250
)

pred_val_cat = np.expm1(gbm_cat.predict(X_val_fused)).clip(0)
pred_te_cat  = np.expm1(gbm_cat.predict(X_te_fused)).clip(0)

mae_cat = mean_absolute_error(y_val, pred_val_cat)
smape_cat = smape(y_val, pred_val_cat)

print(f"\nCatBoost Results:")
print(f"  Best iter: {gbm_cat.best_iteration_}")
print(f"  MAE      : {mae_cat:,.4f}")
print(f"  SMAPE    : {smape_cat:.4f}%")

gbm_cat.save_model(str(OUT_DIR / "catboost_model.cbm"))
np.save(OUT_DIR / "pred_val_cat.npy", pred_val_cat)
np.save(OUT_DIR / "pred_test_cat.npy", pred_te_cat)

# ====================================================================
# SUMMARY & ENSEMBLE
# ====================================================================
print("\n" + "="*60)
print("SUMMARY OF ALL MODELS")
print("="*60)

results = {
    "LightGBM": {"mae": mae_lgb, "smape": smape_lgb, "best_iter": int(gbm_lgb.best_iteration)},
    "XGBoost":  {"mae": mae_xgb, "smape": smape_xgb, "best_iter": int(gbm_xgb.best_iteration)},
    "CatBoost": {"mae": mae_cat, "smape": smape_cat, "best_iter": int(gbm_cat.best_iteration_)}
}

for name, metrics in results.items():
    print(f"\n{name}:")
    print(f"  Best Iteration: {metrics['best_iter']}")
    print(f"  MAE          : {metrics['mae']:,.4f}")
    print(f"  SMAPE        : {metrics['smape']:.4f}%")

# Simple average ensemble
pred_val_ensemble = (pred_val_lgb + pred_val_xgb + pred_val_cat) / 3
pred_te_ensemble  = (pred_te_lgb + pred_te_xgb + pred_te_cat) / 3

mae_ensemble = mean_absolute_error(y_val, pred_val_ensemble)
smape_ensemble = smape(y_val, pred_val_ensemble)

print(f"\nEnsemble (Average):")
print(f"  MAE          : {mae_ensemble:,.4f}")
print(f"  SMAPE        : {smape_ensemble:.4f}%")

results["Ensemble"] = {"mae": mae_ensemble, "smape": smape_ensemble}

np.save(OUT_DIR / "pred_val_ensemble.npy", pred_val_ensemble)
np.save(OUT_DIR / "pred_test_ensemble.npy", pred_te_ensemble)

# Save metadata
meta = {
    "clip_folder": str(CLIP_FOLDER),
    "n_pca": {"clip": N_PCA_CLIP, "e5": N_PCA_E5, "deb": N_PCA_DEB},
    "pack_cols": list(Xpack_tr.columns),
    "results": results
}
with open(OUT_DIR / "results_summary.json", "w") as f:
    json.dump(meta, f, indent=2)

print(f"\nAll artifacts saved to: {OUT_DIR}")
gc.collect();

# Optimal convex blend of [LGB, XGB, CAT] on validation, then apply to test
import numpy as np, json
from pathlib import Path
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

BASE = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP  = BASE/"experiments"/"fusion_multi_gbm"
VAL  = np.vstack([
    np.load(EXP/"pred_val_lgb.npy"),
    np.load(EXP/"pred_val_xgb.npy"),
    np.load(EXP/"pred_val_cat.npy"),
]).T
TEST = np.vstack([
    np.load(EXP/"pred_test_lgb.npy"),
    np.load(EXP/"pred_test_xgb.npy"),
    np.load(EXP/"pred_test_cat.npy"),
]).T
y_val = np.load(EXP/"../tmp_y_val.npy") if (EXP/"../tmp_y_val.npy").exists() else \
        __import__("pandas").read_parquet(BASE/"splits/df_val_processed.parquet")["price"].values

# Non-negative, sum-to-1 fit via projected NNLS
# (simple loop over grid for sum-to-1; or use ElasticNet with constraints off)
w_grid = np.linspace(0,1,101)
best = (1e9, None)
def smape(y_true, y_pred):
    denom = (np.abs(y_true)+np.abs(y_pred))/2.0
    m = denom!=0; out = np.zeros_like(denom); out[m] = np.abs(y_true[m]-y_pred[m])/denom[m]
    return out.mean()*100
for a in w_grid:
    for b in w_grid:
        c = 1-a-b
        if c < 0: continue
        w = np.array([a,b,c])
        pv = VAL @ w
        score = smape(y_val, pv)
        if score < best[0]:
            best = (score, w)

best_smape, w = best
print("Best weights [LGB, XGB, CAT]:", w, "| SMAPE:", best_smape)

p_val_opt = VAL @ w
p_test_opt = TEST @ w
np.save(EXP/"pred_val_optblend.npy", p_val_opt)
np.save(EXP/"pred_test_optblend.npy", p_test_opt)

import numpy as np, pandas as pd
from pathlib import Path

BASE = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP  = BASE / "experiments" / "fusion_multi_gbm"

# Load test dataframe (for sample_id)
df_test = pd.read_parquet(BASE / "splits" / "df_test_processed.parquet")

# Load blended predictions
pred_test = np.load(EXP / "pred_test_optblend.npy")

# Clamp predictions (avoid negative prices just in case)
pred_test = np.clip(pred_test, 0, None)

# Prepare submission
submission = pd.DataFrame({
    "sample_id": df_test["sample_id"],  # or the correct ID column name
    "price": pred_test
})

# Save to CSV
out_path = EXP / "submission_final_optblend.csv"
submission.to_csv(out_path, index=False)

print(f"Submission file saved to: {out_path}")
print(submission.head())



# ===========================================
# FUSION MLP — CLIP ⊕ E5 ⊕ DeBERTa ⊕ PACK
# Non-linear cross-modal regression head
# ===========================================
!pip -q install torch==2.* lightgbm xgboost catboost joblib pyarrow

import os, gc, json, joblib, numpy as np, pandas as pd, torch
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error

# ---------------- Config ----------------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR = BASE_DIR / "splits"
EMB_DIRS_CLIP = [BASE_DIR / "embeddings" / "clip_vitl14_fast",
                 BASE_DIR / "embeddings" / "clip_vitl14"]
E5_DIR   = BASE_DIR / "embeddings" / "e5_large"
DEB_DIR  = BASE_DIR / "models" / "deberta_v3_price_reg"
OUT_DIR  = BASE_DIR / "experiments" / "fusion_mlp"
OUT_DIR.mkdir(parents=True, exist_ok=True)

SEED = 42
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(SEED); np.random.seed(SEED)

# ---------------- Helpers ----------------
def smape(y_true, y_pred):
    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2
    mask = denom != 0
    return (np.abs(y_true[mask] - y_pred[mask]) / denom[mask]).mean() * 100

def load_clip_arrays():
    d = None
    for trydir in EMB_DIRS_CLIP:
        if (trydir / "X_img_tr_clip.npy").exists():
            d = trydir; break
    if d is None:
        raise FileNotFoundError("Could not find CLIP embeddings.")
    X_tr = np.load(d / "X_img_tr_clip.npy")
    X_val = np.load(d / "X_img_val_clip.npy")
    X_te = np.load(d / "X_img_te_clip.npy")
    return X_tr, X_val, X_te, d

def hcat(*Xs): return np.hstack([np.asarray(X, dtype=np.float32) for X in Xs])

# ---------------- Load data ----------------
df_tr  = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
df_val = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
df_te  = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

y_tr  = np.log1p(df_tr["price"].values).astype("float32")
y_val = np.log1p(df_val["price"].values).astype("float32")

# ---------------- Load embeddings ----------------
Ximg_tr, Ximg_val, Ximg_te, CLIP_FOLDER = load_clip_arrays()
X_e5_tr  = np.load(E5_DIR / "X_text_tr_e5l2.npy")
X_e5_val = np.load(E5_DIR / "X_text_val_e5l2.npy")
X_e5_te  = np.load(E5_DIR / "X_text_te_e5l2.npy")
X_deb_tr  = np.load(DEB_DIR / "X_text_train_deberta.npy")
X_deb_val = np.load(DEB_DIR / "X_text_val_deberta.npy")
X_deb_te  = np.load(DEB_DIR / "X_text_test_deberta.npy")

# ---------------- Pack features ----------------
pack_cols_num = [c for c in ["pack_value", "pack_count"] if c in df_tr.columns]
pack_cols_bin = [c for c in ["has_pack"] if c in df_tr.columns]

def extract_pack(df):
    Xn = df[pack_cols_num].astype("float32") if pack_cols_num else pd.DataFrame(index=df.index)
    Xb = df[pack_cols_bin].astype("float32") if pack_cols_bin else pd.DataFrame(index=df.index)
    if "pack_unit" in df.columns:
        vc = df["pack_unit"].fillna("unknown").astype(str)
        oh = pd.get_dummies(vc, prefix="unit", dtype="float32")
    else:
        oh = pd.DataFrame(index=df.index)
    X = pd.concat([Xn, Xb, oh], axis=1)
    return X

Xpack_tr  = extract_pack(df_tr)
Xpack_val = extract_pack(df_val)
Xpack_te  = extract_pack(df_te)

if pack_cols_num:
    scaler = StandardScaler()
    Xpack_tr.loc[:, pack_cols_num]  = scaler.fit_transform(Xpack_tr[pack_cols_num])
    Xpack_val.loc[:, pack_cols_num] = scaler.transform(Xpack_val[pack_cols_num])
    Xpack_te.loc[:, pack_cols_num]  = scaler.transform(Xpack_te[pack_cols_num])
    joblib.dump(scaler, OUT_DIR / "scaler_pack_num.joblib")

Xpack_tr = Xpack_tr.reindex(columns=sorted(Xpack_tr.columns)).fillna(0).astype("float32")
Xpack_val = Xpack_val.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")
Xpack_te = Xpack_te.reindex(columns=Xpack_tr.columns).fillna(0).astype("float32")

# ---------------- Concatenate features ----------------
X_tr = hcat(Ximg_tr, X_e5_tr, X_deb_tr, Xpack_tr.values)
X_val = hcat(Ximg_val, X_e5_val, X_deb_val, Xpack_val.values)
X_te  = hcat(Ximg_te, X_e5_te, X_deb_te, Xpack_te.values)

print("Fused feature dims:", X_tr.shape)

# ---------------- MLP Model ----------------
class FusionMLP(torch.nn.Module):
    def __init__(self, d_in):
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(d_in, 1024),
            torch.nn.BatchNorm1d(1024),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.25),
            torch.nn.Linear(1024, 512),
            torch.nn.BatchNorm1d(512),
            torch.nn.ReLU(),
            torch.nn.Dropout(0.2),
            torch.nn.Linear(512, 1)
        )
    def forward(self, x): return self.net(x)

def train_mlp(Xtr, ytr, Xva, yva, lr=1e-3, epochs=25, bs=512, patience=3):
    model = FusionMLP(Xtr.shape[1]).to(DEVICE)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)
    best, best_ep, best_state, best_preds = 1e9, 0, None, None
    for ep in range(epochs):
        model.train()
        for i in range(0, len(Xtr), bs):
            xb = torch.tensor(Xtr[i:i+bs], dtype=torch.float32, device=DEVICE)
            yb = torch.tensor(ytr[i:i+bs], dtype=torch.float32, device=DEVICE).unsqueeze(1)
            loss = torch.nn.functional.l1_loss(model(xb), yb)
            opt.zero_grad(); loss.backward(); opt.step()
        model.eval(); preds=[]
        with torch.no_grad():
            for i in range(0, len(Xva), bs):
                xb = torch.tensor(Xva[i:i+bs], dtype=torch.float32, device=DEVICE)
                preds.append(model(xb).squeeze(1).cpu().numpy())
        pv = np.concatenate(preds)
        mae = np.mean(np.abs(pv - yva))
        print(f"Epoch {ep+1:02d} MAE={mae:.5f}")
        if mae < best: best, best_ep, best_state, best_preds = mae, ep, model.state_dict(), pv.copy()
        if ep - best_ep >= patience: break
    model.load_state_dict(best_state)
    return model, best_preds

model, pred_val_log = train_mlp(X_tr, y_tr, X_val, y_val)
# predict test
model.eval(); pred_test_log=[]
with torch.no_grad():
    for i in range(0, len(X_te), 1024):
        xb = torch.tensor(X_te[i:i+1024], dtype=torch.float32, device=DEVICE)
        pred_test_log.append(model(xb).squeeze(1).cpu().numpy())
pred_test_log = np.concatenate(pred_test_log)
# Convert from log(price) back to price safely (avoid overflow)
pred_val = np.expm1(np.clip(pred_val_log, -5, 12)).clip(0)
pred_test = np.expm1(np.clip(pred_test_log, -5, 12)).clip(0)


mae_val = mean_absolute_error(df_val["price"].values, pred_val)
smape_val = smape(df_val["price"].values, pred_val)
print(f"Fusion MLP  MAE={mae_val:.4f}, SMAPE={smape_val:.4f}%")

# ---------------- Save artifacts ----------------
np.save(OUT_DIR / "pred_val_mlp.npy", pred_val)
np.save(OUT_DIR / "pred_test_mlp.npy", pred_test)
torch.save(model.state_dict(), OUT_DIR / "fusion_mlp.pt")

meta = {"device": DEVICE, "seed": SEED,
        "dims": X_tr.shape[1],
        "mae_val": mae_val, "smape_val": smape_val,
        "clip_folder": str(CLIP_FOLDER),
        "pack_cols": list(Xpack_tr.columns)}
with open(OUT_DIR / "fusion_mlp_meta.json", "w") as f: json.dump(meta, f, indent=2)

print("Saved Fusion MLP model and predictions to:", OUT_DIR)

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/smart_product_pricing_final/experiments/fusion_multi_gbm/submission_final_optblend.csv")
print(df.shape)
print(df.head())

!pip install flash-attn --no-build-isolation

# ===========================================
# STEP 4B — IMAGE EMBEDDINGS FROM CACHE (DINOv2-Base)
# ===========================================
!pip -q install transformers>=4.44.0 timm pillow einops joblib scikit-learn tqdm

import os, gc, json, math, hashlib
import numpy as np, pandas as pd, torch
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import AutoModel, AutoImageProcessor
from sklearn.decomposition import PCA
import joblib

# ---------------- Config ----------------
SEED        = 42
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE       = torch.float16
MODEL_ID    = "facebook/dinov2-base"
BATCH_SIZE  = 128
NUM_WORKERS = 6
IMG_SIZE    = 224

BASE_DIR    = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR   = BASE_DIR / "splits"
IMG_DIR     = BASE_DIR / "images_cache"        # same cache as OpenCLIP
OUT_DIR     = BASE_DIR / "embeddings" / "dinov2_base"
OUT_DIR.mkdir(parents=True, exist_ok=True)

torch.manual_seed(SEED)
np.random.seed(SEED)
print(f"Using {MODEL_ID} | device={DEVICE} | dtype={DTYPE}")

# ---------------- Load splits ----------------
tr = pd.read_parquet(SPLIT_DIR / "df_train_processed.parquet")
va = pd.read_parquet(SPLIT_DIR / "df_val_processed.parquet")
te = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

# ---------------- Dataset ----------------
class ImgDataset(Dataset):
    """Return image file paths from cache."""
    def __init__(self, df, cache_dir):
        self.paths = [
            cache_dir / (
                str(row["image_link"]) if str(row["image_link"]).endswith(".jpg")
                else hashlib.md5(str(row["image_link"]).encode()).hexdigest() + ".jpg"
            )
            for _, row in df.iterrows()
        ]
    def __len__(self): return len(self.paths)
    def __getitem__(self, i): return str(self.paths[i])

def make_loader(df):
    return DataLoader(
        ImgDataset(df, IMG_DIR),
        batch_size=BATCH_SIZE,
        num_workers=NUM_WORKERS,
        shuffle=False,
        pin_memory=True,
        prefetch_factor=2
    )

# ---------------- Model ----------------
processor = AutoImageProcessor.from_pretrained(MODEL_ID)
model = AutoModel.from_pretrained(MODEL_ID).to(DEVICE).eval()
print("Model loaded.")

@torch.no_grad()
def dino_embed(batch_imgs):
    """Return mean-pooled, L2-normalized DINOv2 embeddings."""
    inputs = processor(images=batch_imgs, return_tensors="pt")
    pixel_values = inputs["pixel_values"].to(DEVICE, dtype=DTYPE)
    outputs = model(pixel_values=pixel_values)
    if hasattr(outputs, "last_hidden_state"):
        emb = outputs.last_hidden_state.mean(dim=1)
    elif hasattr(outputs, "pooler_output"):
        emb = outputs.pooler_output
    else:
        raise ValueError("No valid embedding field found in DINO output.")
    return F.normalize(emb.float(), dim=-1).cpu().numpy()

def encode_split(df, split_name):
    dl = make_loader(df)
    feats = []
    for batch_paths in tqdm(dl, desc=f"DINOv2 — {split_name}", total=len(dl)):
        imgs = [
            Image.open(p).convert("RGB") if os.path.exists(p)
            else Image.new("RGB", (IMG_SIZE, IMG_SIZE), (255,255,255))
            for p in batch_paths
        ]
        feats.append(dino_embed(imgs))
        del imgs
        torch.cuda.empty_cache()
    X = np.concatenate(feats, axis=0)
    np.save(OUT_DIR / f"X_img_{split_name}_dino.npy", X.astype("float32"))
    print(f"Saved X_img_{split_name}_dino.npy → {X.shape}")
    return X

# ---------------- Encode all splits ----------------
X_img_tr  = encode_split(tr, "tr")
X_img_val = encode_split(va, "val")
X_img_te  = encode_split(te, "te")

# ---------------- PCA(128) ----------------
N_PCA = 128
pca = PCA(n_components=N_PCA, random_state=SEED)
X_tr_pca  = pca.fit_transform(X_img_tr)
X_val_pca = pca.transform(X_img_val)
X_te_pca  = pca.transform(X_img_te)

joblib.dump(pca, OUT_DIR / "dino_img_pca.joblib")
np.save(OUT_DIR / "X_img_tr_dino_pca128.npy",  X_tr_pca)
np.save(OUT_DIR / "X_img_val_dino_pca128.npy", X_val_pca)
np.save(OUT_DIR / "X_img_te_dino_pca128.npy",  X_te_pca)

expl = float(np.sum(pca.explained_variance_ratio_))
print(f"PCA(128) explained variance: {expl:.4f}")
print("PCA shapes:", X_tr_pca.shape, X_val_pca.shape, X_te_pca.shape)

# ---------------- Meta Info ----------------
meta = {
    "model_id": MODEL_ID,
    "device": DEVICE,
    "dtype": str(DTYPE),
    "batch_size": BATCH_SIZE,
    "num_workers": NUM_WORKERS,
    "raw_dim": int(X_img_tr.shape[1]),
    "pca_dim": N_PCA,
    "explained_variance": expl
}
with open(OUT_DIR / "meta_dino.json", "w") as f:
    json.dump(meta, f, indent=2)

print("DINOv2-Base embedding & PCA pipeline complete. Artifacts saved to", OUT_DIR)



# ===========================================
# INSPECT SMART_PRODUCT_PRICING_FINAL DIRECTORY (NO IMAGE FILES)
# ===========================================
from pathlib import Path

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")

def list_files_filtered(base_subdir, depth=3):
    base = BASE_DIR / base_subdir
    print(f"\ {base}")
    for p in base.rglob("*"):
        rel = p.relative_to(BASE_DIR)
        # Skip image-heavy dirs
        if "images" in rel.parts or "images_cache" in rel.parts:
            continue
        if p.is_file() and len(rel.parts) <= depth:
            size_mb = p.stat().st_size / (1024*1024)
            print(f"   {rel} ({size_mb:.2f} MB)")

# Embeddings
list_files_filtered("embeddings", depth=3)

# Models (DeBERTa, etc.)
list_files_filtered("models", depth=3)

# Splits (targets and indices)
list_files_filtered("splits", depth=2)

# Experiments (trained models, submissions)
list_files_filtered("experiments", depth=3)

# ===========================================
# RUN PCA(128) FOR NON-PCA EMBEDDINGS (E5, DeBERTa, CLIP)
# ===========================================
!pip -q install scikit-learn joblib tqdm

import numpy as np, joblib, gc
from sklearn.decomposition import PCA
from tqdm import tqdm
from pathlib import Path

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")

PCA_TARGETS = {
    "e5": {
        "train": BASE_DIR / "embeddings/e5_large/X_text_tr_e5l2.npy",
        "val":   BASE_DIR / "embeddings/e5_large/X_text_val_e5l2.npy",
        "out_dir": BASE_DIR / "embeddings/e5_large"
    },
    "deb": {
        "train": BASE_DIR / "models/deberta_v3_price_reg/X_text_train_deberta.npy",
        "val":   BASE_DIR / "models/deberta_v3_price_reg/X_text_val_deberta.npy",
        "out_dir": BASE_DIR / "models/deberta_v3_price_reg"
    },
    "clip": {
        "train": BASE_DIR / "embeddings/clip_vitl14_fast/X_img_tr_clip.npy",
        "val":   BASE_DIR / "embeddings/clip_vitl14_fast/X_img_val_clip.npy",
        "out_dir": BASE_DIR / "embeddings/clip_vitl14_fast"
    }
}

N_PCA = 128
for name, paths in PCA_TARGETS.items():
    print(f"\n=== Running PCA(128) for {name.upper()} ===")
    X_tr = np.load(paths["train"])
    X_val = np.load(paths["val"])
    print(f"Train shape: {X_tr.shape}, Val shape: {X_val.shape}")

    pca = PCA(n_components=N_PCA, random_state=42)
    X_tr_pca = pca.fit_transform(X_tr)
    X_val_pca = pca.transform(X_val)
    expl = pca.explained_variance_ratio_.sum()
    print(f"Explained variance ratio: {expl:.4f}")

    np.save(paths["out_dir"] / f"X_tr_{name}_pca128.npy", X_tr_pca.astype("float32"))
    np.save(paths["out_dir"] / f"X_val_{name}_pca128.npy", X_val_pca.astype("float32"))
    joblib.dump(pca, paths["out_dir"] / f"pca_{name}.joblib")

    print(f"✅ Saved: {name} PCA(128) → {X_tr_pca.shape} (train), {X_val_pca.shape} (val)")
    del X_tr, X_val, X_tr_pca, X_val_pca, pca
    gc.collect()

# ===========================================
# STEP — Generate Test PCA(128) Embeddings (E5, DeBERTa, CLIP)
# ===========================================
!pip -q install joblib tqdm scikit-learn

import numpy as np, joblib, gc
from tqdm import tqdm
from pathlib import Path

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")

PCA_MODELS = {
    "e5": {
        "pca_path": BASE_DIR / "embeddings/e5_large/pca_e5.joblib",
        "test": BASE_DIR / "embeddings/e5_large/X_text_te_e5l2.npy",
        "out_dir": BASE_DIR / "embeddings/e5_large"
    },
    "deb": {
        "pca_path": BASE_DIR / "models/deberta_v3_price_reg/pca_deb.joblib",
        "test": BASE_DIR / "models/deberta_v3_price_reg/X_text_test_deberta.npy",
        "out_dir": BASE_DIR / "models/deberta_v3_price_reg"
    },
    "clip": {
        "pca_path": BASE_DIR / "embeddings/clip_vitl14_fast/pca_clip.joblib",
        "test": BASE_DIR / "embeddings/clip_vitl14_fast/X_img_te_clip.npy",
        "out_dir": BASE_DIR / "embeddings/clip_vitl14_fast"
    }
}

for name, info in PCA_MODELS.items():
    print(f"\n=== Transforming TEST set with PCA for {name.upper()} ===")
    X_te = np.load(info["test"])
    pca = joblib.load(info["pca_path"])
    X_te_pca = pca.transform(X_te)
    np.save(info["out_dir"] / f"X_te_{name}_pca128.npy", X_te_pca.astype("float32"))
    print(f"✅ Saved: {info['out_dir']}/X_te_{name}_pca128.npy → {X_te_pca.shape}")
    del X_te, X_te_pca, pca
    gc.collect()

# ===========================================
# STEP 8 — Cross-Modal Transformer Fusion (PCA-Aligned + Prediction Saving)
# ===========================================
!pip -q install torch torchvision torchaudio scikit-learn tqdm

import os, math, json, gc
from pathlib import Path
import numpy as np
import torch, torch.nn as nn, torch.nn.functional as F
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler

# ---------------- Config ----------------
SEED        = 42
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
D_MODEL     = 256
N_HEADS     = 4
N_LAYERS    = 2
FF_DIM      = 512
DROPOUT     = 0.10
BATCH_SIZE  = 512
EPOCHS      = 15
PATIENCE    = 3
LR          = 1e-4
WD          = 1e-4

BASE_DIR    = Path("/content/drive/MyDrive/smart_product_pricing_final")
SPLIT_DIR   = BASE_DIR / "splits"
CKPT_DIR    = BASE_DIR / "experiments" / "fusion_transformer"
CKPT_DIR.mkdir(parents=True, exist_ok=True)

# ---------------- Paths (all 128-D PCA arrays) ----------------
PCA_PATHS = {
    "e5":   BASE_DIR / "embeddings" / "e5_large" / "X_tr_e5_pca128.npy",
    "deb":  BASE_DIR / "models" / "deberta_v3_price_reg" / "X_tr_deb_pca128.npy",
    "clip": BASE_DIR / "embeddings" / "clip_vitl14_fast" / "X_tr_clip_pca128.npy",
    "dino": BASE_DIR / "embeddings" / "dinov2_base" / "X_img_tr_dino_pca128.npy",
}
PCA_PATHS_VAL = {
    "e5":   BASE_DIR / "embeddings" / "e5_large" / "X_val_e5_pca128.npy",
    "deb":  BASE_DIR / "models" / "deberta_v3_price_reg" / "X_val_deb_pca128.npy",
    "clip": BASE_DIR / "embeddings" / "clip_vitl14_fast" / "X_val_clip_pca128.npy",
    "dino": BASE_DIR / "embeddings" / "dinov2_base" / "X_img_val_dino_pca128.npy",
}
PCA_PATHS_TEST = {
    "e5":   BASE_DIR / "embeddings" / "e5_large" / "X_te_e5_pca128.npy",
    "deb":  BASE_DIR / "models" / "deberta_v3_price_reg" / "X_te_deb_pca128.npy",
    "clip": BASE_DIR / "embeddings" / "clip_vitl14_fast" / "X_te_clip_pca128.npy",
    "dino": BASE_DIR / "embeddings" / "dinov2_base" / "X_img_te_dino_pca128.npy",
}

# ---------------- Helpers ----------------
def smape(y_true, y_pred):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_pred - y_true)
    return 100.0 * np.mean(np.where(denom == 0, 0.0, diff / denom))

def load(path): return np.load(path)

# ---------------- Load data ----------------
y_tr_log = np.load(SPLIT_DIR / "y_train_log.npy")
y_val_log = np.load(SPLIT_DIR / "y_val_log.npy")

Xtr = {k: load(v) for k,v in PCA_PATHS.items()}
Xva = {k: load(v) for k,v in PCA_PATHS_VAL.items()}
Xte = {k: load(v) for k,v in PCA_PATHS_TEST.items()}
DIMS = {k: Xtr[k].shape[1] for k in Xtr}
for k,v in DIMS.items(): print(f"{k.upper():>6s}: {v} dims")

# ---------------- Standardize ----------------
scalers = {}
for k in Xtr:
    s = StandardScaler()
    Xtr[k] = s.fit_transform(Xtr[k])
    Xva[k] = s.transform(Xva[k])
    Xte[k] = s.transform(Xte[k])
    scalers[k] = s

def to_tokens(d):
    # Order: E5, DeBERTa, CLIP, DINO
    return np.stack([d["e5"], d["deb"], d["clip"], d["dino"]], axis=1)

Xtr_tokens = to_tokens(Xtr)
Xva_tokens = to_tokens(Xva)
Xte_tokens = to_tokens(Xte)
print("Tokens:", Xtr_tokens.shape, Xva_tokens.shape, Xte_tokens.shape)

# ---------------- Model ----------------
class TokenProjector(nn.Module):
    def __init__(self, d_in=128, d_model=256):
        super().__init__()
        self.proj = nn.Linear(d_in, d_model)
    def forward(self, x): return self.proj(x)

class CrossModalTransformer(nn.Module):
    def __init__(self, d_in=128, d_model=256, n_heads=4, n_layers=2, ff_dim=512, dropout=0.1):
        super().__init__()
        self.projector = TokenProjector(d_in, d_model)
        self.mod_embed = nn.Embedding(4, d_model)
        layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim,
            dropout=dropout, activation="gelu", batch_first=True, norm_first=True)
        self.encoder = nn.TransformerEncoder(layer, num_layers=n_layers)
        self.head = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, 1),
        )
    def forward(self, x):
        B, T, _ = x.shape
        x = self.projector(x)
        mod_ids = torch.arange(0, 4, device=x.device).view(1, 4)
        x = x + self.mod_embed(mod_ids)
        x = self.encoder(x)
        x = x.mean(dim=1)
        return self.head(x).squeeze(-1)

model = CrossModalTransformer(128, D_MODEL, N_HEADS, N_LAYERS, FF_DIM, DROPOUT).to(DEVICE)

# ---------------- DataLoaders ----------------
class TokenDataset(torch.utils.data.Dataset):
    def __init__(self, X, y): self.X, self.y = X.astype(np.float32), y.astype(np.float32)
    def __len__(self): return len(self.y)
    def __getitem__(self,i): return self.X[i], self.y[i]

tr_ds = TokenDataset(Xtr_tokens, y_tr_log)
va_ds = TokenDataset(Xva_tokens, y_val_log)
tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)
va_dl = torch.utils.data.DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)

# ---------------- Training ----------------
torch.manual_seed(SEED)
opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)
best_val, best_state, no_imp = 1e9, None, 0

def evaluate(dl):
    model.eval(); tot, n = 0.,0; preds, gts = [],[]
    with torch.no_grad():
        for xb,yb in dl:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)
            out = model(xb)
            loss = F.l1_loss(out, yb)
            tot += loss.item()*xb.size(0); n += xb.size(0)
            preds.append(out.cpu().numpy()); gts.append(yb.cpu().numpy())
    preds, gts = np.concatenate(preds), np.concatenate(gts)
    val_log_mae = tot/n
    val_mae = np.mean(np.abs(np.exp(preds)-np.exp(gts)))
    val_smape = smape(np.exp(gts), np.exp(preds))
    return val_log_mae, val_mae, val_smape, preds

print(f"PCA transformer | d_model={D_MODEL} | layers={N_LAYERS} | heads={N_HEADS}")
for epoch in range(1,EPOCHS+1):
    model.train(); tot,n=0,0
    for xb,yb in tr_dl:
        xb,yb = xb.to(DEVICE), yb.to(DEVICE)
        opt.zero_grad()
        out = model(xb)
        loss = F.l1_loss(out, yb)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)
        opt.step()
        tot+=loss.item()*xb.size(0); n+=xb.size(0)
    sched.step()
    tr_mae = tot/n
    va_log_mae, va_mae, va_smape, _ = evaluate(va_dl)
    print(f"Epoch {epoch:02d} | train {tr_mae:.4f} | val logMAE {va_log_mae:.4f} | "
          f"val MAE {va_mae:.4f} | SMAPE {va_smape:.2f}%")
    if va_log_mae<best_val:
        best_val, no_imp = va_log_mae,0
        best_state={"model":model.state_dict(),"epoch":epoch}
    else:
        no_imp+=1
        if no_imp>=PATIENCE:
            print("Early stop.")
            break

# ---------------- Save model ----------------
if best_state:
    model.load_state_dict(best_state["model"])
    torch.save(best_state, CKPT_DIR / f"cm_transformer_pca_best.pt")

# ---------------- Final Eval + Predictions ----------------
val_log_mae, val_mae, val_smape, val_preds = evaluate(va_dl)
print("\n=== Validation Summary ===")
print(f"Val MAE: {val_mae:.4f} | Val SMAPE: {val_smape:.2f}%")

# Save validation predictions
np.save(CKPT_DIR / "pred_val_transformer.npy", val_preds)

# Generate test predictions
@torch.no_grad()
def predict_test(X):
    model.eval(); preds=[]
    for i in tqdm(range(0, len(X), BATCH_SIZE), desc="Predicting test"):
        xb = torch.tensor(X[i:i+BATCH_SIZE], dtype=torch.float32, device=DEVICE)
        preds.append(model(xb).cpu().numpy())
    return np.concatenate(preds)

pred_test = predict_test(Xte_tokens)
np.save(CKPT_DIR / "pred_test_transformer.npy", pred_test)
print(f"✅ Saved predictions: val={val_preds.shape}, test={pred_test.shape}")

# Save summary JSON
summary = {
    "mode":"pca","d_model":D_MODEL,"layers":N_LAYERS,"heads":N_HEADS,
    "ff_dim":FF_DIM,"dropout":DROPOUT,"batch_size":BATCH_SIZE,
    "epochs":best_state["epoch"] if best_state else EPOCHS,
    "val_logMAE":float(val_log_mae),"val_MAE":float(val_mae),
    "val_SMAPE":float(val_smape)
}
with open(CKPT_DIR / "summary_pca.json","w") as f: json.dump(summary,f,indent=2)

# ===========================================
# STEP 9 — META-ENSEMBLE FUSION (GBM + Transformer)
# ===========================================
!pip -q install lightgbm scikit-learn joblib

import numpy as np, pandas as pd, json
from pathlib import Path
from sklearn.linear_model import ElasticNetCV
from sklearn.metrics import mean_absolute_error
from joblib import dump
import lightgbm as lgb

# ---------------- Config ----------------
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP_DIR  = BASE_DIR / "experiments"
STACK_DIR = EXP_DIR / "stacking_final"
STACK_DIR.mkdir(parents=True, exist_ok=True)

# ---------------- Load predictions ----------------
gbm_val = np.load(EXP_DIR / "fusion_multi_gbm" / "pred_val_optblend.npy")   # log(price)
transformer_val = np.load(EXP_DIR / "fusion_transformer" / "pred_val_transformer.npy")  # log(price)
y_val_log = np.load(BASE_DIR / "splits" / "y_val_log.npy")

# For test-time inference later
gbm_test = np.load(EXP_DIR / "fusion_multi_gbm" / "pred_test_optblend.npy")
transformer_test = np.load(EXP_DIR / "fusion_transformer" / "pred_test_transformer.npy")

print(f"Shapes: GBM={gbm_val.shape}, Transformer={transformer_val.shape}, Target={y_val_log.shape}")

# ---------------- Stack features ----------------
X_val_meta  = np.vstack([gbm_val, transformer_val]).T
X_test_meta = np.vstack([gbm_test, transformer_test]).T

# ==========================================================
# Option A: ElasticNetCV Meta-Learner (lightweight, robust)
# ==========================================================
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_val_scaled = scaler.fit_transform(X_val_meta)
X_test_scaled = scaler.transform(X_test_meta)

meta = ElasticNetCV(
    alphas=np.logspace(-3, 1, 50),
    l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9],
    cv=5,
    max_iter=20000,
    n_jobs=-1,
    random_state=42
)
meta.fit(X_val_scaled, y_val_log)

# Predict & evaluate
y_pred_log = meta.predict(X_val_scaled)
val_mae = mean_absolute_error(np.exp(y_val_log), np.exp(y_pred_log))
smape = 100 * np.mean(np.abs(np.exp(y_pred_log) - np.exp(y_val_log)) /
                      ((np.abs(np.exp(y_pred_log)) + np.abs(np.exp(y_val_log))) / 2))

print(f"ElasticNet Meta-Learner — MAE: {val_mae:.4f}, SMAPE: {smape:.2f}%")
dump(meta, STACK_DIR / "meta_ensemble_elasticnet.joblib")

# Generate final test predictions (back in normal price scale)
y_test_log = meta.predict(X_test_scaled)
y_test = np.exp(y_test_log)
np.save(STACK_DIR / "pred_test_stack.npy", y_test)

# Save summary
summary = {
    "meta_model": "ElasticNetCV",
    "val_MAE": float(val_mae),
    "val_SMAPE": float(smape),
    "alphas": list(meta.alphas_),
    "l1_ratio": float(meta.l1_ratio_),
}
with open(STACK_DIR / "meta_summary.json", "w") as f: json.dump(summary, f, indent=2)
print(json.dumps(summary, indent=2))

# ==========================================================
# Option B: LightGBM Meta-Learner (non-linear stacking)
# ==========================================================
train_data = lgb.Dataset(X_val_meta, label=y_val_log)
params = {
    "objective": "regression_l1",
    "metric": "l1",
    "verbosity": -1,
    "seed": 42,
    "learning_rate": 0.05,
    "num_leaves": 15,
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 3,
}

gbm_meta = lgb.train(params, train_data, num_boost_round=1000)
y_pred_lgb = gbm_meta.predict(X_val_meta)
val_mae_lgb = mean_absolute_error(np.exp(y_val_log), np.exp(y_pred_lgb))
smape_lgb = 100 * np.mean(np.abs(np.exp(y_pred_lgb) - np.exp(y_val_log)) /
                          ((np.abs(np.exp(y_pred_lgb)) + np.abs(np.exp(y_val_log))) / 2))
print(f"LightGBM Meta-Learner — MAE: {val_mae_lgb:.4f}, SMAPE: {smape_lgb:.2f}%")

# Save for comparison
dump(gbm_meta, STACK_DIR / "meta_ensemble_lightgbm.joblib")

# ===========================================
# STEP 10 — FINAL SUBMISSION GENERATION + DOWNLOAD (Fixed for sample_id)
# ===========================================
import pandas as pd, numpy as np
from pathlib import Path
from google.colab import files  # for download

# --- Paths ---
BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
STACK_DIR = BASE_DIR / "experiments" / "stacking_final"
SPLIT_DIR = BASE_DIR / "splits"

# --- Load predictions & test IDs ---
y_pred = np.load(STACK_DIR / "pred_test_stack.npy")
df_test = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

# --- Create submission ---
submission = pd.DataFrame({
    "sample_id": df_test["sample_id"].values,
    "price": y_pred
})

# --- Save CSV ---
out_path = STACK_DIR / "submission_final_meta_lightgbm.csv"
submission.to_csv(out_path, index=False)
print(f"Final submission saved → {out_path} | shape = {submission.shape}")

# --- Optional: Download to local machine ---
files.download(out_path)

# ===========================================
# STEP 10B — Weighted Blend: Fusion + Transformer
# ===========================================
import numpy as np, pandas as pd
from pathlib import Path
from google.colab import files

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP_DIR  = BASE_DIR / "experiments"
SPLIT_DIR = BASE_DIR / "splits"

# --- Load predictions ---
gbm_test = np.load(EXP_DIR / "fusion_multi_gbm" / "pred_test_optblend.npy")       # log(price)
transformer_test = np.load(EXP_DIR / "fusion_transformer" / "pred_test_transformer.npy")  # log(price)

# --- Blend weights (tune if needed) ---
w_gbm, w_tr = 0.75, 0.25     # 70–80% GBM, 20–30% Transformer is usually optimal
blend_log = w_gbm * gbm_test + w_tr * transformer_test
blend_pred = np.exp(blend_log)  # convert back to price scale

# --- Load test IDs ---
df_test = pd.read_parquet(SPLIT_DIR / "df_test_processed.parquet")

# --- Form submission ---
submission = pd.DataFrame({
    "sample_id": df_test["sample_id"],
    "price": blend_pred
})

out_path = EXP_DIR / "stacking_final" / "submission_blend_gbm_transformer.csv"
submission.to_csv(out_path, index=False)
print(f"✅ Saved blended submission → {out_path} | shape={submission.shape}")

# --- Download if desired ---
files.download(out_path)

# ===========================================
# STEP 10C — Weighted Blend Validation SMAPE
# ===========================================
import numpy as np

# --- Load predictions ---
BASE_DIR = "/content/drive/MyDrive/smart_product_pricing_final/experiments"
gbm_val = np.load(f"{BASE_DIR}/fusion_multi_gbm/pred_val_optblend.npy")          # log(price)
transformer_val = np.load(f"{BASE_DIR}/fusion_transformer/pred_val_transformer.npy")  # log(price)
y_val_log = np.load("/content/drive/MyDrive/smart_product_pricing_final/splits/y_val_log.npy")

# --- Function to compute SMAPE ---
def smape(y_true, y_pred):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2
    diff = np.abs(y_pred - y_true)
    return 100 * np.mean(np.where(denom == 0, 0.0, diff / denom))

# --- Weighted blend (tune weights as needed) ---
w_gbm, w_tr = 0.75, 0.25     # start here
blend_log = w_gbm * gbm_val + w_tr * transformer_val
blend_pred = np.exp(blend_log)
y_val = np.exp(y_val_log)

# --- Compute metrics ---
mae = np.mean(np.abs(y_val - blend_pred))
smape_val = smape(y_val, blend_pred)

print(f"Weighted Blend (w_gbm={w_gbm}, w_tr={w_tr})")
print(f"Validation MAE  : {mae:.4f}")
print(f"Validation SMAPE: {smape_val:.2f}%")

import numpy as np

BASE_DIR = "/content/drive/MyDrive/smart_product_pricing_final/experiments"
gbm_val = np.load(f"{BASE_DIR}/fusion_multi_gbm/pred_val_optblend.npy")
transformer_val = np.load(f"{BASE_DIR}/fusion_transformer/pred_val_transformer.npy")

print("GBM   min/max:", gbm_val.min(), gbm_val.max())
print("Trans min/max:", transformer_val.min(), transformer_val.max())

# ===========================================
# STEP 10C (Fixed) — Weighted Blend Validation SMAPE
# ===========================================
import numpy as np

BASE_DIR = "/content/drive/MyDrive/smart_product_pricing_final/experiments"

# Load predictions
gbm_val = np.load(f"{BASE_DIR}/fusion_multi_gbm/pred_val_optblend.npy")       # price-scale
transformer_val = np.load(f"{BASE_DIR}/fusion_transformer/pred_val_transformer.npy")  # log-scale
y_val_log = np.load("/content/drive/MyDrive/smart_product_pricing_final/splits/y_val_log.npy")

# Convert all to same log scale
gbm_val_log = np.log(np.clip(gbm_val, 1e-9, None))  # convert GBM to log(price)
transformer_val_log = transformer_val               # already log(price)

# Weighted blend in log-space
w_gbm, w_tr = 0.75, 0.25
blend_log = w_gbm * gbm_val_log + w_tr * transformer_val_log

# Convert back to price-scale
blend_pred = np.exp(blend_log)
y_val = np.exp(y_val_log)

# SMAPE + MAE
def smape(y_true, y_pred):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2
    diff = np.abs(y_pred - y_true)
    return 100 * np.mean(np.where(denom == 0, 0.0, diff / denom))

mae = np.mean(np.abs(y_val - blend_pred))
smape_val = smape(y_val, blend_pred)

print(f"Weighted Blend (w_gbm={w_gbm}, w_tr={w_tr})")
print(f"Validation MAE  : {mae:.4f}")
print(f"Validation SMAPE: {smape_val:.2f}%")

# ===========================================
# STEP 10D — Weight Sweep for Optimal Blend (Validation)
# ===========================================
import numpy as np

BASE_DIR = "/content/drive/MyDrive/smart_product_pricing_final/experiments"

# Load predictions
gbm_val = np.load(f"{BASE_DIR}/fusion_multi_gbm/pred_val_optblend.npy")       # price-scale
transformer_val = np.load(f"{BASE_DIR}/fusion_transformer/pred_val_transformer.npy")  # log-scale
y_val_log = np.load("/content/drive/MyDrive/smart_product_pricing_final/splits/y_val_log.npy")

# Convert to same log scale
gbm_val_log = np.log(np.clip(gbm_val, 1e-9, None))  # convert GBM to log(price)
transformer_val_log = transformer_val               # already log(price)
y_val = np.exp(y_val_log)

# Define metrics
def smape(y_true, y_pred):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2
    diff = np.abs(y_pred - y_true)
    return 100 * np.mean(np.where(denom == 0, 0.0, diff / denom))

def evaluate_blend(w_gbm):
    w_tr = 1 - w_gbm
    blend_log = w_gbm * gbm_val_log + w_tr * transformer_val_log
    blend_pred = np.exp(blend_log)
    mae = np.mean(np.abs(y_val - blend_pred))
    smape_val = smape(y_val, blend_pred)
    return mae, smape_val

# Sweep weights
results = []
for w_gbm in np.arange(0.5, 0.91, 0.05):
    mae, smape_val = evaluate_blend(w_gbm)
    results.append((round(w_gbm, 2), mae, smape_val))
    print(f"w_gbm={w_gbm:.2f} | MAE={mae:.4f} | SMAPE={smape_val:.2f}%")

# Best weight
best = min(results, key=lambda x: x[2])
print("\n=== Best Blend Ratio ===")
print(f"GBM weight: {best[0]:.2f} | Transformer weight: {1-best[0]:.2f}")
print(f"Validation MAE={best[1]:.4f} | SMAPE={best[2]:.2f}%")

# ===========================================
# STEP 10E — Apply Optimal Blend to Test + Generate Submission (Drive version)
# ===========================================
import numpy as np, pandas as pd
from pathlib import Path

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP_DIR  = BASE_DIR / "experiments"
STACK_DIR = EXP_DIR / "stacking_final"
STACK_DIR.mkdir(parents=True, exist_ok=True)

# Load predictions
gbm_test = np.load(EXP_DIR / "fusion_multi_gbm/pred_test_optblend.npy")         # price-scale
transformer_test = np.load(EXP_DIR / "fusion_transformer/pred_test_transformer.npy")  # log-scale

# Convert to same scale (log)
gbm_test_log = np.log(np.clip(gbm_test, 1e-9, None))
transformer_test_log = transformer_test  # already log(price)

# Optimal blend weights
w_gbm, w_tr = 0.50, 0.50

# Blend in log-space → back to price
blend_test_log = w_gbm * gbm_test_log + w_tr * transformer_test_log
blend_test = np.exp(blend_test_log)

# Load your actual test reference file
sample_path = BASE_DIR / "experiments" / "fusion_multi_gbm" / "submission_final_optblend.csv"
sample = pd.read_csv(sample_path)

# Construct submission
sub = pd.DataFrame({
    "sample_id": sample["sample_id"],
    "price": blend_test
})

# Save final CSV
sub_path = STACK_DIR / "submission_weighted_blend.csv"
sub.to_csv(sub_path, index=False)

# Quick sanity check
print(f"✅ Saved submission → {sub_path}")
print(f"Price stats — min: {sub['price'].min():.3f}, max: {sub['price'].max():.3f}, mean: {sub['price'].mean():.3f}")
print(sub.head())

# ===========================================
# STEP 10F — Verify Weighted Blend vs. Meta-Ensemble (Fixed Scaling)
# ===========================================
import numpy as np, json
from pathlib import Path
from sklearn.metrics import mean_absolute_error

BASE_DIR = Path("/content/drive/MyDrive/smart_product_pricing_final")
EXP_DIR  = BASE_DIR / "experiments"
STACK_DIR = EXP_DIR / "stacking_final"

# --- Load validation data and predictions ---
y_val_log = np.load(BASE_DIR / "splits/y_val_log.npy")

# Individual model predictions
gbm_val_price = np.load(EXP_DIR / "fusion_multi_gbm/pred_val_optblend.npy")      # price-scale
tr_val_log = np.load(EXP_DIR / "fusion_transformer/pred_val_transformer.npy")    # log(price)
meta_val_pred = np.load(STACK_DIR / "pred_val_stack.npy") if (STACK_DIR / "pred_val_stack.npy").exists() else None

# --- Convert to log scale for consistent blending ---
gbm_val_log = np.log(np.clip(gbm_val_price, 1e-9, None))

# --- Weighted blend ---
w_gbm, w_tr = 0.50, 0.50
blend_val_log = w_gbm * gbm_val_log + w_tr * tr_val_log
blend_val = np.exp(blend_val_log)
y_true = np.exp(y_val_log)

# --- Metrics ---
def smape(y_true, y_pred):
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    diff = np.abs(y_pred - y_true)
    return 100.0 * np.mean(np.where(denom == 0, 0.0, diff / denom))

mae_blend = mean_absolute_error(y_true, blend_val)
smape_blend = smape(y_true, blend_val)
print(f"Weighted Blend — MAE: {mae_blend:.4f}, SMAPE: {smape_blend:.2f}%")

# --- Compare to Meta-Ensemble (if exists) ---
if meta_val_pred is not None:
    mae_meta = mean_absolute_error(y_true, meta_val_pred)
    smape_meta = smape(y_true, meta_val_pred)
    print(f"Meta-Ensemble (LightGBM) — MAE: {mae_meta:.4f}, SMAPE: {smape_meta:.2f}%")

    best_model = "weighted_blend" if smape_blend < smape_meta else "meta_ensemble"
    print(f"\n=== Best validation performer: {best_model.upper()} ===")

    summary = {
        "weighted_blend": {"MAE": float(mae_blend), "SMAPE": float(smape_blend)},
        "meta_ensemble": {"MAE": float(mae_meta), "SMAPE": float(smape_meta)},
        "best_model": best_model,
        "weights": {"gbm": w_gbm, "transformer": w_tr}
    }
else:
    print("⚠️ Meta ensemble validation predictions not found — only blend evaluated.")
    summary = {
        "weighted_blend": {"MAE": float(mae_blend), "SMAPE": float(smape_blend)},
        "best_model": "weighted_blend",
        "weights": {"gbm": w_gbm, "transformer": w_tr}
    }

# --- Save JSON for reproducibility ---
out_path = STACK_DIR / "blend_vs_meta_comparison_fixed.json"
with open(out_path, "w") as f: json.dump(summary, f, indent=2)
print(f"\n✅ Comparison summary saved → {out_path}")
print(json.dumps(summary, indent=2))

# ===========================================
# DOWNLOAD FINAL WEIGHTED BLEND SUBMISSION
# ===========================================
from google.colab import files
from pathlib import Path

sub_path = Path("/content/drive/MyDrive/smart_product_pricing_final/experiments/stacking_final/submission_weighted_blend.csv")
print(f"Preparing download: {sub_path}")
files.download(sub_path)